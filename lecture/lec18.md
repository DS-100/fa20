---
layout: page
title: Lecture 18 – Logistic Regression, Part 1
nav_exclude: true
---

# Lecture 18 – Logistic Regression, Part 1

by Suraj Rampure (Summer 2020)

- [slides](https://docs.google.com/presentation/d/17ei11-UgjbPVBE-e4h52wxVdtfu5cA5a-xAr-3grUF8/edit#slide=id.p)
- [video playlist](https://www.youtube.com/playlist?list=PLQCcNQgUcDfpVRBS6FnY5z-b-OQvhcvHi)
- [code](https://data100.datahub.berkeley.edu/hub/user-redirect/git-sync?repo=https://github.com/DS-100/su20&subPath=lecture/lec18/)
- [code HTML](../../resources/assets/lectures/lec18/lec18.html)

Make sure to complete the Quick Check questions in between each video. These are ungraded, but it's in your best interest to do them.

<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Video</th>
<th>Quick Check</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>18.1</strong> <br>Classification, and a brief overview of the machine learning taxonomy.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/09Eu9ckflMk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLSfQHLcOhHsR8xyB8mYv62sqXDExZSZEh8pBI9PDQ986CIF6GA/viewform" target="\_blank">18.1</a></td>
</tr>
<tr>
<td><strong>18.2</strong> <br>Pitfalls of using least squares to model probabilities. Creating a graph of averages to motivate the logistic regression model.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/5tO27qVS3zA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLSftgc7NDwOGc4eKRSXKaAXl9jDEwn-b4n8A5Nt7eQTP7EO99Q/viewform" target="\_blank">18.2</a></td>
</tr>
<tr>
<td><strong>18.3</strong> <br>Deriving the logistic regression model from the assumption that the log-odds of the probability of belonging to class 1 is linear.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/RPeLrOS3FjA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLSfVM30icbf7S8miAyDhabH1vZto-1Hr4Zw5KeijXX3SVXH_LA/viewform" target="\_blank">18.3</a></td>
</tr>
<tr>
<td><strong>18.4</strong> <br>Formalizing the logistic regression model. Exploring properties of the logistic function. Interpreting the model coefficients.</td>
<td><iframe width="300" height="500" height src="https://youtube.com/embed/A-mD0g3cXBo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLSewM7CuX528F7zMTl3JIek-UwMrgNiW_JgorC644vEFLlljtg/viewform" target="\_blank">18.4</a></td>
</tr>
<tr>
<td><strong>18.5</strong> <br>Discussing the pitfalls of using squared loss with logistic regression.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/NmxwIgbMhgc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLSd17JvL0Vn6eJgO2RGNgQSCKKrsS600QaqW6glpmrkSjaT5zQ/viewform" target="\_blank">18.5</a></td>
</tr>
<tr>
<td><strong>18.6</strong> <br>Introducing cross-entropy loss, as a better alternative to squared loss for logistic regression.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/zFXrM6Lmlxk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLSfUcNZQMJJj6dcQTQZpCfqhMWCH8HeC3yYEZWXaeM86jwlbAg/viewform" target="\_blank">18.6</a></td>
</tr>
<tr>
<td><strong>18.7</strong> <br>Using maximum likelihood estimation to arrive at cross-entropy loss.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/3wqXRQzJBpE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLSd4QeQPhKB8xY8xcxNNvhh5pPG2o0IPH20KH7kfYTh6oSaMzQ/viewform" target="\_blank">18.7</a></td>
</tr>
<tr>
<td><strong>18.8</strong> <br>Demo of using scikit-learn to fit a logistic regression model. An overview of what's coming next.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/PWm1KYNFkSM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLSd4ld6q87aiV9HT2pyMp675a0IyQGpPcADDliUujvMBWmtCNw/viewform" target="\_blank">18.8</a></td>
</tr>
