{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 19 â€“ Data 100, Fall 2020\n",
    "\n",
    "by Suraj Rampure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from scipy.optimize import minimize\n",
    "import sklearn.linear_model as lm\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (4, 4)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['lines.linewidth'] = 3\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding in Logistic Regression\n",
    "\n",
    "So far, our logistic regression model predicts probabilities. But we originally set out on a mission to create a classifier. How can we use our predicted probabilities to create classifications?\n",
    "\n",
    "Let's get back the NBA data we had last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('nba.csv')\n",
    "df[\"WON\"] = df[\"WL\"]\n",
    "df[\"WON\"] = df[\"WON\"].replace(\"W\", 1)\n",
    "df[\"WON\"] = df[\"WON\"].replace(\"L\", 0)\n",
    "one_team = df.groupby(\"GAME_ID\").first()\n",
    "opponent = df.groupby(\"GAME_ID\").last()\n",
    "games = one_team.merge(opponent, left_index = True, right_index = True, suffixes = [\"\", \"_OPP\"])\n",
    "games[\"FG_PCT_DIFF\"] = games[\"FG_PCT\"] - games[\"FG_PCT_OPP\"]\n",
    "games[\"PF_DIFF\"] = games[\"PF\"] - games[\"PF_OPP\"]\n",
    "games['WON'] = games['WL'].replace('L', 0).replace('W', 1)\n",
    "games = games[['TEAM_NAME', 'MATCHUP', 'WON', 'FG_PCT_DIFF', 'PF_DIFF']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call this model `basic_model` since it only has one feature. (Eventually, we will use more features.) \n",
    "\n",
    "It is the same model we fit in the last lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = lm.LogisticRegression(penalty = 'none', fit_intercept = False, solver = 'lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model.fit(games[['FG_PCT_DIFF']], games['WON'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can use `.predict_proba` to get the predicted probabilities for each class under our logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model.predict_proba(games[['FG_PCT_DIFF']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot our model, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sorted = np.array(games['FG_PCT_DIFF'].sort_values()).reshape(len(games), 1)\n",
    "basic_model_ps_sorted = basic_model.predict_proba(x_sorted)[:, 1]\n",
    "\n",
    "points = go.Scatter(name = 'true observations', \n",
    "                   x = games['FG_PCT_DIFF'],\n",
    "                   y = games['WON'],\n",
    "                   mode = 'markers',\n",
    "                   marker={'opacity':0.5})\n",
    "\n",
    "lr_line = go.Scatter(name = 'Logistic Regression model', \n",
    "                    x = x_sorted.flatten(),\n",
    "                    y = basic_model_ps_sorted)\n",
    "\n",
    "fig = go.Figure([points, lr_line])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to apply a **threshold** to convert these probabilities into class labels (1 or 0). We can define a function that takes in probabilities, and returns their values, thresholded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_at_threshold(prob, threshold):\n",
    "    return np.where(prob >= threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_at_threshold(np.array([0.2, 0.5, 0.8]), 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the resulting predictions for three different (arbitrarily chosen thresholds): $T = 0.25, T = 0.5, T = 0.75$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = [0.25, 0.5, 0.75]\n",
    "\n",
    "thresholds_df = pd.DataFrame()\n",
    "basic_model_ps = basic_model.predict_proba(games[['FG_PCT_DIFF']])[:, 1]\n",
    "thresholds_df['P(Y = 1 | x)'] = basic_model_ps\n",
    "\n",
    "for t in ts:\n",
    "    thresholds_df[f'T = {t}'] = predict_at_threshold(basic_model_ps, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 4))\n",
    "for i, t in enumerate(ts):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.xlim(-0.2, 0.2)\n",
    "    plt.plot(x_sorted, basic_model_ps_sorted);\n",
    "    plt.plot([-0.3, 0.3], [t, t], color = 'r')\n",
    "    plt.title(f'T = {t}')\n",
    "    plt.xlabel('FG_PCT_DIFF')\n",
    "    plt.ylabel(r'$P(Y = 1 | x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase our threshold, fewer and fewer observations are classified as being 1.\n",
    "\n",
    "What about models with more than one feature? Well, if our model has any more than two features, we can't really visualize it. But with two (or three, if we include an intercept term), we can visualize it just fine.\n",
    "\n",
    "Let's now use `FG_PCT_DIFF` and `PF_DIFF` to predict whether or not a team will win. `PF_DIFF` is the difference in the number of personal fouls your team and the other team were charged with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_model = lm.LogisticRegression(penalty = 'none', fit_intercept = True, solver = 'lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_model.fit(games[['FG_PCT_DIFF', 'PF_DIFF']], games['WON'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results mean that our model makes predictions as\n",
    "\n",
    "$$\\hat{y} = P(Y = 1 | x) = \\sigma(0.035 + 34.71 \\cdot \\text{FG_PCT_DIFF} - 0.160 \\cdot \\text{PF_DIFF})$$\n",
    "\n",
    "Interpreting these coefficients, it seems to mean that having a positive `FG_PCT_DIFF` really helps your team's chances of winning, and having a positive `PF_DIFF` hurts your team's chances of winning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games['FG_PCT_DIFF'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games['PF_DIFF'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 50 # increase for better resolution, but it will run more slowly. \n",
    "\n",
    "# if (num_points <= 100):\n",
    "\n",
    "uvalues = np.linspace(-0.2, 0.2, num_points)\n",
    "vvalues = np.linspace(-20, 20, num_points)\n",
    "(u,v) = np.meshgrid(uvalues, vvalues)\n",
    "thetas = np.vstack((u.flatten(),v.flatten()))\n",
    "\n",
    "probs = [better_model.predict_proba(t.reshape(1, 2))[0, 1] for t in thetas.T]\n",
    "\n",
    "probs_surface = go.Surface(x=u, y=v, z=np.reshape(probs, u.shape))\n",
    "\n",
    "# opt_point = go.Scatter3d(x = [ahat], y = [bhat], z = [mse_for_height_model((ahat, bhat))],\n",
    "#             mode = 'markers', name = 'optimal parameters',\n",
    "#             marker=dict(size=10, color='gold'))\n",
    "\n",
    "fig = go.Figure(data=[probs_surface])\n",
    "# fig.add_trace(opt_point)\n",
    "\n",
    "fig.update_layout(scene = dict(\n",
    "    xaxis_title = \"FG_PCT_DIFF\",\n",
    "    yaxis_title = \"PF_DIFF\",\n",
    "    zaxis_title = \"P(Y = 1 | x)\"))\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize thresholds here, too. It'll be a little inconvenient to this three times, so let's just show $T = 0.3$ (chosen arbitrarily):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 50 # increase for better resolution, but it will run more slowly. \n",
    "\n",
    "# if (num_points <= 100):\n",
    "\n",
    "uvalues = np.linspace(-0.2, 0.2, num_points)\n",
    "vvalues = np.linspace(-20, 20, num_points)\n",
    "(u,v) = np.meshgrid(uvalues, vvalues)\n",
    "thetas = np.vstack((u.flatten(),v.flatten()))\n",
    "\n",
    "probs = [better_model.predict_proba(t.reshape(1, 2))[0, 1] for t in thetas.T]\n",
    "\n",
    "probs_surface = go.Surface(x=u, y=v, z=np.reshape(probs, u.shape))\n",
    "\n",
    "threshold_surface = go.Surface(x = u, y = v, z = 0.3 * np.ones_like(u), opacity = 0.8, name = 'T = 0.3')\n",
    "\n",
    "fig = go.Figure(data=[probs_surface, threshold_surface])\n",
    "\n",
    "fig.update_layout(scene = dict(\n",
    "    xaxis_title = \"FG_PCT_DIFF\",\n",
    "    yaxis_title = \"PF_DIFF\",\n",
    "    zaxis_title = \"P(Y = 1 | x)\"))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's compute the **accuracy** of our `better_model`, when using $T = 0.5$ (the default in `scikit-learn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting our predicted 1s and 0s\n",
    "y_pred = better_model.predict(games[['FG_PCT_DIFF', 'PF_DIFF']])\n",
    "\n",
    "# The proportion of times we classified correctly is the accuracy\n",
    "better_model_accuracy = np.mean(y_pred == games['WON'])\n",
    "\n",
    "better_model_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per usual, `scikit-learn` can do this for us. The `.score` method of a `LogisticRegression` classifier gives us the accuracy of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_model.score(games[['FG_PCT_DIFF', 'PF_DIFF']], games['WON'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "Our good old friend `scikit-learn` has an in-built confusion matrix method (of course it does)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Be careful â€“ confusion_matrix takes in y_true as the first parameter and y_pred as the second.\n",
    "# Don't mix these up!\n",
    "cm = confusion_matrix(games['WON'], y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(games['WON'], y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt = 'd', cmap = 'Blues', annot_kws = {'size': 16})\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "\n",
    "We can also compute the number of TP, TN, FP, and TN for our classifier, and hence its precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = np.sum((y_pred == 1) & (games['WON'] == 1))\n",
    "tn = np.sum((y_pred == 0) & (games['WON'] == 0))\n",
    "fp = np.sum((y_pred == 1) & (games['WON'] == 0))\n",
    "fn = np.sum((y_pred == 0) & (games['WON'] == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers match what we see in the confusion matrix above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = tp / (tp + fp)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = tp / (tp + fn)\n",
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to remember that these values are all for the threshold of $T = 0.5$, which is `scikit-learn`'s default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy vs. threshold, Precision vs. threshold, Recall vs. threshold\n",
    "\n",
    "We already have a function `predict_at_threshold`, which takes in a list of probabilities and a threshold and computes the predicted labels (1s and 0s) at that threshold.\n",
    "\n",
    "Let's also create our own `accuracy_custom_threshold` function, that takes in\n",
    "- a `model`\n",
    "- a true `y`\n",
    "- an `x`\n",
    "- a `threshold`\n",
    "\n",
    "And returns the accuracy of the predictions for our `model` with that threshold. Note that this function is not specific to our `better_model`, so we can use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_custom_threshold(model, X, y, threshold):\n",
    "    predicted_probabilities = model.predict_proba(X)[:, 1]\n",
    "    y_pred = predict_at_threshold(predicted_probabilities, threshold)\n",
    "    return np.mean(y_pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_custom_threshold(better_model, games[['FG_PCT_DIFF', 'PF_DIFF']], games['WON'], 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same accuracy that `model.score` gave us before, so we should be good to go.\n",
    "\n",
    "Let's plot what this accuracy looks like, for various thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 100)\n",
    "accs = [accuracy_custom_threshold(better_model,\n",
    "                                  games[['FG_PCT_DIFF', 'PF_DIFF']], \n",
    "                                  games['WON'],\n",
    "                                  t) for t in thresholds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x=thresholds, y=accs)\n",
    "fig.update_xaxes(title=\"Threshold\")\n",
    "fig.update_yaxes(title=\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make similar helper functions for precision and recall, and make similar plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_custom_threshold(model, X, y, threshold):\n",
    "    predicted_probabilities = model.predict_proba(X)[:, 1]\n",
    "    y_pred = predict_at_threshold(predicted_probabilities, threshold)\n",
    "    tp = np.sum((y_pred == 1) & (y == 1))\n",
    "    fp = np.sum((y_pred == 1) & (y == 0))\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "precision_custom_threshold(better_model, games[['FG_PCT_DIFF', 'PF_DIFF']], games['WON'], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_custom_threshold(model, X, y, threshold):\n",
    "    predicted_probabilities = model.predict_proba(X)[:, 1]\n",
    "    y_pred = predict_at_threshold(predicted_probabilities, threshold)\n",
    "    tp = np.sum((y_pred == 1) & (y == 1))\n",
    "    fn = np.sum((y_pred == 0) & (y == 1))\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "recall_custom_threshold(better_model, games[['FG_PCT_DIFF', 'PF_DIFF']], games['WON'], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precs = [precision_custom_threshold(better_model,\n",
    "                                  games[['FG_PCT_DIFF', 'PF_DIFF']], \n",
    "                                  games['WON'],\n",
    "                                  t) for t in thresholds]\n",
    "\n",
    "recs = [recall_custom_threshold(better_model,\n",
    "                                  games[['FG_PCT_DIFF', 'PF_DIFF']], \n",
    "                                  games['WON'],\n",
    "                                  t) for t in thresholds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(name = 'Precision', x = thresholds, y = precs))\n",
    "fig.add_trace(go.Scatter(name = 'Recall', x = thresholds, y = recs))\n",
    "fig.update_xaxes(title=\"Threshold\")\n",
    "fig.update_yaxes(title=\"Proportion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curves\n",
    "\n",
    "We can also plot precision vs. recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x=recs, y=precs, hover_name=thresholds)\n",
    "fig.update_xaxes(title=\"Recall\")\n",
    "fig.update_yaxes(title=\"Precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` can also do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, threshold = precision_recall_curve(games['WON'], \n",
    "                                                              better_model.predict_proba(games[['FG_PCT_DIFF', 'PF_DIFF']])[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.line(x=recall[:-1], y = precision[:-1], hover_name=threshold)\n",
    "fig.update_xaxes(title=\"Recall\")\n",
    "fig.update_yaxes(title=\"Precision\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing, just a bit more granular.\n",
    "\n",
    "We can also plot a ROC curve (explained in lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(games['WON'], \n",
    "                               better_model.predict_proba(games[['FG_PCT_DIFF', 'PF_DIFF']])[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x=fpr, y = tpr, hover_name=threshold)\n",
    "fig.update_xaxes(title=\"False Positive Rate\")\n",
    "fig.update_yaxes(title=\"True Positive Rate\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a random classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(games['WON'], np.random.uniform(0, 1, len(games['WON'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x=fpr, y = tpr, hover_name=threshold)\n",
    "fig.update_xaxes(title=\"False Positive Rate\")\n",
    "fig.update_yaxes(title=\"True Positive Rate\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 5))\n",
    "sns_cmap = ListedColormap(np.array(sns.color_palette())[0:2, :])\n",
    "xx, yy = np.meshgrid(np.linspace(-0.4, 0.4, 2000), np.linspace(-20, 20, 2000))\n",
    "Z_string = predict_at_threshold(\n",
    "    better_model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1],\n",
    "    0.3)\n",
    "categories, Z_int = np.unique(Z_string, return_inverse = True)\n",
    "Z_int = Z_int.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z_int, cmap = sns_cmap)\n",
    "sns.scatterplot(data = games, x = 'FG_PCT_DIFF', y = 'PF_DIFF', hue = 'WON')\n",
    "plt.title('Decision Boundary for NBA Model');\n",
    "# plt.savefig('withoutpts.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(Y = 1 | x) = \\sigma(0.035 + 34.705 \\cdot \\text{FG_PCT_DIFF} - 0.160 \\cdot \\text{PF_DIFF})$$\n",
    "\n",
    "$$\\sigma(0.035 + 34.705 \\cdot \\text{FG_PCT_DIFF} - 0.160 \\cdot \\text{PF_DIFF}) = 0.3$$\n",
    "\n",
    "$$\\boxed{0.035 + 34.705 \\cdot \\text{FG_PCT_DIFF} - 0.160 \\cdot \\text{PF_DIFF} = \\sigma^{-1}(0.3)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (1/0.160) * (0.035 + 34.705*x - np.log(0.3 / 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 5))\n",
    "sns_cmap = ListedColormap(np.array(sns.color_palette())[0:2, :])\n",
    "xx, yy = np.meshgrid(np.linspace(-0.4, 0.4, 2000), np.linspace(-20, 20, 2000))\n",
    "Z_string = predict_at_threshold(\n",
    "    better_model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1],\n",
    "    0.3)\n",
    "categories, Z_int = np.unique(Z_string, return_inverse = True)\n",
    "Z_int = Z_int.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z_int, cmap = sns_cmap)\n",
    "# sns.scatterplot(data = games, x = 'FG_PCT_DIFF', y = 'PF_DIFF', hue = 'WON')\n",
    "plt.title('Decision Boundary for NBA Model');\n",
    "\n",
    "xs = np.linspace(-0.4, 0.4, 50)\n",
    "ys = f(xs)\n",
    "plt.plot(xs, ys, color = 'black', linewidth=5)\n",
    "plt.ylim(-20, 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Separability\n",
    "\n",
    "Consider the following toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_toy = np.array([-1, 1])\n",
    "y_toy = np.array([1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_toy.flatten(), y_toy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the mean cross-entropy loss surface for this toy dataset, and a single feature model $\\hat{y} = \\sigma(\\theta x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = lambda t: 1 / (1 + np.e**(-t))\n",
    "\n",
    "def cross_entropy(y, yhat):\n",
    "    return - y * np.log(yhat) - (1 - y) * np.log(1 - yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mce_loss_single_arg_toy(theta):\n",
    "    y_hat = sigma(X_toy * theta)\n",
    "    return np.mean(cross_entropy(y_toy, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.linspace(-30, 30, 100)\n",
    "plt.plot(thetas, [mce_loss_single_arg_toy(theta) for theta in thetas], color = 'green')\n",
    "plt.ylabel(r'Mean Cross-Entropy($\\theta$)')\n",
    "plt.xlabel(r'$\\theta$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But using **regularization**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mce_regularized_loss_single_arg_toy(theta, reg):\n",
    "    return mce_loss_single_arg_toy(theta) + reg * theta**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.linspace(-30, 30, 100)\n",
    "plt.plot(thetas, [mce_regularized_loss_single_arg_toy(theta, 0.1) for theta in thetas], color = 'green')\n",
    "plt.ylabel(r'Mean Cross-Entropy($\\theta$)')\n",
    "plt.xlabel(r'$\\theta$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another example dataset to illustrate linear separability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following data is linearly separable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = iris[iris['species'] != 'virginica'],\n",
    "               x = 'petal_length',\n",
    "               y = 'petal_width',\n",
    "               hue = 'species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the following here is not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = iris[iris['species'] != 'setosa'],\n",
    "               x = 'petal_length',\n",
    "               y = 'petal_width',\n",
    "               hue = 'species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Demo\n",
    "\n",
    "As a demo of the model fitting process from end-to-end, let's fit a regularized LogisticRegression model on the `iris` data, while performing a train/test split.\n",
    "\n",
    "Let's try and predict the species of our `iris`. But, there are three possible values of `species` right now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['species'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's create a new column `is_versicolor` that is 1 if the iris is a versicolor, and a 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['is_versicolor'] = (iris['species'] == 'versicolor').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_train, iris_test = train_test_split(iris, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at the coefficients if we fit without regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model_no_reg = lm.LogisticRegression(penalty = 'none', solver = 'lbfgs')\n",
    "iris_model_no_reg.fit(iris_train[cols], iris_train['is_versicolor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model_no_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model_no_reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit with regularization, using the default value of `C` (the regularization hyperparameter in `scikit-learn`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model_reg = lm.LogisticRegression(penalty = 'l2', solver = 'lbfgs')\n",
    "iris_model_reg.fit(iris_train[cols], iris_train['is_versicolor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model_reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the coefficients on the regularized model are significantly smaller.\n",
    "\n",
    "Let's evaluate the training and testing accuracy of both models â€“ regularized and not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model_no_reg.score(iris_train[cols], iris_train['is_versicolor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model_reg.score(iris_train[cols], iris_train['is_versicolor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the regularized model performs worse on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model_no_reg.score(iris_test[cols], iris_test['is_versicolor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model_reg.score(iris_test[cols], iris_test['is_versicolor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, they both happened to perform the same on the test data. Interesting!\n",
    "\n",
    "**Question:** What did we forget to do here (that we should always do when performing regularized linear or logistic regression)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
