{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 11: Introduction to Modeling\n",
    "\n",
    "## Data 100, Fall 2020\n",
    "\n",
    "**Fernando Perez and Suraj Rampure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_loss(y, yhat):\n",
    "    return (y - yhat)**2\n",
    "\n",
    "def l1_loss(y, yhat):\n",
    "    return np.abs(y - yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy = np.array([20, 21, 22, 29, 33])\n",
    "single_obs = toy[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the $L_2$ loss for a **single** observation. We'll plot the $L_2$ loss for the first observation; since $y_1 = 20$, we'll be plotting\n",
    "\n",
    "$$L_2(20, \\theta) = (20 - \\theta)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.linspace(10, 30, 1000)\n",
    "l2_loss_single_obvs = l2_loss(single_obs, thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thetas, l2_loss_single_obvs);\n",
    "plt.xlabel(r'$\\theta$');\n",
    "plt.ylabel(r'$L(20, \\theta) = (20 - \\theta)^2$');\n",
    "plt.savefig('l2_single_obvs.png', bbox_inches = 'tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the loss for a single observation is minimized by that observation itself (i.e. when $\\theta = 20$, the above loss is minimized).\n",
    "\n",
    "Let's now compute the average loss over all of our toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_loss(loss, est, data):\n",
    "    return np.mean(np.array([loss(est, y_obs) for y_obs in data]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.linspace(10, 40, 1000)\n",
    "l2_loss_thetas = avg_loss(l2_loss, thetas, toy)\n",
    "\n",
    "plt.plot(thetas, l2_loss_thetas, color = 'green');\n",
    "plt.xlabel(r'$\\theta$');\n",
    "plt.ylabel(r'MSE across all points');\n",
    "plt.savefig('average_l2_loss.png', bbox_inches = 'tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explicit expression for the MSE here is\n",
    "\n",
    "$$R(\\theta) = \\frac{1}{5} \\big((20 - \\theta)^2 + (21 - \\theta)^2 + (22 - \\theta)^2 + (29 - \\theta)^2 + (33 - \\theta)^2\\big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note; the shape looks similar, but the minimizing value of $\\theta$ is now shifted. It appears to be closer to 25 (which you may notice as the mean of[20, 21, 22, 29, 33])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now do the same, but for L1 loss. For our first observation, the $L_1$ loss is\n",
    "\n",
    "$$L_1(20, \\theta) = |20 - \\theta|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.linspace(10, 30, 1000)\n",
    "l1_loss_single_obvs = l1_loss(single_obs, thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thetas, l1_loss_single_obvs);\n",
    "plt.xlabel(r'$\\theta$');\n",
    "plt.ylabel(r'$L(20, \\theta) = |20 - \\theta|$');\n",
    "plt.savefig('l1_single_obvs.png', bbox_inches = 'tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again this is centered on the observation itself, 20.\n",
    "\n",
    "Averaging across all of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.linspace(10, 40, 1000)\n",
    "l1_loss_thetas = avg_loss(l1_loss, thetas, toy)\n",
    "\n",
    "plt.plot(thetas, l1_loss_thetas, color = 'green');\n",
    "plt.xlabel(r'$\\theta$');\n",
    "plt.ylabel(r'MAE across all points');\n",
    "plt.savefig('average_l1_loss.png', bbox_inches = 'tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explicit expression for the MAE here is\n",
    "\n",
    "$$R(\\theta) = \\frac{1}{5} \\big(|20 - \\theta| + |21 - \\theta| + |22 - \\theta| + |29 - \\theta| + |33 - \\theta|\\big)$$\n",
    "\n",
    "Note, it is pointy, and not smooth like the MSE. It also doesn't exactly look like a simple absolute value curve. It's a combination of several absolute value functions.\n",
    "\n",
    "As we show in the lecture, the minimizing value of the MAE here is $\\theta = 22$, as that is the median of our observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 2))\n",
    "plt.plot([18, 35], [5, 5], color = 'grey')\n",
    "plt.scatter([20, 21, 29, 33], [5, 5, 5, 5])\n",
    "plt.scatter([22], [5], color = 'red')\n",
    "\n",
    "plt.ylim(4.5, 5.5)\n",
    "plt.xticks(np.arange(18, 36, 1))\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we instead had an even number of points. There wouldn't be a unique median! Let's see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy2 = np.array([20, 21, 22, 29, 33, 35])\n",
    "thetas = np.linspace(10, 40, 1000)\n",
    "l1_loss_thetas = avg_loss(l1_loss, thetas, toy2)\n",
    "\n",
    "plt.plot(thetas, l1_loss_thetas, color = 'green');\n",
    "plt.xlabel(r'$\\theta$');\n",
    "plt.ylabel(r'MAE across all points');\n",
    "plt.savefig('average_l1_loss_even.png', bbox_inches = 'tight');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
