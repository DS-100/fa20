{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 14, Part 3 â€“ Data 100, Fall 2020\n",
    "\n",
    "**by Joseph Gonzalez**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitfalls of Feature Engineering \n",
    "\n",
    "In this notebook, we present three issues you might encounter in feature engineering: \n",
    "\n",
    "1. **Redundant Features:** When duplicate features are introduced this can create problems when solving the normal equation.  \n",
    "2. **Too many Features:** When you add more features than you have data the solution to the normal equations becomes underdetermined.\n",
    "3. **Overfitting:** When improvements in the model or features introduced to fit the model to the available data result in poor predictions on new data.  \n",
    "\n",
    "The third issue, **overfitting**, will also be the focus on the next several lectures and is the fundamental challenge in modeling. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports \n",
    "\n",
    "In this notebook we will be using the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=True, sharing=False, theme='ggplot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Data and Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem we will use a very simple toy dataset to help illustrate where things will fail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/train.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scatter = go.Scatter(x=data[\"X\"], y=data[\"Y\"], name=\"data\", mode=\"markers\")\n",
    "go.Figure([data_scatter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Basic Linear Model\n",
    "\n",
    "For this notebook, we are going to implement the solution to the normal equations ourselves so that we can observe the resulting numerical issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import solve\n",
    "\n",
    "def fit(X, Y):\n",
    "    return solve(X.T @ X, X.T @ Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to add a ones column to our feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ones_column(data):\n",
    "    n,_ = data.shape\n",
    "    return np.hstack([np.ones((n,1)), data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the original $\\mathbb{X}$ and $\\mathbb{Y}$ matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['X']].to_numpy()\n",
    "Y = data[['Y']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_line(X):\n",
    "    return add_ones_column(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_line = phi_line(X)\n",
    "Phi_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving for $\\hat{\\theta}$ using our Phi matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hat_line = fit(Phi_line, Y)\n",
    "theta_hat_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions at each of the original X values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, X):\n",
    "    return X @ theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Y_hat'] = predict(theta_hat_line, Phi_line)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(data_scatter)\n",
    "basic_ols = go.Scatter(x=data['X'], y=data['Y_hat'], mode=\"lines\", name=\"$y=mx+b$\")\n",
    "fig.add_trace(basic_ols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Average Squared Loss \n",
    "\n",
    "We compute the squared loss so we can compare with it later:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_squared_loss(Y, Y_hat):\n",
    "    return np.mean((Y - Y_hat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_line = average_squared_loss(Y, data[['Y_hat']].to_numpy())\n",
    "loss_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Model Class\n",
    "\n",
    "Because we are going to make a number of models.  We can define a simple helper class to maintain our model. Don't worry too much about how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel:\n",
    "    def __init__(self, phi):\n",
    "        self.phi = phi\n",
    "    def fit(self, X, Y):\n",
    "        Phi = self.phi(X)\n",
    "        self.theta_hat = solve(Phi.T @ Phi, Phi.T @ Y)\n",
    "        return self.theta_hat\n",
    "    def predict(self, X):\n",
    "        Phi = self.phi(X)\n",
    "        return Phi @ self.theta_hat\n",
    "    def average_loss(self, X, Y):\n",
    "        return np.mean((Y - self.predict(X))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_line = LinearModel(phi_line)\n",
    "model_line.fit(X,Y)\n",
    "model_line.average_loss(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## The Issue with Redundant Features\n",
    "\n",
    "Redundant features occur when features are linear combinations of existing features.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we were to duplicate a feature.  For example, suppose we copied the original value of $X$. Here we will make the features matrix:\n",
    "\n",
    "\\begin{align}\n",
    "\\Phi = \\left[1, X, X \\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_dup(x):\n",
    "    return add_ones_column(np.hstack([X, X]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_dup = phi_dup(X)\n",
    "Phi_dup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that any feature that is a linear combination of other features is problematic.  If we attempt to solve for the new $\\hat{\\theta}$ for this revised model we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    theta_hat_dup = solve(Phi_dup.T @ Phi_dup, Phi_dup.T @ Y)\n",
    "    print(theta_hat_dup)\n",
    "except np.linalg.LinAlgError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above fails because the $\\Phi^T \\Phi$ matrix is no longer full rank and we cannot solve for $\\theta$ in the system of linear equations:\n",
    "\n",
    "$$\n",
    "\\Phi^T \\Phi \\theta = \\Phi^T Y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually check the rank of this 3x3 matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_dup.T @ Phi_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import matrix_rank\n",
    "matrix_rank(Phi_dup.T @ Phi_dup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are actually infinitely many possible optimal solutions.  We can consider the two extreme parameterizations where we ignore (set the coefficient to zero) one or the other redundant feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_a = np.array([[theta_hat_line[0,0], theta_hat_line[1,0], 0]]).T\n",
    "theta_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_b = np.array([[theta_hat_line[0,0], 0, theta_hat_line[1,0]]]).T\n",
    "theta_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can examine the squared loss for many possible convex combinations of the two parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convex_comb(theta_a, theta_b, alpha):\n",
    "    return alpha * theta_a + (1-alpha) * theta_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[average_squared_loss(predict(convex_comb(theta_a, theta_b, a), Phi_dup), Y) \n",
    " for a in np.linspace(0, 1, 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are equivalent up to numerical precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Too Many Features\n",
    "\n",
    "In the basic formulation of least squares regression we assumed that there was far more data than features.  What if this is not the case?  Let's examine what happens when we add too many features.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate this, let's introduce a feature that is 1 when we are near a particular location and decreases quickly as we move away.  This is actually called a **radial basis function**.  These are used when you might imagine something happens near a particular value.  For example, if you were trying to predict when someone asks their Smart Alarm to Snooze you might place radial basis functions around common hours (e.g., 6AM, 7AM, and 8AM).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block of code creates a single radial basis function feature. **Don't worry too much about the math behind radial basis functions; they are primarily presented as an example.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_feature(loc, x, beta=0.1):\n",
    "    return np.exp(-((loc - x)**2)/beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there is an additional $\\beta$ **hyper-parameter**.   This hyper-parameter determines the width of each bump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-3,3,300)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=z, y=rbf_feature(0, z, beta=0.1), mode=\"lines\", name=r\"$\\beta=0.1$\"))\n",
    "fig.add_trace(go.Scatter(x=z, y=rbf_feature(0, z, beta=1.), mode=\"lines\", name=r\"$\\beta=1$\"))\n",
    "fig.add_trace(go.Scatter(x=z, y=rbf_feature(0, z, beta=2.), mode=\"lines\", name=r\"$\\beta=2$\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can place these radial basis function features at different locations along the x-axis to implement multiple features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_rbf(locations, X):\n",
    "    return np.hstack([rbf_feature(loc, X) for loc in locations])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we start by placing four bumps at a few locations along the x-axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_bump_locations = [-2, -1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Phi_rbf4 = phi_rbf(four_bump_locations, X)\n",
    "Phi_rbf4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rbf4 = LinearModel(lambda X: phi_rbf(four_bump_locations, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rbf4.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the data and computing the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rbf4.average_loss(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize this model we will want to make predictions at more locations along the x-axis. We will call all these locations `x_test` (that is where we want to test the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.expand_dims(np.linspace(X.min()-1, X.max()+1, 200), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure([data_scatter])\n",
    "\n",
    "# Redraw the line plot so it extends out beyond data.\n",
    "line_plot = go.Scatter(\n",
    "    x=X_test.flatten(), \n",
    "    y=model_line.predict(X_test).flatten(),\n",
    "    mode=\"lines\", name=\"$y=mx+b$\")\n",
    "fig.add_trace(line_plot)\n",
    "\n",
    "# Draw the new model\n",
    "phi_rbf4_plot = go.Scatter(x=X_test.flatten(), \n",
    "                           y=model_rbf4.predict(X_test).flatten(),\n",
    "                           mode=\"lines\", name=\"4 RBF Features\")\n",
    "fig.add_trace(phi_rbf4_plot)\n",
    "\n",
    "# Draw the Bumps\n",
    "for b in four_bump_locations:\n",
    "    fig.add_trace(go.Scatter(x=X_test.flatten(), \n",
    "                             y=rbf_feature(b, X_test).flatten(), \n",
    "                             name=\"Bump @ \" + str(b),\n",
    "                             line=dict(dash=\"longdash\")))\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the Four RBF function model doesn't really follow the linear trend in the data.  Let's add a the line features as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_line_rbf(locations, X):\n",
    "    return np.hstack([phi_line(X), phi_rbf(locations, X)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we create the transformed features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phi_line_rbf(four_bump_locations, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_line_rbf4 = LinearModel(lambda X: phi_line_rbf(four_bump_locations, X))\n",
    "model_line_rbf4.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the data and computing the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_line_rbf4.average_loss(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model at `X_test` locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure([data_scatter, line_plot, phi_rbf4_plot])\n",
    "# Draw the new model\n",
    "phi_line_rbf4_plot = go.Scatter(x=X_test.flatten(), \n",
    "                                y=model_line_rbf4.predict(X_test).flatten(), \n",
    "                                mode=\"lines\", name=\"Line + 4 RBF Features\")\n",
    "fig.add_trace(phi_line_rbf4_plot)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that the newest model which combines RBF features and the line features better follows the trends in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to add many more features.\n",
    "\n",
    "Let's place **20** evenly spaced bumps between -3 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_bump_locations = np.linspace(-3, 3, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the shape of our new matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_line_rbf20 = phi_line_rbf(many_bump_locations, X)\n",
    "Phi_line_rbf20.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit these 9 data points to a 22 feature linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_line_rbf20 = LinearModel(lambda X: phi_line_rbf(many_bump_locations, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_line_rbf20.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked!? Should it have worked?  Recall we are solving:\n",
    "\n",
    "$$\n",
    "\\Phi^T \\Phi \\hat{\\theta} = \\Phi^T \\mathbb{Y} \n",
    "$$\n",
    "\n",
    "There should be more than one solution since we have way more variables than we have equations.  Put another way the inverse operation here should not be well defined:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}   = \\left(\\Phi^T \\Phi \\right)^{-1} \\Phi^T \\mathbb{Y} \n",
    "$$\n",
    "\n",
    "We can examine the rank and inverse of this matrix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Phi_line_rbf20.T @ Phi_line_rbf20\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_rank(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy uses iterative algorithms to invert matrices and does not necessarily check that a solution was found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "inv(A) @ A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparison if we look at something that was more appropriate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = model_rbf4.phi(X).T @ model_rbf4.phi(X)\n",
    "inv(B) @ B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still use the model to make predictions since the algorithm did return a $\\hat{\\theta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure([data_scatter, line_plot, phi_rbf4_plot, phi_line_rbf4_plot])\n",
    "# Draw the new model\n",
    "phi_line_rbf20_plot = go.Scatter(\n",
    "    x=X_test.flatten(), \n",
    "    y=model_line_rbf20.predict(X_test).flatten(), \n",
    "    mode=\"lines\", name=\"Line + 20 RBF Features\")\n",
    "fig.add_trace(phi_line_rbf20_plot)\n",
    "fig.update_yaxes(range=[-5,10])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that despite the solution to the linear systems being not well posed, we were able to find a model that does appear to minimize the loss.  When we cover regularization, we will return to how we can effectively train models when there are more features than data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "One of the big challenges is machine learning and statistics is building models that generalize beyond the data.  Unfortunately, when building our models the data is all we have and we design our models to reflect the patterns in our data.  However, it is possible to go too far. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our earlier example with with a more reasonable number of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bump_locations7 = [-2, -1.5, -1, -0.5, 0, 0.5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Phi_line_rbf7 = phi_line_rbf(bump_locations7, X)\n",
    "Phi_line_rbf7.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have 9 data points and 9 features (2 linear model features and 7 RBF features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_line_rbf7 = LinearModel(lambda X: phi_line_rbf(bump_locations7, X))\n",
    "model_line_rbf7.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the data and computing the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_line_rbf7.average_loss(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure([data_scatter, line_plot, phi_rbf4_plot, phi_line_rbf4_plot])\n",
    "# Draw the new model\n",
    "phi_line_rbf7_plot = go.Scatter(\n",
    "    x=X_test.flatten(), \n",
    "    y=model_line_rbf7.predict(X_test).flatten(), \n",
    "    mode=\"lines\", name=\"Line + 7 RBF Features\")\n",
    "fig.add_trace(phi_line_rbf7_plot)\n",
    "fig.update_yaxes(range=[-10,20])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did we minimize the error?  Let's compare the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_line, model_rbf4, model_line_rbf4, model_line_rbf7]\n",
    "model_names = [\"Line\", \"4 RBF\", \"Line + 4 RBF\", \"Line + 7 RBF\"]\n",
    "train_loss = [m.average_loss(X, Y) for m in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go.Figure([go.Bar(x=model_names, y=train_loss)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we minimized the training loss with the latest model it may not have been our \"best\" model.  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining New (Test) Data \n",
    "\n",
    "To see why the models we developed above may actually be bad models despite minimizing the loss, we collect some more data from the same underlying process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"data/test.csv\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting this new data (in red) on top of the old data we see that while the more complex RBF model fit the original data perfectly, it does not fit the new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_scatter = go.Scatter(name = \"Test Data\", x = test_data['X'], y = test_data['Y'], \n",
    "                       mode = 'markers', marker=dict(symbol=\"cross\", color=\"red\"))\n",
    "fig = go.Figure([data_scatter, test_data_scatter, line_plot, phi_rbf4_plot, phi_line_rbf4_plot, phi_line_rbf7_plot])\n",
    "fig.update_yaxes(range=[-10,20])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we perform on the new data? Computing the loss from before and after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data[[\"X\"]].to_numpy()\n",
    "Y_test = test_data[[\"Y\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = [m.average_loss(X_test, Y_test) for m in models]\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure([go.Bar(x=model_names, y=train_loss, name=\"Train Loss\"), \n",
    "           go.Bar(x=model_names, y=test_loss, name=\"Test Loss\")])\n",
    "fig.update_yaxes(range=[-0.1,15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's happening: _Overfitting_\n",
    "\n",
    "As we increase the expressiveness of our model we begin to **overfit** to the variability in our training data.  That is we are learning patterns that do not **generalize** beyond our training dataset\n",
    "\n",
    "**Overfitting** is a key challenge in machine learning and statistical inference.  At it's core is a fundamental trade-off between **bias** and **variance**: _the desire to explain the training data and yet be robust to variation in the training data_.\n",
    "\n",
    "We will study the **bias-variance** trade-off in future lectures but for now we will focus on the trade-off between underfitting and overfitting:\n",
    "\n",
    "<img src=\"images/under_over_fitting.png\" width=\"500px\">\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/><br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
