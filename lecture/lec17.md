---
layout: page
title: Lecture 17 – Gradient Descent
nav_exclude: true
---

# Lecture 17 – Gradient Descent

by Josh Hug (Fall 2019) and Joseph Gonzalez (Spring 2020)

- [slides](https://docs.google.com/presentation/d/1dRDBcboq5c99xQJRqNxlv0Bmw8KAH7fKnoI1KhlAifk/edit?usp=sharing)
- [video playlist](https://www.youtube.com/playlist?list=PLQCcNQgUcDfqhJlYw9nvHG4nz0tZ0hjAj)
- [code](https://data100.datahub.berkeley.edu/hub/user-redirect/git-sync?repo=https://github.com/DS-100/su20&subPath=lecture/lec17/)
- [code HTML](../../resources/assets/lectures/lec17/lec17.html)

**Important:** This lecture is taken from both Fall 2019 and Spring 2020.
- In order to follow the lecture, you should be familiar with the ideas from Discussion 1 Problem 2 (Calculus).
- The reference to Homework 6 Problem 7 in 17.2 should be a reference to Homework 5 Problem 3.
- In Homework 7, you will get more practice with learning rates and gradient descent.
- There is an updated version of the [Loss Game](https://gradient-game.herokuapp.com/) mentioned in 17.3.

<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Video</th>
<th>Quick Check</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>17.1</strong> <br>Gradient descent in one dimension. Convexity.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/4n2E4UP-yUo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://forms.gle/5mB4nUDr4Wx8cneA8" target="\_blank">17.1</a></td>
</tr>
<tr>
<td><strong>17.2</strong> <br>Various methods of optimizing loss functions in one dimension.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/3-9l6qrLObg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://forms.gle/sBgFpMRJwVBn9DBL9" target="\_blank">17.2</a></td>
</tr>
<tr>
<td><strong>17.3</strong> <br>Gradient descent in multiple dimensions. Interpretation of gradients.</td>
<td><iframe width="300" height="500" height src="https://youtube.com/embed/odDPUat0_h8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://forms.gle/NtDJcHjdMS87eR9p7" target="\_blank">17.3</a></td>
</tr>
<tr>
<td><strong>17.4</strong> <br>Stochastic gradient descent (SGD). Comparison between gradient descent and SGD.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/njqxGc-9aV8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://forms.gle/bJ4ShFcwnorNh1KL9" target="\_blank">17.4</a></td>
</tr>
