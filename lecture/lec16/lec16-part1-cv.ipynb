{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 16, Part 1 â€“ Data 100, Fall 2020\n",
    "\n",
    "**by Joseph Gonzalez (Spring 2020)**\n",
    "\n",
    "**Note:** scikit-learn's `Pipeline` functionality is explored at length in this notebook. **IT IS NOT IN SCOPE FOR FALL 2020.** Instead, focus on the bigger picture, of how we are splitting our data into train and test, and how we are using cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split and Cross Validation\n",
    "\n",
    "In this notebook we will work through the train test-split and the process of cross validation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "As with other notebooks we will use the same set of standard imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=True, sharing=False, theme='ggplot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "For this notebook, we will use the seaborn `mpg` dataset which describes the fuel mileage (measured in miles per gallon or mpg) of various cars along with characteristics of those cars.  Our goal will be to build a model that can predict the fuel mileage of a car based on the characteristics of that car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import load_dataset\n",
    "data = load_dataset(\"mpg\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "The first thing we will want to do with this data is construct a train/test split. Constructing a train test split before EDA and data cleaning can often be helpful.  This allows us to see if our data cleaning and any conclusions we draw from visualizations generalize to new data. This can be done by re-running the data cleaning and EDA process on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pandas Operations\n",
    "\n",
    "We can sample the entire dataset to get a permutation and then select a range of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_data = data.sample(frac=1., random_state=42)\n",
    "shuffled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a range of rows for training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = int(shuffled_data.shape[0]*0.90)\n",
    "split_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = shuffled_data.iloc[:split_point]\n",
    "te = shuffled_data.iloc[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking that they add up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tr) + len(te) == len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SKLearn\n",
    "\n",
    "We can use the `train_test_split` function from `sklearn.model_selection` to do this easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, te = train_test_split(data, test_size=0.1, random_state=83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Visualization\n",
    "\n",
    "Out of curiosity what does the `mgp` field look like?  I am going to look at both the train and test distributions but **in practice we should avoid looking at the test data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.create_distplot([tr['mpg'], te['mpg']], ['train mpg', 'test mpg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Building A Basic Model\n",
    "\n",
    "Let's go through the process of building a model.  Let's start by looking at just engine characteristics like \"cylinders\" and the \"displacement\". We will first use just our own feature function (as we did in previous lectures).  Then we will introduce how to use sklearn `Pipelines` to combine feature functions and models. As we will see, by combining the feature function and model, we can simplify subsequent training and testing since we are guaranteed that our feature functions are the same on both the training and test datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first feature function will just extract the two features that I want to use in my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(df):\n",
    "    return df[[\"cylinders\", \"displacement\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I fit an sklearn LinearRegression model to my training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(phi(tr), tr['mpg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the error we will use the **Root Mean Squared Error (RMSE)** which is like the mean squared error but in the correct units (mpg) instead of (mpg^2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y, yhat):\n",
    "    return np.sqrt(np.mean((y - yhat)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = model.predict(phi(tr))\n",
    "Y = tr['mpg']\n",
    "print(\"Training Error (RMSE):\", rmse(Y, Y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't try this at home! \n",
    "\n",
    "The test error is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = model.predict(phi(te))\n",
    "Y = te['mpg']\n",
    "print(\"Test Error (RMSE):\", rmse(Y, Y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! We just used the test data to evaluate our model! We shouldn't have done that.  \n",
    "\n",
    "(Don't worry, we are trained professionals and this is only for demonstration purposes.  But seriously, don't try this at home.)  \n",
    "\n",
    "**Notice:** The test error is slightly higher than the training error.  This is typically (but not always) the case.  Sometimes we get lucky and the test data is \"easier to predict\" or happens to closely follow the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "## SKLearn Pipelines\n",
    "\n",
    "Again, for Fall 2020, you do not need to know how to use `Pipeline`s in scikit-learn. They are quite involved. Fortunately, they are merely an accessory to the concepts of this lecture, and not the core of it. If you treat each instance of a `Pipeline` as a black-box way of specifying which features our model should have, you will be able to understand the cross-validation content just fine.\n",
    "\n",
    "We have removed much of the dialogue around `Pipeline` from this lecture, but if you're interested, you can [skim the documentation on pipelines](https://scikit-learn.org/stable/modules/compose.html#pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"SelectColumns\", ColumnTransformer([(\"keep\", \"passthrough\", [\"cylinders\", \"displacement\"])])),\n",
    "    (\"LinearModel\", LinearRegression())\n",
    "])\n",
    "\n",
    "model.fit(tr, tr['mpg']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keeping track of all the models.\n",
    "\n",
    "In this notebook (and in life) we will want to keep track of all our models.  Here I will store the models in a dictionary with a (not great) name so I can remember which model is which and can easily compare my models in a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"c+d\": model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Feature Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also want to look at the displacement per cylinder.  This is an additional feature transformation that we can add to the first stage of our pipeline.  To define this transformation we first need to create a function transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def compute_volume(X):\n",
    "    return np.expand_dims(X[:,1] / X[:,0]  , axis=1)\n",
    "\n",
    "volume_transformer = FunctionTransformer(compute_volume, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then add this as an additional column transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"SelectColumns\", ColumnTransformer([\n",
    "        (\"keep\", \"passthrough\", [\"cylinders\", \"displacement\"]),\n",
    "        (\"cyl_vol\", volume_transformer, [\"cylinders\", \"displacement\"])])),\n",
    "    (\"LinearModel\", LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tr, tr['mpg']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we evaluate the model on our training dataset and see a reduction in error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = model.predict(tr)\n",
    "Y = tr['mpg']\n",
    "print(\"Training Error (RMSE):\", rmse(Y, Y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"c+d+d/c\"] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding More Features\n",
    "\n",
    "We can now add additional features about the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantitative_features = [\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\"]\n",
    "model = Pipeline([\n",
    "    (\"SelectColumns\", ColumnTransformer([\n",
    "        (\"keep\", \"passthrough\", quantitative_features),\n",
    "        (\"cyl_vol\", volume_transformer, [\"cylinders\", \"displacement\"])])),\n",
    "    (\"LinearModel\", LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have put the following code in a try/except statement because I know it will raise an error. What do you think will go wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.fit(tr, tr['mpg'])\n",
    "except ValueError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appear to be NaN (missing values) in the data (take a look at the horsepower column).  We need to deal with these missing values.  In previous lectures I mentioned imputation and a standard imputation technique is to replace the missing value with the mean for that column.  Scikit learn has a built-in imputation function that we can add to our pipeline after we select the desired columns.  The imputation will actually be applied to all the columns.  If we wanted to apply it to a specific column then we would need to put it inside the ColumnTransformer.\n",
    "\n",
    "**Notice:** The imputation function actually needs to be **fit** to data so it is also part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "model = Pipeline([\n",
    "    (\"SelectColumns\", ColumnTransformer([\n",
    "        (\"keep\", \"passthrough\", quantitative_features),\n",
    "        (\"cyl_vol\", volume_transformer, [\"cylinders\", \"displacement\"])])),\n",
    "    (\"Imputation\", SimpleImputer()),\n",
    "    (\"LinearModel\", LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tr, tr['mpg']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model for later comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['c+d+d/c+h+w+a'] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the training error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = model.predict(tr)\n",
    "Y = tr['mpg']\n",
    "print(\"Training Error (RMSE):\", rmse(Y, Y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduced the training error but what about the test error?  We really shouldn't look at the test error so instead we will use cross validation to compare the accuracy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "## Cross Validation\n",
    "\n",
    "In the following function we use the sklearn `KFold` cross validation class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a five fold cross validation with \n",
    "\n",
    "```python \n",
    "five_fold = KFold(n_splits=5)\n",
    "```\n",
    "\n",
    "Then we loop over the 5 splits and get the indicies (`tr_ind`) in the training data to use for training and the indices (`va_ind`) in the training data to use for validation:\n",
    "\n",
    "```python\n",
    "for tr_ind, te_ind in five_fold.split(tr):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "def cross_validate_rmse(model):\n",
    "    model = clone(model)\n",
    "    five_fold = KFold(n_splits=5)\n",
    "    rmse_values = []\n",
    "    for tr_ind, va_ind in five_fold.split(tr):\n",
    "        model.fit(tr.iloc[tr_ind,:], tr['mpg'].iloc[tr_ind])\n",
    "        rmse_values.append(rmse(tr['mpg'].iloc[va_ind], model.predict(tr.iloc[va_ind,:])))\n",
    "    return np.mean(rmse_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate_rmse(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following helper function generates a plot comparing all the models in the models dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(models):\n",
    "    # Compute the training error for each model\n",
    "    training_rmse = [rmse(tr['mpg'], model.predict(tr)) for model in models.values()]\n",
    "    # Compute the cross validation error for each model\n",
    "    validation_rmse = [cross_validate_rmse(model) for model in models.values()]\n",
    "    # Compute the test error for each model (don't do this!)\n",
    "    test_rmse = [rmse(te['mpg'], model.predict(te)) for model in models.values()]\n",
    "    names = list(models.keys())\n",
    "    fig = go.Figure([\n",
    "        go.Bar(x = names, y = training_rmse, name=\"Training RMSE\"),\n",
    "        go.Bar(x = names, y = validation_rmse, name=\"CV RMSE\"),\n",
    "        go.Bar(x = names, y = test_rmse, name=\"Test RMSE\", opacity=.3)])\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = compare_models(models)\n",
    "fig.update_yaxes(range=[2,5.1], title=\"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice I made the **Test RMSE** invisible(ish) because you shouldn't look at it until we are done.  But again for demonstration purposes I plotted in so we can see how it compares to the training and cross validation errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you improve the model further?  Let's try adding the model year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantitative_features = [\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model_year\"]\n",
    "model = Pipeline([\n",
    "    (\"SelectColumns\", ColumnTransformer([\n",
    "        (\"keep\", \"passthrough\", quantitative_features),\n",
    "        (\"cyl_vol\", volume_transformer, [\"cylinders\", \"displacement\"])\n",
    "    ])),\n",
    "    (\"Imputation\", SimpleImputer()),\n",
    "    (\"LinearModel\", LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tr, tr['mpg'])\n",
    "models['c+d+d/c+h+w+a+y'] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = compare_models(models)\n",
    "fig.update_yaxes(range=[2,5.1], title=\"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model year improved accuracy quite a bit!  This improvement also appears to generalize as it also reduced the cross validation error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going too Far?\n",
    "\n",
    "Can we use the car's name to predict MPG?  The name contains general features like the manufacturer that might help but it also contains the vehicle make which is probably too specific and not all the vehicles in test will appear in training. \n",
    "\n",
    "Let's try by applying the CountVectorizer (which implements the bag of words features).  At this point we are also likely to have too many dimensions in our model and we are not applying any regularization technique to compensate (because we haven't covered regularization in lecture yet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "model = Pipeline([\n",
    "    (\"SelectColumns\", ColumnTransformer([\n",
    "        (\"keep\", \"passthrough\", quantitative_features),\n",
    "        (\"cyl_vol\", volume_transformer, [\"cylinders\", \"displacement\"]),\n",
    "        (\"text\", CountVectorizer(), \"name\")\n",
    "    ])),\n",
    "    (\"Imputation\", SimpleImputer()),\n",
    "    (\"LinearModel\", LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: That we are using an additional column transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tr, tr['mpg'])\n",
    "models['c+d+d/c+h+w+a+y+n'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = compare_models(models)\n",
    "fig.update_yaxes(range=[0,5.1], title=\"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overfitting!**.  We substantially reduced the training error but actually made the generalization error worse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = clone(models['c+d+d/c+h+w+a+y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(data, data['mpg']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(best_model.predict(te), te['mpg'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
