---
layout: page
title: Lecture 24 – Clustering
nav_exclude: true
---

# Lecture 24 – Clustering

by Josh Hug (Fall 2019)

- [slides](https://docs.google.com/presentation/d/1RS8Pk-w18wwnvpbsksIL_dZzPGctSBve1B0IiMKPRVk/edit?usp=sharing)
- [video playlist](https://www.youtube.com/playlist?list=PLQCcNQgUcDfp0bZiBZBARyBUi4Ru9gTkK)
- [code](https://data100.datahub.berkeley.edu/hub/user-redirect/git-sync?repo=https://github.com/DS-100/su20&subPath=lecture/lec24/)
- [code HTML](../../resources/assets/lectures/lec24/lec24.html)

**Important:** This lecture is taken from the Fall 2019 semester.
- The variant of K-Means mentioned throughout this lecture seeks to minimize distortion, but most packages that implement K-Means (including scikit-learn) seek to minimize inertia instead of distortion. You will work with scikit-learn's implementation of K-Means in Lab 14.
- In the lecture, there are a couple of plots that you might not be familiar with. The initial clustering example from 24.1 is taken from the first problem of Fall 2019 Midterm 2, and you will see the state plot from the beginning of 24.5 on Homework 8.

<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Video</th>
<th>Quick Check</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>24.1</strong> <br>Introduction to clustering. Taxonomy of machine learning. Examples of clustering in practice.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/5cyJ17HTRkI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://forms.gle/Ga4aaMvFNd2nuVHx5" target="\_blank">24.1</a></td>
</tr>
<tr>
<td><strong>24.2</strong> <br>The K-Means clustering algorithm. Example of K-Means clustering.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/iylHsVRqnu8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://forms.gle/WJN6w32HMLvvJJH89" target="\_blank">24.2</a></td>
</tr>
<tr>
<td><strong>24.3</strong> <br>Loss functions for K-Means. Inertia and distortion. Optimizing distortion.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/9xLSe8WQbwU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://forms.gle/aEUAh3h6qJhFgR436" target="\_blank">24.3</a></td>
</tr>
<tr>
<td><strong>24.4</strong> <br>Agglomerative clustering as an alternative to K-Means. Example of agglomerative clustering. Dendrograms and other clustering algorithms.</td>
<td><iframe width="300" height="500" height src="https://youtube.com/embed/VxIWx9HAN-o" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://forms.gle/GLWvtvYRpuF7wzTN6" target="\_blank">24.4</a></td>
</tr>
<tr>
<td><strong>24.5</strong> <br>Picking the number of clusters. The elbow method and silhouette scores. Summary of clustering and machine learning.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/rPbxQo1Uf78" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://forms.gle/nusQtqHKvpiqTi9R9" target="\_blank">24.5</a></td>
</tr>
