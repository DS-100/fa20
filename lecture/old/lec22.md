---
layout: page
title: Lecture 22 – Dimensionality Reduction
nav_exclude: true
---

# Lecture 22 – Dimensionality Reduction

by Josh Hug (Fall 2019)

- [slides](https://docs.google.com/presentation/d/1Yyzi1miV7cusJw7ljqt07XLYxcSvgdLuCeKprrwi7pE/edit?usp=sharing)
- [video playlist](https://www.youtube.com/playlist?list=PLQCcNQgUcDfoW4IbdYs4ccVTgb4M0U_2X)
- [code](https://data100.datahub.berkeley.edu/hub/user-redirect/git-sync?repo=https://github.com/DS-100/su20&subPath=lecture/lec22/)
- [code HTML](../../resources/assets/lectures/lec22/lec22.html)

**Important:** This lecture is a combination of two lectures from the Fall 2019 semester.
- There are a couple of small typos in 20.4. To check whether a set of vectors is an orthonormal set, we should check whether V^T @ V is the identity matrix (not V @ V^T). For matrices whose columns form an orthonormal set, the property that the matrix's transpose is equivalent to its inverse only holds true if the matrix is square.
- There is a set of extra slides at the end of the lecture slides. These slides contain a review of concepts in linear algebra such as matrix multiplication and rank.

<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Video</th>
<th>Quick Check</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>22.1</strong> <br>Dimensionality. Visualizing high-dimensional data.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/qE6Qtf38QsE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://forms.gle/zJLzQN6fDoXKfsNC7" target="\_blank">22.1</a></td>
</tr>
<tr>
<td><strong>22.2</strong> <br>More visualizations of high-dimensional data.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/wafCB6zVbbA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://forms.gle/Tk66Qqxawvwe9AkQA" target="\_blank">22.2</a></td>
</tr>
<tr>
<td><strong>22.3</strong> <br>Matrix decomposition, redundancy, and rank. Introduction to the singular value decomposition (SVD).</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/s2rKQAOG_R0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://forms.gle/9vNk41BYPKkSMMo27" target="\_blank">22.3</a></td>
</tr>
<tr>
<td><strong>22.4</strong> <br>The theory behind the singular value decomposition. Orthogonality and orthonormality.</td>
<td><iframe width="300" height="500" height src="https://youtube.com/embed/INKZS0HWBLw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://forms.gle/SM8VcTTXZG83aQte6" target="\_blank">22.4</a></td>
</tr>
<tr>
<td><strong>22.5</strong> <br>Low rank approximations with the singular value decomposition.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/IGoHl58pm40" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://forms.gle/bF7EHytfXYf4AcsJ8" target="\_blank">22.5</a></td>
</tr>
