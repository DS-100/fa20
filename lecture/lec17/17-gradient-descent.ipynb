{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 17: Gradient Descent â€“ Data 100, Fall 2020\n",
    "\n",
    "**by Josh Hug (Fall 2019)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing an Arbitrary 1D Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure nice plotting defaults - (this must be done in a cell separate\n",
    "# from %matplotlib call)\n",
    "plt.style.use('seaborn')\n",
    "sns.set_context('talk', font_scale=1.4)\n",
    "plt.rcParams['figure.figsize'] = (10, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arbitrary(x):\n",
    "    return (x**4 - 15*x**3 + 80*x**2 - 180*x + 144)/10\n",
    "\n",
    "x = np.linspace(1, 7, 100)\n",
    "plt.plot(x, arbitrary(x))\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([-1, 3]);\n",
    "# plt.savefig(\"fx4.png\", dpi = 300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, we can see above that the minimum is somewhere around 5.3ish. Let's see if we can figure out how to find the exact minimum algorithmically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way very slow and terrible way would be manual guess-and-check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbitrary(5.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A somewhat better approach is to use brute force to try out a bunch of x values and return the one that yields the lowest loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_minimize(f, xs):\n",
    "    y = [f(x) for x in xs]  \n",
    "    return xs[np.argmin(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_minimize(arbitrary, np.linspace(1, 7, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process is essentially the same as before where we made a graphical plot, it's just that we're only looking at 20 selected points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(1, 7, 200)\n",
    "sparse_xs = np.linspace(1, 7, 21)\n",
    "\n",
    "ys = arbitrary(xs)\n",
    "sparse_ys = arbitrary(sparse_xs)\n",
    "\n",
    "plt.plot(xs, ys, label = \"f\")\n",
    "plt.plot(sparse_xs, sparse_ys, 'r*', label = \"f\");\n",
    "# plt.savefig(\"f_brute_force.png\", dpi=300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This basic approach is incredibly inefficient, and suffers from two major flaws:\n",
    "1. If the minimum is outside our range of guesses, the answer will be completely wrong.\n",
    "2. Even if our range of guesses is correct, if the guesses are too coarse, our answer will be inaccurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Approach: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of choosing all of our guesses ahead of time, we can instead start from a single guess and try to iteratively improve on our choice. \n",
    "\n",
    "They key insight is this: If the derivative of the function is negative, that means the function is decreasing, so we should go to the right (i.e. pick a bigger x). If the derivative of the function is positive, that means the function is increasing, so we should go to the left (i.e. pick a smaller x).\n",
    "\n",
    "Thus, the derivative tells us which way to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#desmos demo: https://www.desmos.com/calculator/twpnylu4lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_arbitrary(x):\n",
    "    return (4*x**3 - 45*x**2 + 160*x - 180)/10\n",
    "\n",
    "def line(x):\n",
    "    return (0*x)\n",
    "plt.plot(x, arbitrary(x))\n",
    "plt.plot(x, derivative_arbitrary(x), '--')\n",
    "plt.plot(x, line(x), '.')\n",
    "\n",
    "plt.legend(['f(x)', 'df(x)'])\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([-1, 3]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_arbitrary():\n",
    "    x = np.linspace(1, 7, 100)\n",
    "    plt.plot(x, arbitrary(x))\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([-1, 3])\n",
    "    \n",
    "def plot_x_on_f(f, x):\n",
    "    y = f(x)\n",
    "    default_args = dict(label=r'$ \\theta $', zorder=2,\n",
    "                        s=200, c=sns.xkcd_rgb['green'])\n",
    "    plt.scatter([x], [y], **default_args)\n",
    "    \n",
    "def plot_x_on_f_empty(f, x):\n",
    "    y = f(x)\n",
    "    default_args = dict(label=r'$ \\theta $', zorder=2,\n",
    "                        s=200, c = 'none', edgecolor=sns.xkcd_rgb['green'])\n",
    "    plt.scatter([x], [y], **default_args)    \n",
    "    \n",
    "def plot_tangent_on_f(f, x, eps=1e-6):\n",
    "    slope = ((f(x + eps) - f(x - eps))\n",
    "             / (2 * eps))\n",
    "    xs = np.arange(x - 1, x + 1, 0.05)\n",
    "    ys = f(x) + slope * (xs - x)\n",
    "    plt.plot(xs, ys, zorder=3, c=sns.xkcd_rgb['green'], linestyle='--')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arbitrary()\n",
    "plot_x_on_f(arbitrary, 2)\n",
    "plot_tangent_on_f(arbitrary, 2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)');\n",
    "#plt.savefig('dfx_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arbitrary()\n",
    "plot_x_on_f(arbitrary, 6)\n",
    "plot_tangent_on_f(arbitrary, 6)\n",
    "#plt.savefig('dfx_2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with this knowledge, let's try to see if we can use the derivative to optimize the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative_arbitrary(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "derivative_arbitrary(4 + 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "derivative_arbitrary(4 + 0.4 + 0.64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative_arbitrary(4 + 0.4 + 0.64 + 0.457)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative_arbitrary(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "derivative_arbitrary(4 + 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "derivative_arbitrary(4 + 0.4 + 0.64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "derivative_arbitrary(4 + 0.4 + 0.64 + 0.46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_step(x):\n",
    "    new_x = x - derivative_arbitrary(x)\n",
    "    plot_arbitrary()\n",
    "    plot_x_on_f(arbitrary, new_x)\n",
    "    plot_x_on_f_empty(arbitrary, x)\n",
    "    print(f'old x: {x}')\n",
    "    print(f'new x: {new_x}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_one_step( 5.48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_step_better(x):\n",
    "    new_x = x - 0.3 * derivative_arbitrary(x)\n",
    "    plot_arbitrary()\n",
    "    plot_x_on_f(arbitrary, new_x)\n",
    "    plot_x_on_f_empty(arbitrary, x)\n",
    "    print(f'old x: {x}')\n",
    "    print(f'new x: {new_x}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_one_step_better(5.32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written as a recurrence relation, the process we've described above is:\n",
    "\n",
    "$$\n",
    "x^{(t+1)} = x^{(t)} -  0.3 \\frac{d}{dx} f(x)\n",
    "$$\n",
    "\n",
    "This algorithm is also known as \"gradient descent\". \n",
    "\n",
    "Given a current $\\gamma$, gradient descent creates its next guess for $\\gamma$ based on the sign and magnitude of the derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our choice of 0.3 above was totally arbitrary. Naturally, we can generalize by replacing it with a parameter, typically represented by $\\alpha$, and often called the \"learning rate\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x^{(t+1)} = x^{(t)} -  \\alpha \\frac{d}{dx} f(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write up this procedure in code as given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(df, initial_guess, alpha, n):\n",
    "    guesses = [initial_guess]\n",
    "    guess = initial_guess\n",
    "    while len(guesses) < n:\n",
    "        guess = guess - alpha * df(guess)\n",
    "        guesses.append(guess)\n",
    "    return np.array(guesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = gradient_descent(derivative_arbitrary, 4, 1.5, 20)\n",
    "trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we see a visualization of the trajectory taken by this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = gradient_descent(derivative_arbitrary, 1, 0.9, 20)\n",
    "plot_arbitrary()\n",
    "plt.plot(trajectory, arbitrary(trajectory));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we've simply run our algorithm a fixed number of times. More sophisticated implementations will stop based on a variety of different stopping criteria, e.g. error getting too small, error getting too large, etc. We will not discuss these in our course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part, we'll return to the world of data science and see how this procedure might be useful for optimizing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (4, 4)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['lines.linewidth'] = 3\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll continue where we left off earlier. We'll see 5 different ways of computing parameters for a 1D, then 2D linear model. These five techniques will be:\n",
    "1. Brute Force\n",
    "2. Closed Form Solutions\n",
    "3. Gradient Descent\n",
    "4. scipy.optimize.minimize\n",
    "5. sklean.linear_model.LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression With No Offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a case where we have a linear model with no offset. That is, we want to find the parameter $\\gamma$ such that the L2 loss is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = sns.load_dataset(\"tips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(tips[\"total_bill\"], tips[\"tip\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a one parameter model that the output is $\\hat{\\gamma}$ times the x value. For example if $\\hat{\\gamma} = 0.1$, then $\\hat{y} = \\hat{\\gamma} x$, and we are making the prediction line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(tips[\"total_bill\"], tips[\"tip\"])\n",
    "x = tips[\"total_bill\"]\n",
    "y_hat = 0.1 * x\n",
    "plt.plot(x, y_hat, 'r')\n",
    "plt.legend(['$\\hat{y}$', '$y$']);\n",
    "#plt.savefig(\"tip_vs_total_bill.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we select the L2 loss as our loss function. In this case, our goal will be to minimize the mean squared error. \n",
    "\n",
    "Let's start by writing a function that computes the MSE for a given choice of $\\gamma$ on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(gamma, x, y_obs):\n",
    "    y_hat = gamma * x\n",
    "    return np.mean((y_hat - y_obs) ** 2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tips[\"total_bill\"]\n",
    "y_obs = tips[\"tip\"]\n",
    "mse_loss(0.1, x, y_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to find the $\\hat{\\gamma}$ with minimum MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Closed Form Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On HW5 problem 3, you derived using calculus that the optimal answer is:\n",
    "\n",
    "$$\\hat{\\gamma} = \\frac{\\sum(x_i y_i)}{\\sum(x_i^2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate this value below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.sum(tips[\"tip\"] * tips[\"total_bill\"])/np.sum(tips[\"total_bill\"]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternately, we can use the generic equation for linear regression that we derived in lecture using the definition of orthogonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tips[[\"total_bill\"]]\n",
    "y = tips[\"tip\"]\n",
    "np.linalg.inv(X.T @ X) @ X.T @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Approach #2A: Plotting the MSE vs. $\\hat{\\gamma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `x` and `y_obs` are fixed, the only variable is `gamma`. \n",
    "\n",
    "For clarity, let's define a python function that returns the MSE as a function of a single argument `gamma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_single_arg(gamma):\n",
    "    \"\"\"Returns the MSE on our data for the given gamma\"\"\"\n",
    "    x = tips[\"total_bill\"]\n",
    "    y_obs = tips[\"tip\"]\n",
    "    y_hat = gamma * x\n",
    "    return mse_loss(gamma, x, y_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_single_arg(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can plot the MSE as a function of `gamma`. It turns out to look pretty smooth, and quite similar to a parabola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gammas = np.linspace(0, 0.2, 200)\n",
    "x = tips[\"total_bill\"]\n",
    "y_obs = tips[\"tip\"]\n",
    "\n",
    "MSEs = [mse_single_arg(gamma) for gamma in gammas]\n",
    "\n",
    "plt.plot(gammas, MSEs)\n",
    "plt.xlabel(r\"Choice for $\\hat{\\gamma}$\")\n",
    "plt.ylabel(r\"MSE\");\n",
    "#plt.savefig(\"tips_MSE_vs_gamma.png\", dpi=300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum appears to be around $\\hat{\\gamma} = 0.14$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2B: Brute Force "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall our simple_minimize function from earlier, redefined below for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_minimize(f, xs):\n",
    "    y = [f(x) for x in xs]  \n",
    "    return xs[np.argmin(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simple_minimize(mse_single_arg, np.linspace(0, 0.2, 21))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, what we're doing is computing all the starred values below and then returning the $\\hat{\\theta}$ that goes with the minimum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gammas = np.linspace(0, 0.2, 200)\n",
    "sparse_gammas = np.linspace(0, 0.2, 21)\n",
    "\n",
    "loss = [mse_single_arg(gamma) for gamma in gammas]\n",
    "sparse_loss = [mse_single_arg(gamma) for gamma in sparse_gammas]\n",
    "\n",
    "plt.plot(gammas, loss)\n",
    "plt.plot(sparse_gammas, sparse_loss, 'r*')\n",
    "plt.xlabel(r\"Choice for $\\hat{\\gamma}$\")\n",
    "plt.ylabel(r\"MSE\");\n",
    "#plt.savefig(\"tips_brute_force.png\", dpi=300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3: Use Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is to use our 1D gradient descent algorithm from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(df, initial_guess, alpha, n):\n",
    "    guesses = [initial_guess]\n",
    "    guess = initial_guess\n",
    "    while len(guesses) < n:\n",
    "        guess = guess - alpha * df(guess)\n",
    "        guesses.append(guess)\n",
    "    return np.array(guesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this function, we need to compute the derivative of the MSE. The MSE is repeated below for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(gamma, x, y_obs):\n",
    "    y_hat = gamma * x\n",
    "    return np.mean((y_hat - y_obs) ** 2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_derivative(gamma, x, y_obs):\n",
    "    y_hat = gamma * x\n",
    "    return np.mean(2 * (y_hat - y_obs) * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def squared_loss_derivative(y_hat, y_obs, x):\n",
    "#    \"\"\"Returns the derivative of the squared loss for a single prediction\"\"\"\n",
    "#    return 2*(y_hat - y_obs)*x\n",
    "#    \n",
    "#def mse_derivative(y_hat, y_obs, x):\n",
    "#    \"\"\"Returns the derivative of the MSE\"\"\"\n",
    "#    return np.mean(squared_loss_derivative(y_hat, y_obs, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try out different values of `gamma` and see what we get back as our derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.1\n",
    "x = tips[\"total_bill\"]\n",
    "y_obs = tips[\"tip\"]\n",
    "\n",
    "mse_loss_derivative(gamma, x, y_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like our `mse_of_gamma`, we can write a function that returns the derivative of the MSE as a function of a single argument `gamma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_derivative_single_arg(gamma):\n",
    "    x = tips[\"total_bill\"]\n",
    "    y_obs = tips[\"tip\"]    \n",
    "\n",
    "    return mse_loss_derivative(gamma, x, y_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss_derivative_single_arg(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gradient_descent(mse_loss_derivative_single_arg, 0.05, 0.0001, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of minimizing loss, we can write out the gradient descent rule for generating the next $\\gamma$ as:\n",
    "\n",
    "$$\n",
    "\\gamma^{(t+1)} = \\gamma^{(t)} - \\alpha \\frac{\\partial}{\\partial \\gamma} L(\\gamma^{(t)}, \\Bbb{X}, \\vec{\\hat{y}})\n",
    "$$\n",
    "\n",
    "Here $L$ is our chosen loss function, $\\Bbb{X}$ is our design matrix, and $\\vec{\\hat{y}}$ are our observations. During the gradient descent algorithm, we treat $\\Bbb{X}$ and $\\vec{\\hat{y}}$ as constants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 4: scipy.optimize.minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `scipy.optimize.minimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "from scipy.optimize import minimize\n",
    "minimize(mse_single_arg, x0 = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A natural question that arises: How does `scipy.optimize.minimize` work? We won't discuss the exact algorithm used by the code (see [this wikipedia page about the BFGS algorithm if you're curious, though](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)).\n",
    "\n",
    "Gradient descent is related to BFGS, though generally doesn't work as well. Comparison of numerical optimization algorithms is very far beyond the scope of our course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 5: sklearn.linear_model.LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also go one level of abstraction higher and simply fit a linear model using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tips[[\"total_bill\"]]\n",
    "y = tips[\"tip\"]\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Dimensional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tips datset\n",
    "data = sns.load_dataset(\"tips\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we improve our model so that we want to predict the tip from the total_bill plus a constant offset, in other words:\n",
    "\n",
    "$$\\textrm{tip} = \\hat{\\theta}_0 + \\hat{\\theta}_1 \\textrm{bill}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things in the simple framework from lecture, we can create a design matrix with the bias vector in one column and the total_bill in the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tips_with_bias = tips.copy()\n",
    "tips_with_bias[\"bias\"] = 1\n",
    "X = tips_with_bias[[\"total_bill\", \"bias\"]]\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can give our predictions as $$\\vec{\\hat{y}} = f_{\\vec{\\hat{\\theta}}}(\\Bbb{X}) = \\Bbb{X} \\vec{\\hat{\\theta}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the predictions below correspond to assuming every table leaves a tip of \\$1.50 plus 5% of their total bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X @ np.array([0.05, 1.50]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this problem, we'll assume we want to minimize the mean squared error of our predictions, i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(theta, X, y_obs):\n",
    "    y_hat = X @ theta\n",
    "    return np.mean((y_hat - y_obs) ** 2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the loss assuming the model described above is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss(np.array([0.05, 1.50]), X, y_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Closed Form Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tips_with_bias[[\"total_bill\", \"bias\"]]\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_obs = tips[\"tip\"]\n",
    "y_obs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hat = np.linalg.inv(X.T @ X) @ X.T @ y_obs\n",
    "theta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: It's generally a better idea to use np.linalg.solve.\n",
    "#np.linalg.inv is slow and can sometimes return incorrect results due to rounding issues.\n",
    "#np.linalg.solve is faster and generally better behaved.\n",
    "theta_hat = np.linalg.solve(X.T @ X, X.T @ y_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Brute Force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can simply try out a bunch of theta values and see which works best. In this case, since we have a 2D theta, there's a much bigger space of possible values to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(theta, X, y_obs):\n",
    "    y_hat = X @ theta\n",
    "    return np.mean((y_hat - y_obs) ** 2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, it's convenient to first create an mse function of a single argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tips_with_bias[[\"total_bill\", \"bias\"]]\n",
    "y_obs = tips[\"tip\"]\n",
    "\n",
    "def mse_loss_single_arg(theta):  \n",
    "    return mse_loss(theta, X, y_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss_single_arg([0.01, 0.02])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function, we can create a 3D plot. This uses lots of syntax you've never seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "uvalues = np.linspace(0, 0.2, 10)#)1)\n",
    "vvalues = np.linspace(0, 2, 10)#)1)\n",
    "(u,v) = np.meshgrid(uvalues, vvalues)\n",
    "thetas = np.vstack((u.flatten(),v.flatten()))\n",
    "\n",
    "MSE = np.array([mse_loss_single_arg(t) for t in thetas.T])\n",
    "\n",
    "loss_surface = go.Surface(x=u, y=v, z=np.reshape(MSE, u.shape))\n",
    "\n",
    "ind = np.argmin(MSE)\n",
    "optimal_point = go.Scatter3d(name = \"Optimal Point\",\n",
    "    x = [thetas.T[ind,0]], y = [thetas.T[ind,1]], \n",
    "    z = [MSE[ind]],\n",
    "    marker=dict(size=10, color=\"red\"))\n",
    "\n",
    "fig = go.Figure(data=[loss_surface, optimal_point])\n",
    "fig.update_layout(scene = dict(\n",
    "    xaxis_title = \"theta0\",\n",
    "    yaxis_title = \"theta1\",\n",
    "    zaxis_title = \"MSE\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is exactly like it was before, only now our gradient is somewhat more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_gradient(theta, X, y_obs):\n",
    "    \"\"\"Returns the gradient of the MSE on our data for the given theta\"\"\"    \n",
    "    x0 = X.iloc[:, 0]\n",
    "    x1 = X.iloc[:, 1]\n",
    "    dth0 = np.mean(-2 * (y_obs - theta[0] * x0 - theta[1] * x1) * x0)\n",
    "    dth1 = np.mean(-2 * (y_obs - theta[0] * x0 - theta[1] * x1) * x1)\n",
    "    return np.array([dth0, dth1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tips_with_bias[[\"total_bill\", \"bias\"]]\n",
    "y_obs = tips[\"tip\"]\n",
    "mse_gradient(np.array([0, 0]), X, y_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_gradient_single_arg(theta):\n",
    "    \"\"\"Returns the gradient of the MSE on our data for the given theta\"\"\"\n",
    "    X = tips_with_bias[[\"total_bill\", \"bias\"]]\n",
    "    y_obs = tips[\"tip\"]\n",
    "    return mse_gradient(theta, X, y_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_gradient_single_arg(np.array([0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gradient_descent(mse_gradient_single_arg, np.array([0, 0]), 0.001, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you play around with the code above, you'll see that it's pretty finicky about the start point and learning rate. \n",
    "\n",
    "Thus, another approach is to use a more sophisticated numerical optimization library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, the general matrix form of the gradient is given below. We have not discussed how to derive this in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_gradient(theta, X, y_obs):\n",
    "    \"\"\"Returns the gradient of the MSE on our data for the given theta\"\"\"\n",
    "    n = len(X)\n",
    "    return -2 / n * (X.T @ y_obs - X.T @ X @ theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 4: scipy.optimize.minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scipy.optimize.minimize(mse_loss_single_arg, x0 = [0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 5: sklearn.linear_model.LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can also go one level of abstraction higher and simply fit a linear model using sklearn.\n",
    "\n",
    "We can either do this by using our `tips_with_bias` and `fit_intercept = False`, or with our original `tips` dataframe and `fit_intercept = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tips_with_bias[[\"total_bill\", \"bias\"]]\n",
    "y = tips[\"tip\"]\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "X = tips[[\"total_bill\"]]\n",
    "y = tips[\"tip\"]\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(tips[\"total_bill\"], tips[\"tip\"])\n",
    "x = tips[\"total_bill\"]\n",
    "y_hat = model.intercept_ + model.coef_ * tips[\"total_bill\"]\n",
    "plt.plot(x, y_hat, 'r')\n",
    "plt.legend(['$\\hat{y}$', '$y$']);\n",
    "#plt.savefig(\"tip_vs_total_bill_linear_regression.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
