---
layout: page
title: Lecture 13 – Ordinary Least Squares
nav_exclude: true
---

# Lecture 13 – Ordinary Least Squares

by Suraj Rampure (Summer 2020)

- [slides](https://docs.google.com/presentation/d/1J73TCoDT40j4ncEJWmrPeeV0bLse7icNdF6xJnYebqQ/edit#slide=id.p)
- [video playlist](https://www.youtube.com/playlist?list=PLQCcNQgUcDfo1CBqvKT_eZyzkTfjMB5ho)
- [code](https://data100.datahub.berkeley.edu/hub/user-redirect/git-sync?repo=https://github.com/DS-100/su20&subPath=lecture/lec13/)
- [code HTML](../../resources/assets/lectures/lec13/lec13.html)

Make sure to complete the Quick Check questions in between each video. These are ungraded, but it's in your best interest to do them.

<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Video</th>
<th>Quick Check</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>13.1</strong> <br>A quick recap of the modeling process, and a roadmap for lecture.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/qL_enPmtwk8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLSfd9ur47_73vrpEhS29YeWJWolXEjidrnbRvgUs-TYM4Wtl6Q/viewform" target="\_blank">13.1</a></td>
</tr>
<tr>
<td><strong>13.2</strong> <br>Defining the multiple linear regression model using linear algebra (dot products and matrix multiplication). Introducing the idea of a design matrix.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/oGIPhLtVb6k" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLSfUPMnbj9HNy_0Fm4Ek8Zk0i9mGTD7kYBzVRrpfyRNbI-HAwQ/viewform" target="\_blank">13.2</a></td>
</tr>
<tr>
<td><strong>13.3</strong> <br>Defining the mean squared error of the multiple linear regression model as the (scaled) norm of the residual vector.</td>
<td><iframe width="300" height="500" height src="https://youtube.com/embed/odY5eSwJ02w" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLSdo4Yds2D8gYxv90llJpAyY6QfzdnE2A5Ukq1lHFHqqf2wP5w/viewform" target="\_blank">13.3</a></td>
</tr>
<tr>
<td><strong>13.4</strong> <br>Using a geometric argument to determine the optimal model parameter.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/nkLUTatnK0s" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/u/1/d/e/1FAIpQLSfa51EPNYZaKOzDdwSHOa2q30azG-FfptGVsYd3-FWR3P7l8Q/viewform" target="\_blank">13.4</a></td>
</tr>
<tr>
<td><strong>13.5</strong> <br>Residual plots. Properties of residuals, with and without an intercept term in our model.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/lT_gzva-dKg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLScFzfjMMjJPkbbM6GnT1cahV2JXMmlsctkWxnnsE4wYXgILJA/viewform" target="\_blank">13.5</a></td>
</tr>
<tr>
<td><strong>13.6</strong> <br>Discussing the conditions in which there isn't a unique solution for the optimal model parameter. A summary, and outline of what is to come.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/9e_w8up-8Yc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLSfv2dziBpYygsAdICIZr9N9OmLQWeKDYDhN82PJjVLSkZvggA/viewform" target="\_blank">13.6</a></td>
</tr>
