{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Homework 7: Gradient Descent, Logistic Regression\n",
    "## Due Date:  Thursday Nov 12, 11:59 PM\n",
    "\n",
    "## This Assignment\n",
    "\n",
    "This assignment consists of 2 parts. In these parts, we will:\n",
    "\n",
    "1. Implement gradient descent, and show how it can be used to minimize differentiable functions, even including loss functions for non-linear models.\n",
    "1. Fit a logistic regression model on NBA data.\n",
    "\n",
    "\n",
    "Note that the first part of this assignment will use bold notation to represent vectors, i.e. $\\mathbf{x}$.\n",
    "\n",
    "## Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the assignment, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point Distribution\n",
    "| Question | Points | \n",
    "|----------|--------|\n",
    "| 1 | 1 |\n",
    "| 2a | 2 |\n",
    "| 2b | 2 |\n",
    "| 3a | 2 |\n",
    "| 3b | 2 |\n",
    "| 3c | 1 |\n",
    "| 4a | 1 |\n",
    "| 4b | 1 |\n",
    "| 4c | 1 |\n",
    "| 5a | 1 |\n",
    "| 5b | 1 |\n",
    "| 5c | 1 |\n",
    "| 6a | 1 |\n",
    "| 6b | 1 |\n",
    "| 6c | 1 |\n",
    "| 6d | 1 |\n",
    "| 7a | 1 |\n",
    "| 7b | 1 |\n",
    "| 7c | 1 |\n",
    "| Total | 23 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8cbb07a2cc27f7d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Part 1: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import re\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "# Set some parameters\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.size'] = 16\n",
    "np.set_printoptions(4)\n",
    "\n",
    "# We will use plot_3d helper function to help us visualize gradients\n",
    "from hw7_utils import plot_3d\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "load",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Load Data\n",
    "Load the data.csv file into a pandas dataframe.  \n",
    "Note that we are reading the data directly from the URL address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "load-data",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to load our sample data\n",
    "part_1_data = pd.read_csv(\"https://github.com/DS-100/su20/raw/gh-pages/resources/assets/datasets/hw7_data.csv\", index_col=0)\n",
    "part_1_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part-1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## A Simple Model\n",
    "Let's start by examining our data and creating a simple model that can represent this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "First, run the cell below to visualize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1a-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def scatter(x, y):\n",
    "    \"\"\"\n",
    "    Generate a scatter plot using x and y\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(x, y, marker='.')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    \n",
    "x = part_1_data['x']\n",
    "y = part_1_data['y']\n",
    "scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "The data looks roughly linear, with some extra sinusoidal noise. For now, let's assume that the data follows some underlying linear model. We define the underlying linear model that predicts the value $y$ using the value $x$ as: $f_{\\theta^*}(x) = \\theta^* \\cdot x$\n",
    "\n",
    "Since we cannot find the value of the population parameter $\\theta^*$ exactly, we will assume that our dataset approximates our population and use our dataset to estimate $\\theta^*$. We denote an estimate with $\\theta$ and the fitted estimated chosen based on the data as $\\hat{\\theta}$. Our parameterized model is:\n",
    "\n",
    "$$\\Large\n",
    "f_{\\theta}(x) = \\theta \\cdot x\n",
    "$$\n",
    "\n",
    "Based on this equation, we will define the linear model function `linear_model` below to estimate $\\textbf{y}$ (the $y$-values) given $\\textbf{x}$ (the $x$-values) and $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1c-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def linear_model(x, theta):\n",
    "    \"\"\"\n",
    "    Returns the estimate of y given x and theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    theta -- the scalar theta\n",
    "    \"\"\"\n",
    "    return theta * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "In class, we learned that the squared loss function is smooth and continuous. Let's use squared loss to evaluate our estimate $\\theta$, which we will use later to identify an optimal $\\theta$, denoted $\\hat{\\theta}$. Given observations $y$ and their corresponding predictions $\\hat{y}$, we can compute the average loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1d-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def average_squared_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Returns the averge squared loss for observations y and predictions y_hat.\n",
    "\n",
    "    Keyword arguments:\n",
    "    y -- the vector of true values y\n",
    "    y_hat -- the vector of predicted values y_hat\n",
    "    \"\"\"\n",
    "    return np.mean((y - y_hat) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1e",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Finally, we will visualize the average squared loss as a function of $\\theta$, where several different values of $\\theta$ are given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1e-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize(x, y, thetas):\n",
    "    \"\"\"\n",
    "    Plots the average l2 loss for given x, y as a function of theta.\n",
    "    Use the functions you wrote for linear_model and l2_loss.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    thetas -- an array containing different estimates of the scalar theta\n",
    "    \"\"\" \n",
    "    avg_loss = np.array([average_squared_loss(linear_model(x, theta), y) for theta in thetas])\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(thetas, avg_loss)\n",
    "    plt.xlabel(\"Theta\")\n",
    "    plt.ylabel(\"Average Loss\")\n",
    "    \n",
    "thetas = np.linspace(-1, 5, 70)\n",
    "visualize(x, y, thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that $\\hat{\\theta}$ is approximately 1.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Fitting our Simple Model\n",
    "Now that we have defined a simple linear model and loss function, let's begin working on fitting our model to the data.\n",
    "\n",
    "### Question 1\n",
    "Let's confirm our visual findings for the optimal $\\hat{\\theta}$.\n",
    "\n",
    "Recall from homework 5 that the analytical solution for the optimal $\\hat{\\theta}$ for the average squared loss is: \n",
    "\n",
    "$$\\hat{\\theta} = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Now that we have the analytic solution for $\\hat{\\theta}$, implement the function `find_theta` that calculates the numerical value of $\\hat{\\theta}$ based on our data $\\textbf{x}$, $\\textbf{y}$.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_theta(x, y):\n",
    "    \"\"\"\n",
    "    Find optimal theta given x and y\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "theta_hat_simple = find_theta(x, y)\n",
    "print(f'theta_hat = {theta_hat_simple}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Now, let's plot our risk function again using the `visualize` function. But this time, we will add a vertical line at the optimal value of theta (plot the line $\\theta = \\hat{\\theta}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q2c-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "theta_opt = find_theta(x, y)\n",
    "visualize(x, y, thetas)\n",
    "plt.axvline(x=theta_opt, color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "We now have an optimal value for $\\theta$ that minimizes the empirical risk. We can use the scatter plot of the data and add the line $f_{\\hat{\\theta}}(x) = \\hat{\\theta} \\cdot \\textbf{x}$ using the $\\hat{\\theta}$ computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q2d-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "theta_opt_2 = find_theta(x, y)\n",
    "scatter(x, y)\n",
    "line_values = linear_model(x, theta_opt_2)\n",
    "plt.plot(x, line_values, color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2e",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Great! It looks like our estimator $f_{\\hat{\\theta}}(x)$ is able to estimate the average $y$ for each $x$ quite well using a single parameter $\\theta$. \n",
    "\n",
    "The difference between the true $y$'s and the predictions is known as the residual, $\\textbf{r}=\\textbf{y}-\\hat{\\theta} \\cdot \\textbf{x}$. Below, we find the residual and plot the residuals corresponding to $x$ in a scatter plot. We also plot a horizontal line at $y=0$ to assist visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q2e-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize_residual(x, y):\n",
    "    \"\"\"\n",
    "    Plot a scatter plot of the residuals, the remaining \n",
    "    values after removing the linear model from our data.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    ...\n",
    "    theta_hat = find_theta(x, y)\n",
    "    y_sin = y - linear_model(x, theta_hat)\n",
    "    plt.scatter(x, y_sin)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('residual (true y - estimated y)')\n",
    "    plt.title('Residual vs x for Linear Model')\n",
    "    plt.axhline(y=0, color='r')\n",
    "\n",
    "visualize_residual(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part-3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 2: Increasing Model Complexity\n",
    "\n",
    "It looks like the residual is sinusoidal, meaning our original data follows a linear function and a sinusoidal function. Let's define a new model to address this discovery and find optimal parameters to best fit the data:\n",
    "\n",
    "$$\\Large\n",
    "f_\\boldsymbol\\theta(x) = \\theta_1x + sin(\\theta_2x)\n",
    "$$\n",
    "\n",
    "Now, our model is parameterized by both $\\theta_1$ and $\\theta_2$, which we can represent in the vector, $\\boldsymbol{\\theta}$.\n",
    "\n",
    "Note that a general sine function $a\\sin(bx+c)$ has three parameters: amplitude scaling parameter $a$, frequency parameter $b$ and phase shifting parameter $c$. Looking at the residual plot above, it looks like the residual is zero at x = 0, and the residual swings between -1 and 1. Thus, it seems reasonable to effectively set the scaling and phase shifting parameter ($a$ and $c$ in this case) to 1 and 0 respectively. While we could try to fit $a$ and $c$, we're unlikely to get much benefit. When you're done with this assignment, you can try adding $a$ and $c$ to our model and fitting these parameters to see if you can get a better loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "We define the `sin_model` function below that predicts $\\textbf{y}$ (the $y$-values) using $\\textbf{x}$ (the $x$-values) based on our new equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sin_model(x, theta):\n",
    "    \"\"\"\n",
    "    Predict the estimate of y given x, theta_1, theta_2\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    theta -- a vector of length 2, where theta[0] = theta_1 and theta[1] = theta_2\n",
    "    \"\"\"\n",
    "    theta_1 = theta[0]\n",
    "    theta_2 = theta[1]\n",
    "    return theta_1 * x + np.sin(theta_2 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q3b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Question 2a\n",
    "\n",
    "Recall the optimal value of $\\theta$ should minimize our loss function. One way we've approached solving for $\\theta$ is by taking the derivative of our loss function with respect to $\\theta$, like we did in HW5.  \n",
    "\n",
    "Write/derive the expressions for following values and write them with LaTeX in the space below.\n",
    "\n",
    "* $R(\\textbf{x}, \\textbf{y}, \\theta_1, \\theta_2)$: our loss function, the empirical risk/mean squared error\n",
    "* $\\frac{\\partial R }{\\partial \\theta_1}$: the partial derivative of $R$ with respect to $\\theta_1$\n",
    "* $\\frac{\\partial R }{\\partial \\theta_2}$: the partial derivative of $R$ with respect to $\\theta_2$\n",
    "\n",
    "Recall that $R(\\textbf{x}, \\textbf{y}, \\theta_1, \\theta_2) = \\frac{1}{n} \\sum_{i=1}^{n} (\\textbf{y}_i - \\hat{\\textbf{y}_i})^2$\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a\n",
    "manual: True\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q3c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "#### Question 2b\n",
    "Now, implement the functions `sin_MSE`, `sin_MSE_dt1` and `sin_MSE_dt2`, which should compute $R$, $\\frac{\\partial R }{\\partial \\theta_1}$ and $\\frac{\\partial R }{\\partial \\theta_2}$ respectively. Use the expressions you wrote for $\\frac{\\partial R }{\\partial \\theta_1}$ and $\\frac{\\partial R }{\\partial \\theta_2}$ in the previous exercise. In the functions below, the parameter `theta` is a vector that looks like $( \\theta_1, \\theta_2 )$. We have completed `sin_MSE_gradient` for you.\n",
    "\n",
    "Notes: \n",
    "* Keep in mind that we are still working with our original set of data, `part_1_data`\n",
    "* To keep your code a bit more concise, be aware that `np.mean` does the same thing as `np.sum` divided by the length of the numpy array.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2b\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3c-answer-1",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sin_MSE(theta):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the l2 loss of our sinusoidal model given theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "def sin_MSE_dt1(theta):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the partial of l2 loss with respect to theta_1\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "def sin_MSE_dt2(theta):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the partial of l2 loss with respect to theta_2\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "# This function calls dt1 and dt2 and returns the gradient dt. It is already implemented for you.\n",
    "def sin_MSE_gradient(theta):\n",
    "    \"\"\"\n",
    "    Returns the gradient of l2 loss with respect to vector theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    x = part_1_data['x']\n",
    "    y = part_1_data['y']     \n",
    "    return np.array([sin_MSE_dt1(theta), sin_MSE_dt2(theta)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q4a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 3: Gradient Descent\n",
    "\n",
    "In lecture, we saw that there are a number of ways to optimize a linear model:\n",
    "\n",
    "1. Brute force guess and check\n",
    "2. Analytically derive a closed form solution\n",
    "3. Use a numerical method like gradient descent\n",
    "\n",
    "You can try to solve for the optimal $\\hat{\\mathbf{\\theta}}$ analytically using your answers from 2a, but we don't recommend it. Notably, observe that the model is not even linear, e.g. it contains $\\theta_2$ inside a sine function.\n",
    "\n",
    "To demonstrate how truly powerful techniques like gradient descent are, we'll use it in this assignment to optimize our nonlinear model.\n",
    "\n",
    "### Question 3\n",
    "\n",
    "Let's now implement gradient descent. \n",
    "\n",
    "Note that the function you're implementing here is somewhat different than the gradient descent function we created in lecture. The version in lecture was `gradient_descent(df, initial_guess, alpha, n)`, where `df` was the gradient of the function we are minimizing and `initial_guess` are the starting parameters for that function. Here our signature is a bit different (described below).\n",
    "\n",
    "#### Question 3a\n",
    "Implement the `grad_desc` function that performs gradient descent for a finite number of iterations. This function takes in an array for $\\textbf{x}$ (`x`), an array for $\\textbf{y}$ (`y`), and an initial value for $\\theta$ (`theta`). `alpha` will be the learning rate (or step size, whichever term you prefer). In this part, we'll use a static learning rate (i.e. the same learning rate at every time step), just like in lecture.\n",
    "\n",
    "At each time step, use the gradient and `alpha` to update your current `theta`. Also at each time step, be sure to save the current `theta` in `theta_history`, along with the average squared loss (computed with the current `theta`) in `loss_history`.\n",
    "\n",
    "After completing the function, the cell will output the trajectory from running gradient descent over time.\n",
    "\n",
    "Hints:\n",
    "- Write out the gradient update equation (1 step). What variables will you need for each gradient update? Of these variables, which ones do you already have, and which ones will you need to recompute at each time step?\n",
    "- You may need a loop here to update `theta` several times.\n",
    "- Recall that the gradient descent update function follows the form:\n",
    "$$\\large\n",
    "\\boldsymbol\\theta^{(t+1)} \\leftarrow \\boldsymbol\\theta^{(t)} - \\alpha \\left(\\nabla_\\boldsymbol\\theta \\mathbf{R}(\\textbf{x}, \\textbf{y}, \\boldsymbol\\theta^{(t)}) \\right)\n",
    "$$\n",
    "- Be sure to include the initial theta and loss into the trajectory because the test checks for this.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3a\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_theta():\n",
    "    \"\"\"Creates an initial theta [0, 0] of shape (2,) as a starting point for gradient descent\"\"\"\n",
    "    return np.zeros((2,))\n",
    "\n",
    "def grad_desc(loss_f, gradient_loss_f, theta, num_iter=20, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Run gradient descent update for a finite number of iterations and static learning rate\n",
    "\n",
    "    Keyword arguments:\n",
    "    loss_f -- the loss function to be minimized (used for computing loss_history)\n",
    "    gradient_loss_f -- the gradient of the loss function to be minimized\n",
    "    theta -- the vector of values theta to use at first iteration\n",
    "    num_iter -- the max number of iterations\n",
    "    alpha -- the learning rate (also called the step size)\n",
    "    \n",
    "    Return:\n",
    "    theta -- the optimal value of theta after num_iter of gradient descent\n",
    "    theta_history -- the series of theta values over each iteration of gradient descent\n",
    "    loss_history -- the series of loss values over each iteration of gradient descent\n",
    "    \"\"\"\n",
    "    theta_history = []\n",
    "    loss_history = []\n",
    "    ...\n",
    "    return theta, theta_history, loss_history\n",
    "\n",
    "theta_start = init_theta()\n",
    "theta_hat, thetas_used, losses_calculated = grad_desc(sin_MSE, sin_MSE_gradient, theta_start, num_iter=20, alpha=0.1)\n",
    "for b, l in zip(thetas_used, losses_calculated):\n",
    "    print(f\"theta: {b}, Loss: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q4b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 3b\n",
    "Now, let's try using a decaying learning rate. Implement `grad_desc_decay` below, which performs gradient descent with a learning rate that decreases slightly with each time step. You should be able to copy most of your work from the previous part, but you'll need to tweak how you update `theta` at each time step.\n",
    "\n",
    "By decaying learning rate, we mean instead of just a number $\\alpha$, the learning should be now $\\frac{\\alpha}{i+1}$ where $i$ is the current number of iteration. (Why do we need to add a '+ 1' in the denominator?)\n",
    "\n",
    "**Note:** Be sure to include the initial theta and loss into the trajectory because the test checks for this.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3b\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def init_theta():\n",
    "    \"\"\"Creates an initial theta [0, 0] of shape (2,) as a starting point for gradient descent\"\"\"\n",
    "    return np.zeros((2,))\n",
    "\n",
    "def grad_desc_decay(loss_f, gradient_loss_f, theta, num_iter=20, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Run gradient descent update for a finite number of iterations and decaying learning rate\n",
    "\n",
    "    Keyword arguments:\n",
    "    loss_f -- the loss function to be minimized (used for computing loss_history)\n",
    "    gradient_loss_f -- the gradient of the loss function to be minimized\n",
    "    theta -- the vector of values theta to use at first iteration\n",
    "    num_iter -- the max number of iterations\n",
    "    alpha -- the learning rate (also called the step size)\n",
    "    \n",
    "    Return:\n",
    "    theta -- the optimal value of theta after num_iter of gradient descent\n",
    "    theta_history -- the series of theta values over each iteration of gradient descent,\n",
    "                     should include the starting and ending theta (i.e. num_iter + 1 items)\n",
    "    loss_history -- the series of loss values over each iteration of gradient descent, \n",
    "                     should include the starting and ending theta (i.e. num_iter + 1 items)\n",
    "    \"\"\"\n",
    "    theta_history = []\n",
    "    loss_history = []\n",
    "    ...\n",
    "    return theta, theta_history, loss_history\n",
    "\n",
    "theta_start = init_theta()\n",
    "theta_hat_decay, thetas_used_decay, losses_calculated_decay =  grad_desc_decay(sin_MSE, sin_MSE_gradient, theta_start, num_iter=20, alpha=0.1)\n",
    "for b, l in zip(thetas_used_decay, losses_calculated_decay):\n",
    "    print(f\"theta: {b}, Loss: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Let's visually inspect our results of running gradient descent to optimize $\\boldsymbol\\theta$. The code below plots our $x$-values with our model's predicted $\\hat{y}$-values over the original scatter plot. You should notice that gradient descent successfully optimized $\\boldsymbol\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_init = init_theta()\n",
    "theta_est, thetas, loss = grad_desc(sin_MSE, sin_MSE_gradient, theta_init)\n",
    "\n",
    "theta_init = init_theta()\n",
    "theta_est_decay, thetas_decay, loss_decay = grad_desc_decay(sin_MSE, sin_MSE_gradient, theta_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4c-answer-2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = sin_model(x, theta_est)\n",
    "\n",
    "plt.plot(x, y_pred, label='Model ($\\hat{y}$)')\n",
    "plt.scatter(x, y, alpha=0.5, label='Observation ($y$)', color='gold')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Run the following cell to see a plot of the loss values over each iteration of gradient descent for both static learning rate and decaying learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q4d-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(loss)), loss, label='Static Alpha')\n",
    "plt.plot(np.arange(len(loss)), loss_decay, label='Decaying Alpha')\n",
    "plt.xlabel('Iteration #')\n",
    "plt.ylabel('Avg Loss')\n",
    "plt.title('Avg Loss vs Iteration # vs Learning Rate Type')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "loss-3d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Visualizing Loss\n",
    "Let's visualize our loss functions and gain some insight as to how gradient descent optimizes our model parameters.\n",
    "\n",
    "In the previous plot we saw the loss decrease with each iteration. In this part, we'll see the trajectory of the algorithm as it travels the loss surface? Run the following cells to see visualization of this trajectory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "loss-3d-2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "thetas = np.array(thetas).squeeze()\n",
    "thetas_decay = np.array(thetas_decay).squeeze()\n",
    "loss = np.array(loss)\n",
    "loss_decay = np.array(loss_decay)\n",
    "thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "loss-3d-3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run me to see a 3D plot (gradient descent with static alpha)\n",
    "plot_3d(thetas[:, 0], thetas[:, 1], loss, average_squared_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "loss-3d-4",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run me to see another 3D plot (gradient descent with decaying alpha)\n",
    "plot_3d(thetas_decay[:, 0], thetas_decay[:, 1], loss_decay, average_squared_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q5b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Another common way of visualizing 3D dynamics is with a _contour_ plot. Run the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q5b-import",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q5b-1",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def contour_plot(title, theta_history, loss_function, model, x, y):\n",
    "    \"\"\"\n",
    "    The function takes the following as argument:\n",
    "        theta_history: a (N, 2) array of theta history\n",
    "        loss: a list or array of loss value\n",
    "        loss_function: for example, l2_loss\n",
    "        model: for example, sin_model\n",
    "        x: the original x input\n",
    "        y: the original y output\n",
    "    \"\"\"\n",
    "    theta_1_series = theta_history[:,0] # a list or array of theta_1 value\n",
    "    theta_2_series = theta_history[:,1] # a list or array of theta_2 value\n",
    "\n",
    "    ## In the following block of code, we generate the z value\n",
    "    ## across a 2D grid\n",
    "    theta1_s = np.linspace(np.min(theta_1_series) - 0.1, np.max(theta_1_series) + 0.1)\n",
    "    theta2_s = np.linspace(np.min(theta_2_series) - 0.1, np.max(theta_2_series) + 0.1)\n",
    "\n",
    "    x_s, y_s = np.meshgrid(theta1_s, theta2_s)\n",
    "    data = np.stack([x_s.flatten(), y_s.flatten()]).T\n",
    "    ls = []\n",
    "    for theta1, theta2 in data:\n",
    "        l = loss_function(model(x, np.array([theta1, theta2])), y)\n",
    "        ls.append(l)\n",
    "    z = np.array(ls).reshape(50, 50)\n",
    "    \n",
    "    # Create trace of theta point\n",
    "    # Create the contour \n",
    "    theta_points = go.Scatter(name=\"theta Values\", \n",
    "                              x=theta_1_series, \n",
    "                              y=theta_2_series,\n",
    "                              mode=\"lines+markers\")\n",
    "    lr_loss_contours = go.Contour(x=theta1_s, \n",
    "                                  y=theta2_s, \n",
    "                                  z=z, \n",
    "                                  colorscale='Viridis', reversescale=True)\n",
    "\n",
    "    plotly.offline.iplot(go.Figure(data=[lr_loss_contours, theta_points], layout={'title': title}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q5b-2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "contour_plot('Gradient Descent with Static Learning Rate', thetas, average_squared_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q5b-3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "contour_plot('Gradient Descent with Decay Learning Rate', thetas_decay, average_squared_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3c: Analyzing Learning Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "In 1-2 sentences, describe what you notice about the path that theta takes with a static learning rate vs. a decaying learning rate. In your answer, refer to either pair of plots above (the 3d plot or the contour plot).\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3c\n",
    "manual: true\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "# Part 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment we will fit a logistic regression model on NBA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (4, 4)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['lines.linewidth'] = 3\n",
    "sns.set() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('nba.csv')\n",
    "df[\"WON\"] = df[\"WL\"]\n",
    "df[\"WON\"] = df[\"WON\"].replace(\"W\", 1)\n",
    "df[\"WON\"] = df[\"WON\"].replace(\"L\", 0)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: 1D Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lecture we buit a logistic regression classifier for the NBA data loaded above. Specifically, our model took an observation $x$ and a parameter vector $\\theta$ and used them to generate a prediction $\\hat{y}$. Note that in this question we will assume that $x$ is a one-dimensional scalar.\n",
    "\n",
    "In this case, our predictions represented the probability that the observation belonged to a specific category. In lecture the category was whether or not the team won. That is, $\\hat{y} = P(Y = 1 | x)$, where $Y = 1$ indicates that the team we're observing won the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 4a: Implementing a 1D Logistic Model\n",
    "\n",
    "As discussed in lecture, the prediction of our model is $ \\hat{y} = \\sigma(x \\hat{\\theta})$. _Note: Here, both $x$ and $\\hat{\\theta}$ are scalars, not vectors._ \n",
    "\n",
    "In this part of the assignment we'll start by trying to build a model that predicts the winning probability as a function of the number of points that a team scored.\n",
    "\n",
    "Below, first define `sigma` to be the sigmoid function we saw in lecture. Then, fill in `predicted_probability_of_winning_given_pts` so that it returns the correct prediction. Your function should work for both scalar and array arguments for `pts`. That is, `predicted_probability_of_winning_given_pts(100, 0.01)` should return a single value `(0.731)` and `predicted_probability_of_winning_given_pts(np.array([100, 110])), 0.01)` should return an array of values `(0.731, 0.750)`. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4a\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(t):\n",
    "    ...\n",
    "\n",
    "def predicted_probability_of_winning_given_pts(pts, theta):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring an Example Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we pick $\\hat{\\theta}$ = 0.01. We can generate predictions for each of the games in our real world dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0.01\n",
    "x = df[\"PTS\"]\n",
    "y_obs = df[\"WON\"]\n",
    "y_hat = predicted_probability_of_winning_given_pts(x, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at our predictions, we see that every team is given a greater than 50 percent prediction of winning based on their number of points. This suggests a problem with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_hat)\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"y_hat\")\n",
    "plt.title(\"Predictions\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what's going on, we make a plot of the prediction our model will make as a function of the number of points scored for $\\hat{\\theta} = 0.01$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0.01\n",
    "pts = np.linspace(0, 140, 140)\n",
    "plt.plot(pts, predicted_probability_of_winning_given_pts(pts, theta), color = 'orange')\n",
    "plt.ylabel(\"Win Probability\")\n",
    "plt.xlabel(\"Points Scored\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also provide the actual results from the NBA dataset as blue stars for comparison to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0.01\n",
    "pts = np.linspace(0, 140, 140)\n",
    "plt.plot(pts, predicted_probability_of_winning_given_pts(pts, theta), color = 'orange')\n",
    "plt.plot(df[[\"PTS\"]], df[[\"WON\"]], 'b*')\n",
    "plt.ylabel(\"Win Probability\")\n",
    "plt.xlabel(\"Points\")\n",
    "plt.legend(['$\\hat{y}$', 'y']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 4b\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4b\n",
    "manual: true\n",
    "points: 1\n",
    "-->\n",
    "\n",
    "Is this model reasonable? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 4c\n",
    "\n",
    "Try playing around with other theta values. You should observe that the models are all pretty bad, no matter what $\\theta$ you pick. Explain why below.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4c\n",
    "manual: true\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "## Question 5: Adding an Intercept Term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you observe your plot(s) from the previous question, you'll see that the chance of winning is always at least 0.5 under our model (assuming $\\hat{\\theta} \\geq 0$). This is unreasonable, e.g. suppose a team somehow scored only 36 points, they'd have no chance of winning in an NBA game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Logistic Regression with an Intercept Term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with this, we should add another feature to our model. Specifically, we'll add a bias term, i.e. a feature that is equal to 1 for all observations. We've done this for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_and_bias = df[[\"PTS\"]].copy()\n",
    "points_and_bias[\"bias\"] = np.ones(len(points_and_bias))\n",
    "points_and_bias.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression generalizes to multiple features in exactly the same manner as linear regression.\n",
    "\n",
    "Recall that whereas linear regression on one parameter gave predictions $\\hat{y} = x \\hat{\\theta}$, multiple linear regression gave predictions $\\hat{y} = \\vec{x} \\cdot \\vec{\\hat{\\theta}} = \\vec{x}^T \\vec{\\hat{\\theta}} = \\sum_{i = 1}^p x_i \\hat{\\theta}_i$.\n",
    "\n",
    "Logistic regression generalizes in exactly the same way. That is logistic regression in 1 variable is given by $\\hat{y} = \\sigma(x \\hat{\\theta})$, whereas multiple logistic regression is given by $\\hat{y} = \\sigma(\\vec{x} \\cdot \\vec{\\hat{\\theta}}) = \\sigma(\\vec{x}^T \\vec{\\hat{\\theta}}) = \\sigma(\\sum_{i = 1}^p x_i \\hat{\\theta}_i)$.\n",
    "\n",
    "Fill in the function below so that it returns predictions as described above. As in the previous question, your model should be able to handle scalar and array arguments for x. For example `predicted_probability_of_winning_given_features(X.iloc[0:3, :], [0.1, -10])` should return a list, series, or numpy array of the values `[0.6899744811276126, 0.5, 0.21416501695744153]`.\n",
    "\n",
    "Your function only needs to work for array inputs to `x`. That is, your code does not need to work properly for `predicted_probability_of_winning_given_features(110, [0.1, -10])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_probability_of_winning_given_features(X, theta):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Logistic Regression with an Intercept Term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two parameters $\\theta_1$ and $\\theta_2$. Suppose $\\theta_1 = 0.001$ and $\\theta_2 = 2$. We can compute the predicted probability that each team won during each game as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([0.001, 2])\n",
    "predicted_probability_of_winning_given_features(points_and_bias.iloc[0:3, :], theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_and_bias.iloc[0:3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a bias term, we have more freedom to adjust our model.\n",
    "\n",
    "For example, if $\\theta_1 = 0.05$ and $\\theta_2 = -5$, we get the curve below. Here, the prediction of your model is $\\sigma(\\theta_1 \\times \\text{PTS} + \\theta_2)$. That is, $\\theta_1$ is the weight of `PTS`, and $\\theta_2$ is the weight of the bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = [0.05, -5]\n",
    "pts = np.linspace(50, 160, 111).reshape(-1, 1)\n",
    "bias = np.ones(len(pts)).reshape(-1, 1)\n",
    "point_range_and_bias = np.hstack((pts, bias))\n",
    "plt.plot(pts, predicted_probability_of_winning_given_features(point_range_and_bias, theta), 'orange')\n",
    "plt.ylabel(\"Win Probability\")\n",
    "plt.xlabel(\"Points\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as before, we can also plot the actual data from our NBA dataset for comparison with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df[\"PTS\"], df[\"WON\"], 'b*')\n",
    "theta = [0.05, -5]\n",
    "pts = np.linspace(50, 160, 111).reshape(-1, 1)\n",
    "bias = np.ones(len(pts)).reshape(-1, 1)\n",
    "point_range_and_bias = np.hstack((pts, bias))\n",
    "plt.plot(pts, predicted_probability_of_winning_given_features(point_range_and_bias, theta), 'orange')\n",
    "plt.ylabel(\"Win Probability\")\n",
    "plt.xlabel(\"Points\")\n",
    "plt.legend(['$\\hat{y}$', 'y']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 5b\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5b\n",
    "manual: true\n",
    "points: 1\n",
    "-->\n",
    "Using the plot above, try adjusting $\\theta_2$ (only). Describe how changing $\\theta_2$ affects the prediction curve. Provide your description in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "### Question 5c\n",
    "Now using the plot below try adjusting $\\theta_1$ and $\\theta_2$ such that you get a sharp curve that is centered at 100 points. In the cell below `theta` should be a list with your chosen values of $\\theta_1$ and $\\theta_2$.\n",
    "\n",
    "- By \"centered at 100 points\", we mean that $\\hat{y}$ should be equal to 0.5 when $x = 100$.\n",
    "- By \"sharp\", we mean that the probability should be less than 5% percent for $x = 80$, and greater than 95% for $x = 120$.\n",
    "- *Hint*: $\\sigma(t) = 0.5$ when $t = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df[\"PTS\"], df[\"WON\"], 'b*')\n",
    "theta = ...\n",
    "pts = np.linspace(50, 160, 111).reshape(-1, 1)\n",
    "bias = np.ones(len(pts)).reshape(-1, 1)\n",
    "point_range_and_bias = np.hstack((pts, bias))\n",
    "plt.plot(pts, predicted_probability_of_winning_given_features(point_range_and_bias, theta), 'orange')\n",
    "plt.ylabel(\"Win Probability\")\n",
    "plt.xlabel(\"Points\")\n",
    "plt.legend(['$\\hat{y}$', 'y']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Provide your $\\theta_1$ and $\\theta_2$ in the cell below.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5c\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1 = ...\n",
    "theta2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Optimizing Logistic Regresion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now work towards finding the optimal theta $\\hat{\\theta}$ for our given data.\n",
    "\n",
    "_Note: In the previous question, we referred to our $\\theta$s without a hat, since we had yet to find the optimal values of $\\theta$ procedurally._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6a: Calculating MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function `mse_for_model_on_NBA_data(theta)` that takes in a value of $\\theta$ and returns the MSE on the dataset from above. You will first need to define the function `mse(y_obs, y_hat)` which finds the mean squared error between `y_obs` and `y_hat`.\n",
    "\n",
    "**Hint:** You need to compute $\\hat{y}$ using the given $\\theta$, then the mean squared error between $\\hat{y}$ and the observed data $y$.\n",
    "\n",
    "**Hint:** Use `points_and_bias` and `df[\"WON\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_obs, y_hat):\n",
    "    ...\n",
    "\n",
    "def mse_for_model_on_NBA_data(theta):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting MSE\n",
    "\n",
    "The cell below plots your MSE function. We're providing this plot purely for your edification. Warning: This code can be pretty slow and might take a minute or two to run.\n",
    "\n",
    "Note that the surface has a huge almost completely flat region. This means this loss function is very difficult to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "num_points = 50 # increase for better resolution, but it will run more slowly. \n",
    "\n",
    "if (num_points <= 100):\n",
    "\n",
    "    uvalues = np.linspace(-0.3, 0.3, num_points)\n",
    "    vvalues = np.linspace(-20, 20, num_points)\n",
    "    (u,v) = np.meshgrid(uvalues, vvalues)\n",
    "    thetas = np.vstack((u.flatten(),v.flatten()))\n",
    "\n",
    "    MSE = np.array([mse_for_model_on_NBA_data(t) for t in thetas.T])\n",
    "\n",
    "    loss_surface = go.Surface(x=u, y=v, z=np.reshape(MSE, u.shape))\n",
    "\n",
    "    fig = go.Figure(data=[loss_surface])\n",
    "    fig.update_layout(scene = dict(\n",
    "        xaxis_title = \"theta0\",\n",
    "        yaxis_title = \"theta1\",\n",
    "        zaxis_title = \"MSE\"))\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Picking num points > 100 can be really slow. If you really want to try, edit the code above so that this if statement doesn't trigger.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 6b: Minimizing MSE\n",
    "\n",
    "Using `scipy.optimize.minimize`, find the optimal $\\hat{\\theta}$.  Give your answer as `theta_hat_1` and `theta_hat_2`. The resulting MSE should be less than 0.2.\n",
    "\n",
    "Note: Your starting guess should be (0, 0). If you start somewhere over in the flat region like (0, 20), then scipy.optimize.minimize will get stuck.\n",
    "\n",
    "Note: The test(s) for this question requires that you did the previous part correctly.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6b\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "optimal_theta = ...\n",
    "theta_hat_1 = ...\n",
    "theta_hat_2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6c\n",
    "\n",
    "Finally, let's try to understand how our model can be practically useful. As we'll see in lecture 19, we often convert our logistic regression models into a concrete prediction by thresholding. That is, if our $\\hat{y} \\geq 0.5$, we say our prediction is that the team will win; otherwise, we say that we predict that we will lose. A simple way to do this is just to round our $\\hat{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = predicted_probability_of_winning_given_features(points_and_bias, np.array([theta_hat_1, theta_hat_2])) \n",
    "games_and_predictions = df.copy()\n",
    "games_and_predictions[\"predicted_to_win\"] = np.round(y_hat)\n",
    "games_and_predictions[[\"TEAM_NAME\", \"GAME_DATE\", \"WON\", \"predicted_to_win\"]].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "To evaluate the quality of your model, compute the fraction of the rows of the table for which your model was able to correctly predict the outcome of the game based on only the points scored by one team. Assign this to the variable `percentage_correct`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6c\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_correct = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 6d\n",
    "\n",
    "Recall that the surface for the MSE has a huge almost completely flat region, which means that the loss function is very difficult to optimize.\n",
    "\n",
    "In lecture we talked about an alternate loss function called the cross-entropy loss that yielded a much nicer loss surface (no big flat regions). You will need to define the function `cel(y_obs, y_hat)` which calculates the cross-entropy loss betweeen `y_obs` and `y_hat`. Then, create a function `cel_for_model_on_NBA_data(theta)` that takes in a value of $\\vec{{\\theta}}$ and returns the cross-entropy loss on the dataset from the previous question.\n",
    "\n",
    "\n",
    "**Hint:** Your code for this part should be very similar to your code for part a of this question.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6d\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cel(y_obs, y_hat):\n",
    "    ...\n",
    "\n",
    "def cel_for_model_on_NBA_data(theta):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Cross-Entropy Loss\n",
    "\n",
    "The cell below plots your cross-entropy loss function. Note that the surface has no big flat regions, which makes it easy to optimize.\n",
    "\n",
    "Note: Feel free to ignore the divide by zero warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "num_points = 50 # increase for better resolution, but it will run more slowly. \n",
    "\n",
    "if (num_points <= 100):\n",
    "\n",
    "    uvalues = np.linspace(-0.3, 0.3, num_points)\n",
    "    vvalues = np.linspace(-20, 20, num_points)\n",
    "    (u,v) = np.meshgrid(uvalues, vvalues)\n",
    "    thetas = np.vstack((u.flatten(),v.flatten()))\n",
    "\n",
    "    CEL = np.array([cel_for_model_on_NBA_data(t) for t in thetas.T])\n",
    "\n",
    "    loss_surface = go.Surface(x=u, y=v, z=np.reshape(CEL, u.shape))\n",
    "\n",
    "    fig = go.Figure(data=[loss_surface])\n",
    "    fig.update_layout(scene = dict(\n",
    "        xaxis_title = \"theta0\",\n",
    "        yaxis_title = \"theta1\",\n",
    "        zaxis_title = \"CEL\"))\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Picking num points > 100 can be really slow. If you really want to try, edit the code above so that this if statement doesn't trigger.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: Higher Dimensional Models\n",
    "\n",
    "To wrap up this part of the assignment, let's try training a logistic regression model on all useful fields. Note we won't include the difference in the number of points between the two teams (given as PLUS_MINUS), otherwise the logistic regression will be right 100% of the time (since a team wins if and only if PLUS_MINUS > 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_numeric_fields = df.loc[:, 'FGM':'PTS'].copy()\n",
    "useful_numeric_fields[\"BIAS\"] = np.ones(len(useful_numeric_fields))\n",
    "useful_numeric_fields.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 7a\n",
    "Using the ideas from earlier in this part of the assignment, find `theta_19_hat`, which should be a numpy array of length 19 that minimizes the MSE. For this, your starting guess should be a vector of 19 zeros. If you start from a random vector, scipy minimize will probably get stuck. The mean squared error for your model should be less than 0.12. You will need to define a function `mse_for_model_on_full_data(theta)` similar to the `mse_for_model_on_NBA_data(theta)` function earlier in this part of the assignment.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7a\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mse_for_model_on_full_data(theta):\n",
    "    ...\n",
    "theta_19_hat = ...\n",
    "mse_for_model_on_full_data(theta_19_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that `mse_for_model_on_full_data(theta_19_hat)` is less than 0.12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 7b\n",
    "\n",
    "Compute `frac_19_correct`, which should be equal to the fraction of predictions which are correct given this new model. You should be able to get at least 84% accuracy.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7b\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_19_correct = ...\n",
    "frac_19_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 7c\n",
    "\n",
    "Look at the coefficients in `theta_19_hat` and identify which of the parameters have the biggest effect on the prediction. For this, you might find `useful_numeric_fields.columns` useful. Which attributes have the biggest positive effect on a team's success? The biggest negative effects? Do the results surprise you?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7c\n",
    "manual: true\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(\"hw7.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
