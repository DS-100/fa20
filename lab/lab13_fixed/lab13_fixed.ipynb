{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Lab 13: Using the Bootstrap for Estimation\n",
    "**Collaboration Policy**\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about\n",
    "the homework, we ask that you **write your solutions individually**. If you do\n",
    "discuss the assignments with others please **include their names** at the top\n",
    "of your solution.\n",
    "\n",
    "## Due Date\n",
    "\n",
    "This assignment is due on **Monday, December 7th, at 11:59PM.**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you will examine the bootstrap in greater detail. The goal is to develop a functional approach to bootstrapping any statistic for any sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators:** *list names here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer about `sns.distplot()`\n",
    "\n",
    "This lab was designed for a slightly older version of seaborn, which does not support the new `displot` method taught in Lecture 9. Instead, in this lab, we will occasionally call `distplot` (with a `t`). As you may have noticed in several of the previous assignments, use of the `distplot` function triggers a deprecation warning to notify the user that they should replace all deprecated functions with the updated version. Generally, warnings should not be suppressed but we will do so in this assignment to avoid cluttering.\n",
    "\n",
    "See the seaborn documentation on [distributions](https://seaborn.pydata.org/tutorial/distributions.html) and [functions](https://seaborn.pydata.org/tutorial/function_overview.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "setup",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to suppress all FutureWarnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Run this cell to set up your notebook\n",
    "\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model as lm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "bootstrap_description",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## The Bootstrap Procedure\n",
    "\n",
    "The bootstrap is a very simple process: \n",
    "* Sample with replacement from the original sample (now the **bootstrap population**). These samples are called **bootstrap samples**. We typically take thousands of bootstrap samples (~10,000 is common).\n",
    "* Calculate the statistic of interest for each bootstrap sample. This statistic is called the **bootstrap statistic**, and the empirical distribution of these bootstrap statistics is an approximation to the **sampling distribution** of the bootstrapped statistic.\n",
    "\n",
    "But why bootstrap instead of just calculating the statistic of interest once on the whole sample? \n",
    "\n",
    "Take sample mean estimator as an example. Suppose $\\{x_i\\}$ are samples coming from an unknown distribution. We can use sample mean $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n x_i$ to estimate the the mean of the population. However, if we obtain another sample set $\\{x_i\\}$, we can get very different results. Therefore, computing the sampling distribution (distribution of the sample mean for all possible sample sets) would be very helpful. From such a distribution, we can obtain the variance of the estimator: $E[(E(\\bar{X})-\\bar{X})^2]$.\n",
    "\n",
    "In order to compute the sampling distribution of $\\bar{X}$, we could directly use sampling methods. But that requires us to obtain several different sets of samples $\\{x_i\\}$ directly from the population. If we have $m$ sets of samples and each set contains $n$ subjects (totaling $m \\cdot n$ subjects), we can then use $\\hat{X} = \\frac{1}{m}\\sum_{j=1}^m \\bar{X_j}$ to approximate $E(\\bar{X})$, and $\\frac{1}{m}\\sum_{j=1}^m (\\hat{X} - \\bar{X_j})^2$ to approximate $E[(E(\\bar{X})-\\bar{X})^2]$ where $\\bar{X_j}$ is the mean of the j'th sample.\n",
    "\n",
    "However, in reality, this is often unfeasible, and we only have one set of samples (**bootstrap population**). Therefore we can use bootstrap method to resample (sample with replacement) from the **bootstrap population** to obtain $m$ different **bootstrap samples**, where each **bootstrap sample** contains the same amount of data as in the **bootstrap population**. \n",
    "\n",
    "Why do we sample with replacement?\n",
    "Recall that we are trying to mimic the ideal scenario of directly sampling from the original population. In the case where each bootstrap sample has the same amount of data as the bootstrap population, sampling without replacement would cause every bootstrap sample to be identical to the bootstrap population. Even when the bootstrap samples have less numbers of data than the bootstrap population, sampling without replacement would cause the samples to be *based on the original sample*. Instead, sampling with replacement causes our bootstrap samples to be *based on the distribution of the original sample* (which we assume is representative of the original population)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q1_text1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 1\n",
    "In this lab, let's use the bootstrap method to estimate the distribution of sample mean and sample standard deviation of a previous year's midterm grades. You will be given a noisy sample of grades from the midterm, which is the **bootstrap population**. You should sample from the bootstrap population **with replacement** to obtain **bootstrap samples** and compute the **bootstrap statistic**.\n",
    "\n",
    "First, write your own sampling function. The function `simple_resample` samples with replacement from the integers 0 through *n-1* and returns an array of length *n*\n",
    "with the sampled integers. That is, `simple_resample` produces the indices for\n",
    "a single bootstrap replicate from the bootstrap population.\n",
    "\n",
    "Use the `numpy.random.randint` function to do the random sampling. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1_code",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def simple_resample(n):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        n: an integer\n",
    "        \n",
    "    Returns:\n",
    "        an array of length n of a random sample with replacement of\n",
    "        the integers 0, 1, ..., n-1\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "simple_resample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2_text1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 2\n",
    "\n",
    "### Question 2a\n",
    "\n",
    "Next, let's write the function `bootstrap` which returns an array of length `replicates`, each entry being the `statistic` of interest computed on a bootstrap sample from the `boot_pop` (bootstrap population).\n",
    "\n",
    "In our case, the `statistic` could be the `np.mean` or `np.std` function, and the `resample` could be `simple_resample`. Here we leave them as parameters so that we can switch to other statistic and resample functions later.\n",
    "\n",
    "For each bootstrap sample, you should first use `resample` to obtain samples from the `boot_pop`, then compute the statistic of those samples using the `statistic` method, and put it into your result.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2_code1",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def bootstrap(boot_pop, statistic, resample, replicates = 1000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        boot_pop: an array of shape n x d.\n",
    "        statistic: a function which takes boot_pop and returns a number or array (in the case where we are estimating multiple parameters at once).\n",
    "        resample: a function which takes n and returns a random sample from the integers [0, n)\n",
    "        replicates: the number of resamples\n",
    "        \n",
    "    Returns:\n",
    "        an array of length replicates, each entry being the statistic computed on a bootstrap sample of the data.\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2_text2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Now let's use the bootstrap function to compute the distribution of the sample mean for the midterm grades.\n",
    "\n",
    "First, let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2_setup",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "grades = pd.read_csv(\"grades_sample.csv\")\n",
    "grades.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2_setup2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(grades)\n",
    "plt.plot([np.mean(grades), np.mean(grades)], [0, 0.07], label = 'sample mean')\n",
    "plt.ylim(0, 0.06)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2_text3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 2b\n",
    "\n",
    "Now, use the right parameters to call our `bootstrap` method to obtain the sample mean $\\bar{X}$ for $m$ different bootstrap samples $\\{x_i\\}$. \n",
    "\n",
    "**Note**: Re-running the below cell will give us a different mean and variance each time (Why?).\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2_code2",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = 1000  # the number of resamples \n",
    "boot_pop = np.array(grades[\"Grade\"])\n",
    "\n",
    "boot_sample_means = ...\n",
    "\n",
    "boot_mean_mean = np.mean(boot_sample_means)\n",
    "boot_var_mean = np.var(boot_sample_means)\n",
    "print('Mean of all bootstrap sample means:', boot_mean_mean, '\\nVariance of bootstrap sample means:', boot_var_mean)\n",
    "sns.distplot(boot_sample_means)\n",
    "plt.xlabel(r\"$\\bar{X}$\")\n",
    "plt.ylabel(\"Frequency\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do your results compare to the true midterm distribution with $\\mu=80.1$ and $\\sigma=9.20?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2_text4",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 2c\n",
    "\n",
    "Now, use the right parameters to call our `bootstrap` method to obtain the standard deviation $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (x_i-\\bar{X})^2}$ for $m$ different bootstrap samples $\\{x_i\\}$. You should use `np.std`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2c\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2_code3",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "m = 1000\n",
    "boot_pop = np.array(grades[\"Grade\"])\n",
    "\n",
    "boot_sample_std = ...\n",
    "\n",
    "boot_mean_std = np.mean(boot_sample_std)\n",
    "boot_var_std = np.var(boot_sample_std)\n",
    "print('Mean of bootstrap sample SDs:', boot_mean_std, '\\nVariance of bootstrap sample SDs:', boot_var_std)\n",
    "sns.distplot(boot_sample_std)\n",
    "plt.xlabel(r\"$SD(X)$\")\n",
    "plt.ylabel(\"Frequency\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 2d\n",
    "\n",
    "Our bootstrap estimates were fairly accurate for the midterm grades, but this may not necessarily be the case when we sample from more complex distributions. In the following parts, we will create bootstrap estimates for the mean and standard deviation of an exponential population, as done in lecture.\n",
    "\n",
    "Below is the probability density function (pdf) of the exponential distribution with $\\lambda=\\frac{1}{2}$. The exponential distribution has a neat property where the mean and standard deviation are both given by $\\frac{1}{\\lambda}$. That means for our exponential population, $\\mu=2$ and $\\sigma=2$.\n",
    "\n",
    "![exponential distribution](exp.png)\n",
    "\n",
    "A bootstrap population of 20 samples from the exponential distribution has been generated for you in `exp_pop`. Use the `bootstrap` function that you wrote below to find the sample mean $\\bar{X}$ for 1000 bootstrap samples.\n",
    "\n",
    "***Note***: We purposely set the random seed to 15 to generate a bootstrap population that results in a particularly bad estimate of the true mean in this part, but you will see in part (e) that there is a lot of variance in the bootstrap mean estimate depending on the bootstrap population that we sample from.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2d\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(15)\n",
    "\n",
    "m = 1000\n",
    "exp_pop = np.random.exponential(scale=2, size=20)\n",
    "\n",
    "exp_boot_means = ...\n",
    "\n",
    "exp_boot_mean = np.mean(exp_boot_means)\n",
    "exp_boot_var = np.var(exp_boot_means)\n",
    "print('Mean of all bootstrap sample means:', exp_boot_mean, '\\nVariance of bootstrap sample means:', exp_boot_var)\n",
    "sns.distplot(exp_boot_means)\n",
    "plt.axvline(x=2, label='Population Mean', c='red', linewidth=1)\n",
    "plt.axvline(x=np.mean(exp_boot_means), label='Bootstrap Sample Mean', c='black', linewidth=1)\n",
    "plt.title('Bootstrap sample means for n=20')\n",
    "plt.xlabel(r\"$\\bar{X}$\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 2e\n",
    "\n",
    "Observe that the distribution of our bootstrapped sample means is not at all centered on the true population mean of the exponential distribution ($\\mu=2$). Repeat the experiment done in part (d) for five different populations of size $n=20$ drawn from the Exponential ($\\lambda = \\frac{1}{2}$) distribution and plot the distributions side by side. How many of the distributions are centered near the true population mean?\n",
    "\n",
    "(Note: This is a subtle point, but what we are plotting in this question is the distribution of several bootstrapped sample means, not the distribution of a single bootstrapped resample. We can only do this because we have access to the entire population, which we typically don't.)\n",
    "\n",
    "**Hint:** You may want to look at the documentation of `np.random.exponential` to determine how to set its parameters.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2e\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = 1000\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 4))\n",
    "plt.tight_layout()\n",
    "for i in range(5):\n",
    "    exp_pop = ...\n",
    "    exp_boot_means = ...\n",
    "    axs[i].hist(exp_boot_means) \n",
    "    \n",
    "    axs[i].axvline(x=np.mean(exp_boot_means), label='Bootstrap Sample Mean', c='black', linewidth=2)\n",
    "    axs[i].axvline(2, color='r', label='Population Mean', c='r', linewidth=2)\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title('Bootstrap sample means for n=20')\n",
    "    axs[i].set_xlabel(r\"$\\bar{X}$\")\n",
    "    axs[i].set_ylabel(\"Frequency\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 2f\n",
    "\n",
    "There seems to be a lot of variance in the bootstrap sample mean distributions, but this may simply be caused by our small sample size. Plot the sample mean distribution for five different Exponential ($\\lambda = \\frac{1}{2}$) populations, but increase the sample size of each bootstrap population to $n=200$. Do your results differ from part (e)? \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2f\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 4))\n",
    "plt.tight_layout()\n",
    "for i in range(5):\n",
    "    exp_pop = ...\n",
    "    exp_boot_means = ...\n",
    "    axs[i].hist(exp_boot_means) \n",
    "    axs[i].axvline(2, color='r', label='Population Mean')\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title('Bootstrap sample means for n=200')\n",
    "    axs[i].set_xlabel(r\"$\\bar{X}$\")\n",
    "    axs[i].set_ylabel(\"Frequency\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you should take away from this is that the usefulness of the bootstrap as a tool in inference depends greatly on the shape of the population distribution. Tricky distributions will lead to less-than-stellar bootstrap results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3_text1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 3\n",
    "Now let's use the bootstrap method to analyze more sophisticated estimators – the coefficients of linear models.\n",
    "\n",
    "Let's use the `mpg` dataset from `seaborn` that we looked at in an earlier lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg = sns.load_dataset('mpg')\n",
    "mpg = mpg.dropna()\n",
    "mpg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at the relationship between `horsepower` and `mpg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(mpg['horsepower'], mpg['mpg']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be clear that a line of best fit going through these points would not pass through the origin, but for the sake of simplicity, let's start by building a simple linear model **without an intercept term** to model `mpg` as a function of `horsepower`. We will assume there is some true slope $\\theta^*$ that we are trying to estimate, such that\n",
    "\n",
    "$$\\text{mpg} = \\theta^* \\cdot \\text{horsepower}$$\n",
    "\n",
    "Our prediction for `mpg` is then\n",
    "\n",
    "$$\\hat{\\text{mpg}} = \\theta \\cdot \\text{horsepower}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q3_text2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "In Homework 5, you showed that the value of $\\hat{\\theta}$ that minimized average squared loss for this model is given by\n",
    "\n",
    "$${\\hat{\\theta}} = \\frac{\\sum x_iy_i}{\\sum x_i^2}$$\n",
    "\n",
    "We describe the above $\\hat{\\theta}$ as the **least squares estimator** for $\\theta^*$.\n",
    "\n",
    "### Question 3a\n",
    "\n",
    "Below, implement the function `single_parameter_estimator`, which takes in a dataset `d` and returns the least squares estimate for $\\theta^*$ defined above.\n",
    "\n",
    "Note that in order to perform the bootstrap, we need to combine `x`, `y` into a $n \\times 2$ array `d`. So `d[:,0]` is equivalent to `x`, and `d[:,1]` corresponds to `y`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3_code1",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def single_parameter_estimator(d):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        d: A n*2 array which contains x and y. d[:,0] corresponds to x and d[:,1] to y.\n",
    "        \n",
    "    Returns:\n",
    "        The optimal theta that minimizes average squared loss for this dataset and model\n",
    "        .\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "data_3a = mpg[['horsepower', 'mpg']].values\n",
    "single_parameter_estimator(data_3a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3_text3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Now, we can use our previous `bootstrap` function to obtain different bootstrap estimations for $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3_code2",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "replicates = 1000\n",
    "\n",
    "boot_theta = bootstrap(data_3a, single_parameter_estimator, simple_resample, replicates)\n",
    "\n",
    "boot_theta_mean = np.mean(boot_theta)\n",
    "boot_theta_var = np.var(boot_theta)\n",
    "print('mean of bootstrap theta:', boot_theta_mean, '\\nvariance of bootstrap theta:', boot_theta_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3_text4",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Finally, let's plot the distribution of `boot_theta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3_plot",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(boot_theta);\n",
    "plt.xlabel(r\"$\\theta*$\")\n",
    "plt.ylabel(\"Frequency\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 3b\n",
    "\n",
    "As we saw in lecture, we can extend this procedure to a linear model with any number of coefficients. Let's now suppose we are trying to model `mpg` as a linear function of `horsepower`, `weight`, and `acceleration`, that contains an intercept term. We are looking to estimate $\\theta_0^*, \\theta_1^*, \\theta_2^*,$ and $\\theta_3^*$ in\n",
    "\n",
    "$$\\text{mpg} = \\theta_0^* + \\theta_1^* \\cdot \\text{horsepower} + \\theta_2^* \\cdot \\text{weight} + \\theta_3^* \\cdot \\text{acceleration} + \\epsilon$$\n",
    "\n",
    "After we find our estimated/fitted parameters, this model makes predictions using the formula\n",
    "$$\\text{predicted mpg} = \\hat{\\theta_0} + \\hat{\\theta_1} \\cdot \\text{horsepower} + \\hat{\\theta_2} \\cdot \\text{weight} + \\hat{\\theta_3} \\cdot \\text{acceleration}$$\n",
    "\n",
    "\n",
    "If we want to bootstrap the sampling distribution of the estimators of multiple coefficients, it's probably best to use `scikit-learn`'s `LinearRegression` package to determine what the least squares estimates of our parameters are. Here's how we _could have_ written the `single_parameter_estimator` function above, using `scikit-learn` instead of hard-coding the optimal $\\hat{\\theta}$:\n",
    "\n",
    "```py\n",
    "def single_parameter_estimator_sk(d):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        d: A n*2 array which contains x and y. d[:,0] would be x, d[:,1] would be y.\n",
    "        \n",
    "    Returns:\n",
    "        The optimal theta that minimizes average squared loss for this dataset and model\n",
    "        .\n",
    "    \"\"\"\n",
    "    model = lm.LinearRegression(fit_intercept = False)\n",
    "    model.fit(d[:, 0].reshape(-1, 1), d[:, 1])\n",
    "    return model.coef_[0]\n",
    "```\n",
    "\n",
    "Using the method described above, fill in the code for `four_parameter_estimator(d)`, that takes in a dataset `d` that has the same number of rows as the `mpg` dataset, and 4 columns (one each for `horsepower`, `acceleration`, `weight`, and `mpg`). It should return an **array** with 4 elements – the least squares estimates for all four model parameters (i.e. the model parameters that minimize average squared loss for this dataset).\n",
    "\n",
    "To be explicit, the parameters we're solving for are the intercept term and the slopes on `horsepower`, `acceleration`, and `weight`. \n",
    "\n",
    "HINT: Try using `model.intercept_` and `model.coef_`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def four_parameter_estimator(d):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        d: A n*4 array which contains X and y. \n",
    "        d[:, :3] contains our design matrix X, \n",
    "        d[:, 3] contains our true response values y.\n",
    "\n",
    "    Returns:\n",
    "        The optimal theta that minimizes average squared loss for this dataset and model\n",
    "        .\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "data_3b = mpg[['horsepower', 'weight', 'acceleration', 'mpg']].values\n",
    "four_parameter_estimator(data_3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to use our `bootstrap` method to compute the estimated sampling distribution for all four of our parameters. Observe what happens when we call `bootstrap(data_3b, four_parameter_estimator, simple_resample, replicates)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_multiple = bootstrap(data_3b, four_parameter_estimator, simple_resample, replicates)\n",
    "bootstrap_multiple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous questions, the result of calling `bootstrap` was an array of length `replicates`. However, now it is a matrix of size `(replicates, 4)` since for each bootstrap resample, we are estimating four parameters, not one.\n",
    "\n",
    "In `bootstrap_multiple`, column `i` contains the estimated values of $\\theta_i^*$.\n",
    "\n",
    "Below, we display a plot with the bootstrapped sampling distributions of all four parameter estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(2,2,figsize=(10, 8))\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.title(r'Bootstrapped Sampling Distribution for $\\theta_{}^*$'.format(i))\n",
    "    sns.distplot(bootstrap_multiple[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 3c\n",
    "\n",
    "Recall, we can use bootstrapped parameter estimates to create confidence intervals for the true model parameters.\n",
    "\n",
    "Let's focus on the bootstrapped estimates for $\\theta_3^*$ which corresponds to the model weight for `acceleration`. Below, set `left_endpt` and `right_endpt` to be the left and right endpoints for a **95% confidence interval** for the value of $\\theta_3^*$.\n",
    "- Hint: You will need to use `np.percentile`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3c\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "left_endpt = ...\n",
    "right_endpt = ...\n",
    "\n",
    "left_endpt, right_endpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we plot your confidence interval along with the distribution of bootstrapped estimates for $\\theta_3^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(bootstrap_multiple[:, 3]);\n",
    "plt.plot([left_endpt, right_endpt], [0, 0], linewidth = 10, label = '95% CI');\n",
    "plt.legend();\n",
    "plt.xlabel(r\"$\\theta_3*$\")\n",
    "plt.ylabel(\"Frequency\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did Question 3c correctly, you should notice that 0 is in the confidence interval for $\\theta_3^*$. Since this is the case, we would say we don't have enough evidence to reject the claim that the true slope is 0, i.e. that `acceleration` does not help explain `mpg` in a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "That seems to be a little strange. Intuitively, we'd think that the acceleration of a car may impact its fuel economy. So why is it that the slope for acceleration in the previous question was found to be not significantly different than 0? Let's explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's look at a scatter plot between `acceleration` and `mpg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data = mpg,\n",
    "           x = 'acceleration', \n",
    "           y = 'mpg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the correlation is not particularly strong, we can see a slight positive correlation between `acceleration` and `mpg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg[['acceleration', 'mpg']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means, at the very least, that `acceleration` should provide some explanatory power when predicting `mpg`. So why was 0 in the confidence interval for the true slope on `acceleration` in the previous question?\n",
    "\n",
    "Let's dig a little deeper. Let's look at the correlation between `horsepower`, `acceleration`, and `weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg[['horsepower', 'acceleration', 'weight']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 4a\n",
    "\n",
    "What do you notice above? What does this have to do with 0 being in the 95% confidence interval for $\\theta_3^*$ above?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4a\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4b\n",
    "\n",
    "Let's now create a `two_parameter_estimator` that returns the estimates for the intercept and slope for the following model:\n",
    "\n",
    "$$\\text{mpg} = \\theta_0^* + \\theta_1^* \\cdot \\text{acceleration} + \\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is already filled in for you\n",
    "def two_parameter_estimator(d):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        d: A n*2 array which contains X and y. \n",
    "        d[:, 0] contains our x,\n",
    "        d[:, 1] contains our true y.\n",
    "\n",
    "    Returns:\n",
    "        The optimal theta that minimizes average squared loss for this dataset and model.\n",
    "    \"\"\"\n",
    "    model = lm.LinearRegression(fit_intercept = True)\n",
    "    model.fit(d[:, 0].reshape(-1, 1), d[:, 1])\n",
    "    return np.append(model.intercept_, model.coef_)\n",
    "\n",
    "data_4 = mpg[['acceleration', 'mpg']].values\n",
    "two_parameter_estimator(data_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In the following cell, set `bootstrap_4` to a `nx2` array that contains the results of calling our `bootstrap` method on the above estimator and data.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_4 = ...\n",
    "bootstrap_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of the acceleration weight, $\\theta_1^*$, for our new one parameter model. Notice that 0 is no longer in the 95% confidence interval after we removed weight and horsepower from the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 5))\n",
    "sns.distplot(bootstrap_4[:, 1]);\n",
    "plt.xlabel(r\"$\\theta_1*$\");\n",
    "plt.ylabel(\"Frequency\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 4c\n",
    "\n",
    "We could have alternatively caught the collinearity in our model features by thinking about their physical meaning. Suppose that we can calculate the horsepower of a car as \n",
    "$$hp = F\\frac{d}{t}$$ where $F$ is the force in pounds and we set $d$ and $t$ to be constant. \n",
    "\n",
    "How can we use this fact and [Newton's Second Law](https://en.wikipedia.org/wiki/Newton%27s_laws_of_motion#Newton's_second_law) to explain the negative correlation between `weight` and `acceleration`?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4c\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(\"lab13_fixed.ipynb\", pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
