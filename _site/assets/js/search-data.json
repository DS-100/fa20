{
  "0": {
    "id": "0",
    "title": "Announcements",
    "content": "Announcements Announcements are stored in the _announcements directory and rendered according to the layout file, _layouts/announcement.html. Week 0 Announcements Welcome to Data 100! We’re still getting this site set up. Some information here may be inaccurate. We will remove this warning once the site is complete. Announcements We will not be updating this page with announcements. For the latest announcements, make sure to check our Piazza.",
    "url": "http://localhost:4000/fa20/announcements/",
    "relUrl": "/announcements/"
  },
  "1": {
    "id": "1",
    "title": "Calendar",
    "content": "Calendar This page has yet to be updated for Fall 2020. Please stay tuned!",
    "url": "http://localhost:4000/fa20/calendar/",
    "relUrl": "/calendar/"
  },
  "2": {
    "id": "2",
    "title": "Final Project",
    "content": "Graduate Final Project Coming Soon",
    "url": "http://localhost:4000/fa20/gradproject/",
    "relUrl": "/gradproject/"
  },
  "3": {
    "id": "3",
    "title": "Home",
    "content": "Principles and Techniques of Data Science UC Berkeley, Fall 2020 Anthony Joseph adj@berkeley.edu Fernando Perez fernando.perez@berkeley.edu This entire website is a work in progress, and all information is subject to change! All announcements are on Piazza. Make sure you are enrolled and active there. The Syllabus contains a detailed explanation of how each course component will work this fall, given that the course is being taught entirely online. The scheduling of all weekly events is in the Calendar.",
    "url": "http://localhost:4000/fa20/",
    "relUrl": "/"
  },
  "4": {
    "id": "4",
    "title": "Lecture 10 – Visualization, Part 2",
    "content": "Lecture 10 – Visualization, Part 2 by Suraj Rampure (Summer 2020) slides video playlist code code HTML Extra reading on colormaps: How the Rainbow Color Map Misleads When to use Sequential and Diverging Palettes Color Use Guidelines Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 10.1 Ensuring that the axes in our visualizations aren&#39;t misleading. 10.1 10.2 Designing visualizations that are well-suited for making comparisons. 10.2 10.3 How to use color to create effective visualizations. How to choose color schemes that are clear and accessible. 10.3 10.4 How to choose markings that the human eye can easily interpret. Issues to avoid, such as jiggling baselines and overplotting. 10.4 10.5 Discussing the supplemental text that publication-ready plots need. 10.5 10.6 When to use smoothing. How kernel density estimates are created. Looking at various kernels. Understanding the impact of the bandwidth hyperparameter. 10.6 10.7 Discussing why we prefer linear relationships. Understanding how to &quot;reverse-engineer&quot; a linearized relationship to determine the true relationship. Identifying which transformations to use in order to linearize a relationship. 10.7",
    "url": "http://localhost:4000/fa20/lecture/lec10/",
    "relUrl": "/lecture/lec10/"
  },
  "5": {
    "id": "5",
    "title": "Lecture 11 – Introduction to Modeling",
    "content": "Lecture 11 – Introduction to Modeling by Suraj Rampure (Summer 2020) slides video playlist code code HTML Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 11.1 Motivating examples of models. 11.1 11.2 Defining the constant model. Formalizing the notion of a parameter. 11.2 11.3 Loss functions and their purpose. Squared loss and absolute loss. Minimizing average loss (i.e. empirical risk). 11.3 11.4 Minimizing mean squared error for the constant model using calculus, to show that the sample mean is the optimal model parameter in this case. 11.4 11.5 Performing the same optimization as in the last video, but by using a non-calculus algebraic manipulation. 11.5 11.6 Minimizing mean absolute error for the constant model using calculus, to show that the sample median is the optimal parameter in this case. Identifying that this solution isn&#39;t necessarily unique. 11.6 11.7 Comparing the loss surfaces of MSE and MAE for the constant model. Discussing the benefits and drawbacks of squared and absolute loss. Recapping the &quot;modeling process&quot;. 11.7",
    "url": "http://localhost:4000/fa20/lecture/lec11/",
    "relUrl": "/lecture/lec11/"
  },
  "6": {
    "id": "6",
    "title": "Lecture 12 – Simple Linear Regression",
    "content": "Lecture 12 – Simple Linear Regression by Suraj Rampure (Summer 2020) slides video playlist code code HTML Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 12.1 The correlation coefficient and its properties. 12.1 12.2 Defining the simple linear regression model, our first model with two parameters and an input variable. Motivating linear regression with the graph of averages. 12.2 12.3 Using calculus to derive the optimal model parameters for the simple linear regression model, when we choose squared loss as our loss function. 12.3 12.4 Visualizing and interpreting loss surface of the SLR model. 12.4 12.5 Interpreting the slope of the simple linear model. 12.5 12.6 Defining key terminology in the regression context. Expanding the simple linear model to include any number of features. 12.6 12.7 RMSE as a metric of accuracy. Multiple R-squared as a metric of explained variation. Summary. 12.7",
    "url": "http://localhost:4000/fa20/lecture/lec12/",
    "relUrl": "/lecture/lec12/"
  },
  "7": {
    "id": "7",
    "title": "Lecture 13 – Ordinary Least Squares",
    "content": "Lecture 13 – Ordinary Least Squares by Suraj Rampure (Summer 2020) slides video playlist code code HTML Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 13.1 A quick recap of the modeling process, and a roadmap for lecture. 13.1 13.2 Defining the multiple linear regression model using linear algebra (dot products and matrix multiplication). Introducing the idea of a design matrix. 13.2 13.3 Defining the mean squared error of the multiple linear regression model as the (scaled) norm of the residual vector. 13.3 13.4 Using a geometric argument to determine the optimal model parameter. 13.4 13.5 Residual plots. Properties of residuals, with and without an intercept term in our model. 13.5 13.6 Discussing the conditions in which there isn&#39;t a unique solution for the optimal model parameter. A summary, and outline of what is to come. 13.6",
    "url": "http://localhost:4000/fa20/lecture/lec13/",
    "relUrl": "/lecture/lec13/"
  },
  "8": {
    "id": "8",
    "title": "Lecture 14 – Feature Engineering",
    "content": "Lecture 14 – Feature Engineering by Joseph Gonzalez (Spring 2020) slides video playlist code code HTML: Part 1, Part 2, Part 3 Important: This lecture is a combination of two lectures from Spring 2020 (this is why the video titles don’t match our numbering). Read this before proceeding with the lectures, as it details which concepts you should focus on. Sections 14.1 through 14.4 discuss the core techniques of feature engineering. Slides are linked above, and code is in “Part 1” and “Part 2”. 14.1: Throughout this lecture, Radial Basis Functions are used as an example. For our purposes, they are purely an example, and are not in-scope. 14.2, 14.3: Entirely in scope. 14.4: Of the three techniques discussed, one-hot encoding is most important, though the others are still in scope. Sections 14.5 through 14.7 discuss pitfalls to be aware of in feature engineering. There are no accompanying slides; these ideas are primarily explained in the lecture notebook “Part 3”. 14.5: Focus on the numerical ideas here, not the syntax of model creation (though the code is linked above). 14.6: The focus of this video is about the content at the end where our design matrix has too many columns, not about the details of Radial Basis Functions. 14.7: See the above comment. Video Quick Check 14.1 A demonstration of how to use scikit-learn to fit linear models. 14.1 14.2 Feature functions, as a method of transforming existing numerical data, and encoding non-numerical data for use in modeling. 14.2 14.3 Defining what it means for a model to be linear. The constant feature. More sophisticated numerical features. 14.3 14.4 Numerically encoding categorical data using various encodings (one-hot, bag of words, n-gram). 14.4 14.5 Issues we may run into when our design matrix has redundant features. 14.5 14.6 Issues we may run into when our design matrix has more features than observations. Radial basis functions. 14.6 14.7 Overfitting our model to the data we used to train it leads to poor generalizability to unseen data, which is the goal of modeling. 14.7",
    "url": "http://localhost:4000/fa20/lecture/lec14/",
    "relUrl": "/lecture/lec14/"
  },
  "9": {
    "id": "9",
    "title": "Lecture 15 – Bias and Variance",
    "content": "Lecture 15 – Bias and Variance by Ani Adhikari (Spring 2020) slides video playlist Bias-Variance decomposition derivation Important: This lecture is taken from Spring 2020 (which again is why the video titles don’t match up with our numbering). In order to follow it, you must be familiar with the ideas from Lecture 3 (Random Variables). In the last lecture, we touched on this idea of “model complexity”. It is mentioned towards the end of this lecture, but will be covered more in-depth in Lecture 16 (Cross-Validation and Regularization). The algebra behind the decomposition of model risk into observational variance, model variance, and bias, is not in the slides or video but is in the link above. You should read it after watching this lecture. Video Quick Check 15.1 Introducing the data generating process and prediction error. Model risk. 15.1 15.2 Looking at different sources of error in our model – observation variance, model variance, and bias – and discussing how to mitigate them. 15.2 15.3 Decomposing model risk into the sum of observation variance, model variance, and the square of bias. 15.3",
    "url": "http://localhost:4000/fa20/lecture/lec15/",
    "relUrl": "/lecture/lec15/"
  },
  "10": {
    "id": "10",
    "title": "Lecture 16 – Cross-Validation and Regularization",
    "content": "Lecture 16 – Cross-Validation and Regularization by Joseph Gonzalez, Paul Shao (Spring 2020), and Suraj Rampure (Summer 2020) slides video playlist code code HTML: Part 1, Part 2 Important: This lecture is a combination of several lectures from Spring 2020 (this is why the video titles don’t match our numbering), plus a piece of “glue” added in this summer. Read this before proceeding with the lectures, as it details what materials you should focus on. Sections 16.1 through 16.4 discuss train-test splits and cross-validation. 16.1 walks through why we need to split our data into train and test in the first place, and how cross-validation works. It primarily consists of slides. 16.2 and 16.3 walk through the process of creating a basic train-test split, and evaluating models that we’ve fit on our training data using our testing data. Code is in “Part 1”. 16.4 walks through the process of implementing cross-validation. In this video there references to a Pipeline object in scikit-learn. This is not in scope for Summer 2020, so do not worry about its details. Code is in “Part 1”. Sections 16.5 and 16.6 discuss regularization. 16.5 discusses why we need to regularize, and how penalties on the norm of our parameter vector accomplish this goal. 16.6 explicitly lists the optimal model parameter when using the L2 penalty on our linear model (called “ridge regression”). There are also three supplementary videos accompanying this lecture. They don’t introduce any new material, but may still be helpful for your understanding. They are listed as supplementary and not required since the runtime of this lecture is already quite long. They do not have accompanying Quick Checks for this reason. 16.7 and 16.8 walk through implementing ridge and LASSO regression in a notebook. These videos are helpful in explaining how regularization and cross-validation are used in practice. These videos again use Pipeline, which is not in scope. Code is in “Part 2”. 16.9 is another supplementary video, created by Paul Shao (a TA for Data 100 in Spring 2020). It gives a great high-level overview of both the bias-variance tradeoff and regularization. Video Quick Check 16.1 Training error vs. testing error. Why we need to split our data into train and test. How cross-validation works, and why it is useful. 16.1 16.2 Using scikit-learn to construct a train-test split. 16.2 16.3 Building a linear model and determining its training and test error. 16.3 16.4 Implementing cross-validation, and using it to help select a model. 16.4 16.5 An overview of regularization. 14.5 16.6 Ridge regression and LASSO regression. 16.6 16.7 *Supplemental.* Using ridge regression and cross-validation in scikit-learn. N/A 16.8 *Supplemental.* Using LASSO regression and cross-validation in scikit-learn. N/A 16.9 *Supplemental.* An overview of the bias-variance tradeoff, and how it interfaces with regularization. N/A",
    "url": "http://localhost:4000/fa20/lecture/lec16/",
    "relUrl": "/lecture/lec16/"
  },
  "11": {
    "id": "11",
    "title": "Lecture 17 – Gradient Descent",
    "content": "Lecture 17 – Gradient Descent by Josh Hug (Fall 2019) and Joseph Gonzalez (Spring 2020) slides video playlist code code HTML Important: This lecture is taken from both Fall 2019 and Spring 2020. In order to follow the lecture, you should be familiar with the ideas from Discussion 1 Problem 2 (Calculus). The reference to Homework 6 Problem 7 in 17.2 should be a reference to Homework 5 Problem 3. In Homework 7, you will get more practice with learning rates and gradient descent. There is an updated version of the Loss Game mentioned in 17.3. Video Quick Check 17.1 Gradient descent in one dimension. Convexity. 17.1 17.2 Various methods of optimizing loss functions in one dimension. 17.2 17.3 Gradient descent in multiple dimensions. Interpretation of gradients. 17.3 17.4 Stochastic gradient descent (SGD). Comparison between gradient descent and SGD. 17.4",
    "url": "http://localhost:4000/fa20/lecture/lec17/",
    "relUrl": "/lecture/lec17/"
  },
  "12": {
    "id": "12",
    "title": "Lecture 18 – Logistic Regression, Part 1",
    "content": "Lecture 18 – Logistic Regression, Part 1 by Suraj Rampure (Summer 2020) slides video playlist code code HTML Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 18.1 Classification, and a brief overview of the machine learning taxonomy. 18.1 18.2 Pitfalls of using least squares to model probabilities. Creating a graph of averages to motivate the logistic regression model. 18.2 18.3 Deriving the logistic regression model from the assumption that the log-odds of the probability of belonging to class 1 is linear. 18.3 18.4 Formalizing the logistic regression model. Exploring properties of the logistic function. Interpreting the model coefficients. 18.4 18.5 Discussing the pitfalls of using squared loss with logistic regression. 18.5 18.6 Introducing cross-entropy loss, as a better alternative to squared loss for logistic regression. 18.6 18.7 Using maximum likelihood estimation to arrive at cross-entropy loss. 18.7 18.8 Demo of using scikit-learn to fit a logistic regression model. An overview of what&#39;s coming next. 18.8",
    "url": "http://localhost:4000/fa20/lecture/lec18/",
    "relUrl": "/lecture/lec18/"
  },
  "13": {
    "id": "13",
    "title": "Lecture 19 – Logistic Regression Part 2, Classification",
    "content": "Lecture 19 – Logistic Regression Part 2, Classification by Suraj Rampure (Summer 2020) slides video playlist code code HTML Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 19.1 Using thresholds to convert from predicted probabilities to classifications. 19.1 19.2 Defining several metrics of classifier performance – accuracy, precision, and recall. Confusion matrices. 19.2 19.3 Using scikit-learn to compute accuracy, precision, recall, and confusion matrices. 19.3 19.4 Exploring how threshold impacts accuracy, precision, and recall. Precision-recall curves. ROC curves. AUC. 19.4 19.5 Exploring the decision boundaries that result from a logistic regression classifier, and their relationship to the model&#39;s parameters. 19.5 19.6 Linear separability. Why we sometimes need regularization for logistic regression. 19.6 19.7 Summary. Brief introduction to multiclass classification. N/A",
    "url": "http://localhost:4000/fa20/lecture/lec19/",
    "relUrl": "/lecture/lec19/"
  },
  "14": {
    "id": "14",
    "title": "Lecture 2 – Data Sampling and Probability",
    "content": "Lecture 2 – Data Sampling and Probability by Suraj Rampure (Summer 2020) slides video playlist This is the first lecture of a brand-new format! Make sure to complete the Quick Check problems as you progress through the videos to confirm your understanding. Video Quick Check 2.1 Censuses and surveys. Issues with the US Census. 2.1 2.2 Samples. Drawbacks to convenience and quota samples. 2.2 2.3 A case study in sampling bias (1936 election). 2.3 2.4 Sources of bias, and a formal definition of sampling frames. 2.4 2.5 Probability samples, and why we need them. 2.5 2.6 Introducing binomial and multinomial probability calculations. 2.6 2.7 Generalizing binomial and trinomial probability calculations. 2.7 2.8 (Extra) Using permutations and combinations to derive the binomial coefficient. 2.8 2.9 (Extra) Example usages of the binomial coefficient. 2.9",
    "url": "http://localhost:4000/fa20/lecture/lec2/",
    "relUrl": "/lecture/lec2/"
  },
  "15": {
    "id": "15",
    "title": "Lecture 20 – Decision Trees",
    "content": "Lecture 20 – Decision Trees by Josh Hug (Fall 2019) slides video playlist code code HTML Important: This lecture is taken from the Fall 2019 semester. The reference to Lecture 22 in 20.1 should be a reference to Lecture 19. The references to lec25-decision-trees.ipynb in 20.1 should be references to lec20-decision-trees.ipynb. The slides in 20.4 should say: “Bagging often isn’t enough to reduce model variance!” Without selecting a random subset of features at each split, the decision trees fitted on bagged data often look very similar to one another; this means that they make similar predictions. As a result, the ensemble of decision trees would still have low bias and high model variance. Selecting a random subset of features at each split helps reduce model variance by making the decision trees in the ensemble different from one another. Video Quick Check 20.1 Decision tree basics. Decision trees in scikit-learn. 20.1 20.2 Overfitting and decision trees. 20.2 20.3 Decision tree generation. Finding the best split. Entropy and weighted entropy. 20.3 20.4 Restricting decision tree complexity. Preventing growth and pruning. Random forests and bagging. 20.4 20.5 Regression trees. Summary of decision trees, classification, and regression. 20.5",
    "url": "http://localhost:4000/fa20/lecture/lec20/",
    "relUrl": "/lecture/lec20/"
  },
  "16": {
    "id": "16",
    "title": "Lecture 21 – Inference for Modeling",
    "content": "Lecture 21 – Inference for Modeling by Suraj Rampure (Summer 2020) slides video playlist code code HTML The Data 8 textbook chapter on estimation may be very helpful. Video Quick Check 21.1 A big picture overview of inference. Parameters and estimators. Bias and variance of estimators. The sample mean estimator. 21.1 21.2 Using bootstrap resampling in order to estimate the sampling distribution of an estimator. 21.2 21.3 Defining confidence intervals more generally. Describing and demoing how we can use the bootstrap to create confidence intervals for population parameters. 21.3 21.4 The assumptions we make when modeling with linear regression.. 21.4 21.5 Using the bootstrap to estimate the sampling distributions of parameters in a linear regression model. Inference for the true slope of a feature. 21.5 21.6 Multicollinearity, and its impacts on the interpretability of the parameters of our model. A summary of the lecture, and a brief overview of the ML taxonomy. 21.6",
    "url": "http://localhost:4000/fa20/lecture/lec21/",
    "relUrl": "/lecture/lec21/"
  },
  "17": {
    "id": "17",
    "title": "Lecture 22 – Dimensionality Reduction",
    "content": "Lecture 22 – Dimensionality Reduction by Josh Hug (Fall 2019) slides video playlist code code HTML Important: This lecture is a combination of two lectures from the Fall 2019 semester. There are a couple of small typos in 20.4. To check whether a set of vectors is an orthonormal set, we should check whether V^T @ V is the identity matrix (not V @ V^T). For matrices whose columns form an orthonormal set, the property that the matrix’s transpose is equivalent to its inverse only holds true if the matrix is square. There is a set of extra slides at the end of the lecture slides. These slides contain a review of concepts in linear algebra such as matrix multiplication and rank. Video Quick Check 22.1 Dimensionality. Visualizing high-dimensional data. 22.1 22.2 More visualizations of high-dimensional data. 22.2 22.3 Matrix decomposition, redundancy, and rank. Introduction to the singular value decomposition (SVD). 22.3 22.4 The theory behind the singular value decomposition. Orthogonality and orthonormality. 22.4 22.5 Low rank approximations with the singular value decomposition. 22.5",
    "url": "http://localhost:4000/fa20/lecture/lec22/",
    "relUrl": "/lecture/lec22/"
  },
  "18": {
    "id": "18",
    "title": "Lecture 23 – Principal Component Analysis",
    "content": "Lecture 23 – Principal Component Analysis by Josh Hug (Fall 2019) slides video playlist code code HTML Live Lecture PCA notes (Optional) PCA Tutorial Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 23.1 Definition and computation of principal components. Geometric interpretation of principal components and low rank approximations. Data centering. 23.1 23.2 Review of regression. Comparing the SVD and regression. Minimizing the perpendicular error. 23.2 23.3 Interpretation of singular values. The relationship between singular values and variance. Analyzing scree plots. 23.3 23.4 Introduction to principal Component analysis (PCA). PCA for exploratory data analysis. 23.4",
    "url": "http://localhost:4000/fa20/lecture/lec23/",
    "relUrl": "/lecture/lec23/"
  },
  "19": {
    "id": "19",
    "title": "Lecture 24 – Clustering",
    "content": "Lecture 24 – Clustering by Josh Hug (Fall 2019) slides video playlist code code HTML Important: This lecture is taken from the Fall 2019 semester. The variant of K-Means mentioned throughout this lecture seeks to minimize distortion, but most packages that implement K-Means (including scikit-learn) seek to minimize inertia instead of distortion. You will work with scikit-learn’s implementation of K-Means in Lab 14. In the lecture, there are a couple of plots that you might not be familiar with. The initial clustering example from 24.1 is taken from the first problem of Fall 2019 Midterm 2, and you will see the state plot from the beginning of 24.5 on Homework 8. Video Quick Check 24.1 Introduction to clustering. Taxonomy of machine learning. Examples of clustering in practice. 24.1 24.2 The K-Means clustering algorithm. Example of K-Means clustering. 24.2 24.3 Loss functions for K-Means. Inertia and distortion. Optimizing distortion. 24.3 24.4 Agglomerative clustering as an alternative to K-Means. Example of agglomerative clustering. Dendrograms and other clustering algorithms. 24.4 24.5 Picking the number of clusters. The elbow method and silhouette scores. Summary of clustering and machine learning. 24.5",
    "url": "http://localhost:4000/fa20/lecture/lec24/",
    "relUrl": "/lecture/lec24/"
  },
  "20": {
    "id": "20",
    "title": "Lecture 25 – Big Data",
    "content": "Lecture 25 – Big Data by Josh Hug (Fall 2018) slides slides PDF video playlist code code HTML Important: This lecture is a combination of two lectures from the Fall 2018 semester. The Google slides version of the lecture slides has a lot of formatting issues. The PDF version might be better for viewing purposes. 25.2 has a little bit of redundancy, so feel free to skip through some of the redundant parts. Unlike the Fall 2018 semester, SQL was covered toward the beginning of the class this summer. There are also some references to “the project” or “project 2,” which would refer to Homework 9 this summer. Video Quick Check 25.1 Data in the organization. Operational data stores and data warehouses. Extract, transform, load (ETL). 25.1 25.2 The multidimensional data model. Fact tables and dimension tables. Star schemas and snowflake schemas. Online analytics processing (OLAP). 25.2 25.3 Data warehouses and data lakes. 25.3 25.4 Distributed file systems and fault tolerance. 25.4 25.5 Distributed aggregation with MapReduce. The MapReduce abstraction. 25.5 25.6 MapReduce technologies. Hadoop and Spark. Resilient Distributed Datasets (RDDs). 25.6 25.7 Spark notebook demo. 25.7",
    "url": "http://localhost:4000/fa20/lecture/lec25/",
    "relUrl": "/lecture/lec25/"
  },
  "21": {
    "id": "21",
    "title": "Lecture 3 – Random Variables",
    "content": "Lecture 3 – Random Variables by Suraj Rampure (Summer 2020) slides video playlist Lecture Recap 1, Part 1 (video) (notes) Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 3.1 Formal definition of random variables. 3.1 3.2 Distributions of random variables. 3.2 3.3 Defining the Bernoulli and binomial distributions. (Stat 88 reading) 3.3 3.4 Discussing equality of random variables – equal vs. equal in distribution. 3.4 3.5 Expectation. Linearity of expectation. Sample calculations, and the expectation of the Bernoulli and binomial distributions. 3.5 3.6 Variance of random variables. Walking through an alternate calculation of variance. Variance of a linear transformation. 3.6 3.7 Deriving the variance of a sum. Understanding covariance, correlation, and independence. 3.7 3.8 Variance of an i.i.d. sum. Variance of the Bernoulli and binomial distributions. 3.8 3.9 Variability of the sample mean. Reviewing inferential concepts from Data 8, but with the framework of random variables. 3.9",
    "url": "http://localhost:4000/fa20/lecture/lec3/",
    "relUrl": "/lecture/lec3/"
  },
  "22": {
    "id": "22",
    "title": "Lecture 4 – SQL",
    "content": "Lecture 4 - SQL by Allen Shen (Summer 2020) slides video playlist code code HTML code walkthrough (Josh Hug) code walkthrough (Allen Shen) Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 4.1 Databases and database management systems. 4.1 4.2 Relational database schemas. 4.2 4.3 SQL overview and the DISTINCT keyword. 4.3 4.4 Types of joins in SQL. 4.4 4.5 NULL values in SQL. 4.5 4.6 SQL predicates and casting. 4.6 4.7 SQL sampling, subqueries, and common table expressions. 4.7 4.8 SQL CASE expressions and the SUBSTR function. 4.8 4.9 SQL summary and conclusion. 4.9 Notebook Notebook walkthrough (Josh Hug). N/A Notebook Notebook walkthrough (Allen Shen). N/A",
    "url": "http://localhost:4000/fa20/lecture/lec4/",
    "relUrl": "/lecture/lec4/"
  },
  "23": {
    "id": "23",
    "title": "Lecture 5 – Pandas, Part 1",
    "content": "Lecture 5 - Pandas, Part 1 by Josh Hug (Fall 2019) slides video playlist code code HTML Intro to Pandas if you’ve taken Data 8 (zip) Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 5.1.1 Pandas data frames, series, and indices. 5.1.1 5.1.2 Pandas indices demo. 5.1.2 5.2 Pandas indexing with the bracket operator. 5.2 5.3 Pandas boolean array selection, the isin function, and the query command. 5.3 5.4.1 Pandas indexing with .loc. 5.4.1 5.4.2 Pandas indexing with .iloc and Pandas sampling. 5.4.2 5.5.1 Pandas utility functions, properties, and the sort_values method. 5.5.1 5.5.2 The value_counts and unique methods in Pandas. An exploration of the baby names data set. 5.5.2",
    "url": "http://localhost:4000/fa20/lecture/lec5/",
    "relUrl": "/lecture/lec5/"
  },
  "24": {
    "id": "24",
    "title": "Lecture 6 – Pandas, Part 2",
    "content": "Lecture 6 - Pandas, Part 2 by Josh Hug (Fall 2019) slides video playlist code code HTML joins code HTML Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 6.1 Pandas string methods. 6.1 6.2 Adding, modifying, and removing columns in Pandas. 6.2 6.3 Using the Pandas groupby function for aggregation. 6.3 6.4 Puzzles using the Pandas groupby function. 6.4 6.5 Other features of the Pandas groupby function including size and filter. 6.5 6.6 Grouping by multiple columns and pivot tables in Pandas. 6.6 6.7 Joining two tables in Pandas. 6.7",
    "url": "http://localhost:4000/fa20/lecture/lec6/",
    "relUrl": "/lecture/lec6/"
  },
  "25": {
    "id": "25",
    "title": "Lecture 7 – Data Cleaning and EDA",
    "content": "Lecture 7 – Data Cleaning and EDA by Joseph Gonzalez (Spring 2020) slides video playlist code (bonus) Joe Hellerstein’s primer on data models Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 7.1 Exploratory data analysis and its position in the data science lifecycle. The relationship between data cleaning and EDA. 7.1 7.2 Exploring various different data storage formats and their tradeoffs. 7.2 7.3 Primary keys and foreign keys. Eliminating redundancy in tables. 7.3 7.4 Defining and discussing the terms quantitative discrete, quantitative continuous, qualitative ordinal, qualitative nominal. 7.4 7.5 Discussing the granularity and scope of our data to ensure that it&#39;s appropriate for analysis. Discussing various methods of encoding time, and flaws to be aware of. 7.5 7.6 Ways in which our data can be incorrect or corrupt. Different methods for addressing missing values, and their tradeoffs. 7.6 7.7 Summarizing the process of EDA, and a demo of EDA on real data. 7.7",
    "url": "http://localhost:4000/fa20/lecture/lec7/",
    "relUrl": "/lecture/lec7/"
  },
  "26": {
    "id": "26",
    "title": "Lecture 8 – Regular Expressions",
    "content": "Lecture 8 - Regular Expressions by Josh Hug (Fall 2019) slides video playlist code code HTML Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 8.1 Canonicalizing strings and using the split method to extract from textual data. 8.1 8.2 Basic regular expression syntax (i.e. closures). Order of operations in regular expressions. 8.2 8.3 Expanded regular expression syntax (i.e. character classes). A couple of regular expression exercises. 8.3 8.4 Limitations of regular expressions. Other regular expression syntax (i.e. lazy closures). 8.4 8.5 Using regular expressions in Python. Regular expression groups. 8.5 8.6 Regular expression case studies on police data and restaurant data. 8.6",
    "url": "http://localhost:4000/fa20/lecture/lec8/",
    "relUrl": "/lecture/lec8/"
  },
  "27": {
    "id": "27",
    "title": "Lecture 9 – Visualization, Part 1",
    "content": "Lecture 9 – Visualization, Part 1 by Suraj Rampure (Summer 2020) slides video playlist code code HTML Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 9.1 Formal definition of visualization. The purpose of visualization in the data science lifecycle. 9.1 9.2 Different ways we can map from data to properties of a visualization. 9.2 9.3 Defining distributions, and determining whether or not given visualizations contain a distribution. 9.3 9.4 Bar plots as a means of displaying the distribution of a qualitative variable, as well as for plotting a quantitative variable across several different categories. 9.4 9.5 Rug plots. Histograms, where areas are proportions. Reviewing histogram calculations from Data 8. Density curves as smoothed versions of histograms. 9.5 9.6 Describing distributions of quantitative variables using terms such as modes, skew, tails, and outliers. 9.6 9.7 Using box plots and violin plots to visualize quantitative distributions. Using overlaid histograms and density curves, and side by side box plots and violin plots, to compare multiple quantitative distributions. 9.7 9.8 Using scatter plots, hex plots, and contour plots to visualize the relationship between pairs of quantitative variables. Summary of visualization thus far. 9.8",
    "url": "http://localhost:4000/fa20/lecture/lec9/",
    "relUrl": "/lecture/lec9/"
  },
  "28": {
    "id": "28",
    "title": "Resources",
    "content": "Resources Exam Resources Semester Midterm 1 Midterm 2 Final Summer 2020 Exam (Solutions) Exam (Solutions) Exam (Solutions) Spring 2020 Checkpoint (Solutions)   N/A Fall 2019 Exam (Solutions) Exam (Solutions) Exam (Solutions) Summer 2019 Exam (Solutions) [Video]   Exam (Solutions) Spring 2019 Exam (Solutions) [Video] Exam (Solutions) [Video] Exam (Solutions) Fall 2018 Exam (Solutions)   Exam (Solutions) Spring 2018 Exam (Solutions)   Exam (Solutions) [Video] Fall 2017 Exam (Solutions) [Video]   Exam (Solutions) Spring 2017 Exam (Solutions)   Exam (Solutions) Spring 2020 Checkpoint Reference Sheet Fall 2019 Midterm 1 Reference Sheet Spring 2019 Midterm 1 Reference Sheet Other Resources We will be posting all lecture materials on the course syllabus. In addition, they will also be listed in the following publicly visible Github Repo. Here is a collection of resources that will help you learn more about various concepts and skills covered in the class. Learning by reading is a key part of being a well rounded data scientist. We will not assign mandatory reading but instead encourage you to look at these and other materials. If you find something helpful, post it on Piazza, and consider contributing it to the course website. You can send us changes to the course website by forking and sending a pull request to the course website github repository. You will then become part of the history of Data 100 at Berkeley. Local Setup Click here to read our guide on how to set up our development environment locally (as an alternative to using DataHub). SQL Resources We’ve assembled some SQL Review Slides to help you brush up on SQL. We’ve also compiled a list of SQL practice problems, which can be found here, along with their solutions. This SQL Cheat Sheet is an awesome resource that was created by Luke Harrison, a former Data 100 student. Probability Practice We’ve compiled a few practice probability problems that we believe may help in understanding the ideas covered in the course. They can be found here, along with their solutions. We’d also like to point you to the textbook for Stat 88, an introductory probability course geared towards data science students at Berkeley. Regex Practice We’ve organized some regex problems to help you get extra practice on regex in a notebook format. They can be found here, along with their solutions. Web References As a data scientist you will often need to search for information on various libraries and tools. In this class we will be using several key python libraries. Here are their documentation pages: The Bash Command Line: Linux and Bash: Intro to Linux, Cloud Computing (which you can skip for the purposes of this class), and the Bash command line. You can skip all portions that don’t pertain to using the command line. Bash Part 2: Part 2 of the intro to command line. Python: Python Tutorial: Teach yourself python. This is a pretty comprehensive tutorial. Python + Numpy Tutorial this tutorial provides a great overview of a lot of the functionality we will be using in DS100. Python 101: A notebook demonstrating a lot of python functionality with some (minimal explanation). Data Visualization: matplotlib.pyplot tutorial: This short tutorial provides an overview of the basic plotting utilities we will be using. Altair Documentation: Altair(Vega-Lite) is a new and powerful visualization library. We might not get to teach it this semester, but you should check it out if you are interested in pursuing visualization deeper. In particular, you should find the example gallery helpful. Prof. Jeff Heer’s Visualization Curriculum: This repository contains a series of Python-based Jupyter notebooks that teaches data visualization using Vega-Lite and Altair. If you are interested in learning more about data visualization, you can find more materials in: Edward Tufte’s book sequences – a classic! Prof. Heer’s class. Pandas: The Pandas Cookbook: This provides a nice overview of some of the basic Pandas functions. However, it is slightly out of date. Learn Pandas A set of lessons providing an overview of the Pandas library. Python for Data Science Another set of notebook demonstrating Pandas functionality. Books Because data science is a relatively new and rapidly evolving discipline there is no single ideal textbook for this subject. Instead we plan to use reading from a collection of books all of which are free. However, we have listed a few optional books that will provide additional context for those who are interested. Principles and Techniques of Data Science This is the accompanying textbook written for DS100 course. Introduction to Statistical Learning (Free online PDF) This book is a great reference for the machine learning and some of the statistics material in the class Data Science from Scratch (Available as eBook for Berkeley students) This more applied book covers many of the topics in this class using Python but doesn’t go into sufficient depth for some of the more mathematical material. Doing Data Science (Available as eBook for Berkeley students) This books provides a unique case-study view of data science but uses R and not Python. Python for Data Analysis (Available as eBook for Berkeley students). This book provides a good reference for the Pandas library. Data Science Education Interested in bringing the Data Science major or curriculum to your academic institution? Please fill out this form if you would like support from Berkeley in offering some variant of our Data Science courses at your institution (or just to let us know that you’re interested). Information about the courses appear at data8.org and ds100.org. Please note that this form is for instructors. If you are only interested in learning Python or data science, please look at our Data 8 or Data 100 websites mentioned above.",
    "url": "http://localhost:4000/fa20/resources/",
    "relUrl": "/resources/"
  },
  "29": {
    "id": "29",
    "title": "Local Setup",
    "content": "Local Setup We will still be using datahub as our primary computing environment. This page serves as a guide for alternative environment setup. In other words: you don’t have to follow these instructions unless you’d like an alternative to datahub. Contents Installing conda by OS OSX Windows Linux Creating your environment Working on assignments locally Opening notebooks locally Verifying your environment Removing the environment to start over Submitting your work FAQ OSX You will need access to the command line. On a Mac, you can open the Terminal by opening Spotlight (Cmd + Space) and typing &quot;Terminal&quot;. Alternatively, you can go to your Applications screen and select Terminal (it might be in the folder named &quot;Other&quot;) Homebrew is a package manager for OSX. If you haven’t already, install it by running the following in the command line (copy, paste, and enter): # This downloads the Ruby code of the installation script and runs it /usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; Verify your installation by making sure brew --version doesn’t error at your terminal. Download and install Anaconda: # Uses curl to download the installation script curl https://repo.continuum.io/miniconda/Miniconda2-4.5.11-MacOSX-x86_64.sh &gt; miniconda.sh # Run the miniconda installer (you will need to enter your password) bash miniconda.sh Close and restart your terminal. Ensure the installation worked by running conda --version. You may remove the miniconda.sh script now if you’d like. Click here to continue to the next part of the setup. Windows Windows is especially prone to error if you aren’t careful about your configuration. If you’ve already had Anaconda or git installed and can’t get the other to work, try uninstalling everything and starting from scratch. Installing Anaconda: Visit the Anaconda website and download the installer for Python 3.7. Download the 64-bit installer if your computer is 64-bit (most likely), the 32-bit installer if not. See this FAQ if you are unsure. Run the exe file to install Anaconda. Leave all the options as default (install for all users, in the default location). Make sure both of these checkboxes are checked: 1) Verify that the installation is working by starting the Anaconda Prompt (you should be able to start it from the Start Menu) and typing python: Notice how the python prompt shows that it is running from Anaconda. Now you have conda installed! From now on, when we talk about the “Terminal” or “Command Prompt”, we are referring to the Anaconda Prompt that you just installed. Click here to continue to the next part of the setup. Linux These instructions assume you have apt-get (Ubuntu and Debian). For other distributions of Linux, substitute the appropriate package manager. Your terminal program allows you to type commands to control your computer. On Linux, you can open the Terminal by going to the Applications menu and clicking “Terminal”. Install wget. This is a command-line tool that lets you download files / webpages at the command line. sudo apt-get install wget Download the Anaconda installation script: wget -O install_anaconda.sh https://repo.continuum.io/miniconda/Miniconda2-4.5.11-Linux-x86_64.sh 4) Install Anaconda: bash install_anaconda.sh 5) Close and restart your terminal. Ensure the installation worked by running `conda --version`. You may remove the install_anaconda.sh script now if you’d like. Click here to continue to the next part of the setup. Creating your environment These instructions are the same for OSX, Windows, and Linux. Download the data100 data100_environment.yml] from the course repository here or: # download via curl curl https://raw.githubusercontent.com/DS-100/su20/gh-pages/resources/assets/local_setup/data100_environment.yml &gt; data100_environment.yml # OR download via wget wget -O data100_environment.yml https://raw.githubusercontent.com/DS-100/su20/gh-pages/resources/assets/local_setup/data100_environment.yml This YAML file is what we use to specify the dependencies and packages (and their versions) we wish to install into the conda environment we will make for this class. The purpose of the environment is to ensure that everyone in the course is using the same package versions for every assignment whether or not they are working on datahub. This is to prevent inconsistent behavior due to differences in package versions. Using the Terminal, navigate to the directory where you downloaded data100_environment.yml. Run these commands to create a new conda environment. Each conda environment maintains its own package versions, allowing us to switch between package versions easily. For example, this class uses Python 3, but you might have another that uses Python 2. With a conda environment, you can switch between those at will. # sanity check on conda installation. Should be 4.5 or higher conda --version # update conda just in case it&#39;s out of date # enter y if prompted to proceed conda update conda # download git conda install -c anaconda git # Create a python 3.6 conda environment with the full set # of packages specified in environment.yml (jupyter, numpy, pandas, ...) conda env create -f data100_environment.yml # Switch to the data100 environment conda activate data100 # Check if packages are in the environment # This should not be empty! conda list From now on, you can switch to the data100 env with conda activate data100, and switch back to the default env with conda deactivate. Working on assignments locally These instructions are the same for OSX, Windows, and Linux. To work on assignments, you should fetch the assignment on datahub, navigate to the assignment folder and click on the download icon on the top right: Then you can unzip the files into a folder of your choosing. Remember the location of your assignment files because you’ll need to navigate to that folder to open the notebook. Opening notebooks locally To open Jupyter notebooks, you’ll navigate to parent directory of the assignment in your terminal, activate the environment, and start up a jupyter server. This will look something like: cd path/to/assignment/directory conda activate data100 jupyter notebook This will automatically open the notebook interface in your browser. You can then browse to a notebook and open it. Make sure to always work in the data100 conda environment when you are using jupyter notebooks for this class. This ensures you have all the necessary packages required for the notebook to run. Verifying Your Environment You can tell if you are correct environment if your terminal looks something like: Additionally, conda env list outputs a list of all your conda environments, and data100 should appear with a * next to it (the active one). Removing the environment to start over If you feel as if you’ve messed up and need to start over, you can remove the environment with conda remove --name data100 --all To verify that the environment was removed, in your Terminal window or an Anaconda Prompt, run: conda info --envs Which should then no longer display the data100 environment. Submitting your work Submissions will still be handled via datahub. To upload your work, navigate to the appropriate assignment folder on datahub and click on the upload button on the top right. Remember to validate, submit, and upload to Gradescope (for homeworks and projects). FAQ Shell not properly configured to use conda activate If you had an older version of Anaconda installed (perhaps for another class), you may see the following message. Follow the instructions in the prompt to: Enable conda for all users sudo ln -s ... Put the base environment on PATH echo &quot;conda activate&quot; &gt;&gt; ~/.bash_profile&quot;. Note that ~/.bash_profile may be something different like ~/.bashrc. Manually remove the line that looks like export PATH=&quot;/usr/local/miniconda3/bin:$PATH&quot; from your .bash_profile. Use your favorite plaintext editor to do this (do not use a rich text editor like Microsoft Word!).",
    "url": "http://localhost:4000/fa20/setup/",
    "relUrl": "/setup/"
  },
  "30": {
    "id": "30",
    "title": "Staff",
    "content": "Staff Jump to Instructors, Teaching Assistants, or Tutors Note: Consult the calendar for the most up-to-date office hours for each GSI. Instructors {% assign instructors = site.staffers | where: &#39;role&#39;, &#39;Instructor&#39; %} {% for staffer in instructors %} {{ staffer }} {% endfor %} Teaching Assistants {% assign teaching_assistants = site.staffers | where: &#39;role&#39;, &#39;Teaching Assistant&#39; %} {% for staffer in teaching_assistants %} {{ staffer }} {% endfor %} Tutors {% assign readers = site.staffers | where: &#39;role&#39;, &#39;Tutor&#39; %} {% for staffer in readers %} {{ staffer }} {% endfor %}",
    "url": "http://localhost:4000/fa20/staff/",
    "relUrl": "/staff/"
  },
  "31": {
    "id": "31",
    "title": "Syllabus",
    "content": "Syllabus Jump to: About Data 100 Online Format Policies This page has yet to be updated for Fall 2020. Please stay tuned! About Data 100 Combining data, computation, and inferential thinking, data science is redefining how people and organizations solve challenging problems and understand their world. This intermediate level class bridges between Data8 and upper division computer science and statistics courses as well as methods courses in other fields. In this class, we explore key areas of data science including question formulation, data collection and cleaning, visualization, statistical inference, predictive modeling, and decision making.​ Through a strong emphasis on data centric computing, quantitative critical thinking, and exploratory data analysis, this class covers key principles and techniques of data science. These include languages for transforming, querying and analyzing data; algorithms for machine learning methods including regression, classification and clustering; principles behind creating informative data visualizations; statistical concepts of measurement error and prediction; and techniques for scalable data processing. Goals Prepare students for advanced Berkeley courses in data-management, machine learning, and statistics, by providing the necessary foundation and context Enable students to start careers as data scientists by providing experience working with real-world data, tools, and techniques Empower students to apply computational and inferential thinking to address real-world problems Prerequisites While we are working to make this class widely accessible, we currently require the following (or equivalent) prerequisites. We are not enforcing prerequisites during enrollment. However, all of the prerequisties will be used starting very early on in the class. It is your responsibility to know the material in the prerequisites.: Foundations of Data Science: Data8 covers much of the material in Data 100 but at an introductory level. Data8 provides basic exposure to python programming and working with tabular data as well as visualization, statistics, and machine learning. Computing: The Structure and Interpretation of Computer Programs (CS 61A) or Computational Structures in Data Science (CS 88). These courses provide additional background in python programming (e.g., for loops, lambdas, debugging, and complexity) that will enable Data 100 to focus more on the concepts in Data Science and less on the details of programming in python. Math: Linear Algebra (Math 54, EE 16a, or Stat89a): We will need some basic concepts like linear operators, eigenvectors, derivatives, and integrals to enable statistical inference and derive new prediction algorithms. This may be satisfied concurrently to Data 100. Online Format This fall, Data 100 will be run entirely online. This section details exactly how each component of the course will operate. To see when any live events are scheduled, check the Calendar. To see when lectures, discussions, and assignments are released (and due), check the Home Page. Policies Collaboration Policy and Academic Dishonesty Assignments Data science is a collaborative activity. While you may talk with others about the homework, we ask that you write your solutions individually in your own words. If you do discuss the assignments with others please include their names at the top of your notebook. Keep in mind that content from assignments will likely be covered on both the midterm and final. If we suspect that you have submitted plagiarized work, we will call you in for a meeting. If we then determine that plagiarism has occurred, we reserve the right to give you a negative full score (-100%) or lower on the assignments in question, along with reporting your offense to the Center of Student Conduct. Rather than copying someone else’s work, ask for help. You are not alone in this course! The entire staff is here to help you succeed. If you invest the time to learn the material and complete the assignments, you won’t need to copy any answers. (taken from 61A) Exams Cheating on exams is a serious offense. We have methods of detecting cheating on exams – so don’t do it! Students caught cheating on any exam will fail this course. We will be following the EECS departmental policy on Academic Honesty, so be sure you are familiar with it. We want you to succeed! If you are feeling overwhelmed, visit our office hours and talk with us. We know college can be stressful – and especially so during the COVID-19 pandemic – and we want to help you succeed.",
    "url": "http://localhost:4000/fa20/syllabus/",
    "relUrl": "/syllabus/"
  }
  
}
