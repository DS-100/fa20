{
  "0": {
    "id": "0",
    "title": "Announcements",
    "content": "Announcements Announcements are stored in the _announcements directory and rendered according to the layout file, _layouts/announcement.html. Week 0 Announcements Welcome to Data 100! We’re still getting this site set up. Some information here may be inaccurate. We will remove this warning once the site is complete. Announcements We will not be updating this page with announcements. For the latest announcements, make sure to check our Piazza.",
    "url": "http://localhost:4000/fa20/announcements/",
    "relUrl": "/announcements/"
  },
  "1": {
    "id": "1",
    "title": "Calendar",
    "content": "Calendar Discussion, Lab, and Special Events Calendar Office Hours Calendar Discussion, Lab, and Special Events Calendar This calendar contains times for live discussion sessions (in red) live lab sessions (in blue) other special events, such as lost office hours (in yellow) To access these events, use the Zoom links posted in @15 on Piazza. Office Hours Calendar GSI and tutor office hours are in grey. Click on each event to see which GSI or tutor is running each office hour time. You should come to these with questions about anything – labs, homeworks, projects, discussions, concepts, etc. To access GSI and tutor office hours, go to our Office Hours Queue. When it’s your turn, you will be given the Zoom link to join. Prof. Joseph and Prof. Perez’ office hours are in dark pink. You should come to these with questions about concepts. To access instructor office hours, use the Zoom links posted on Piazza. Lost office hours are also in dark pink. You should come to these if you feel behind in the course and would like help with the material (but not with assignments).",
    "url": "http://localhost:4000/fa20/calendar/",
    "relUrl": "/calendar/"
  },
  "2": {
    "id": "2",
    "title": "Final Project",
    "content": "Graduate Final Project Coming Soon",
    "url": "http://localhost:4000/fa20/gradproject/",
    "relUrl": "/gradproject/"
  },
  "3": {
    "id": "3",
    "title": "Home",
    "content": "Principles and Techniques of Data Science UC Berkeley, Fall 2020 Anthony Joseph adj@berkeley.edu Fernando Perez fernando.perez@berkeley.edu All announcements are on Piazza. Make sure you are enrolled and active there. Please read our course FAQ before contacting staff with questions that might be answered there. The Syllabus contains a detailed explanation of how each course component will work this fall, given that the course is being taught entirely online. The scheduling of all weekly events is in the Calendar. The Zoom links for all live events are in @15 on Piazza. Week 1 Aug 26 N/A Aug 27 Lecture 1 Introduction, Course Overview (QC due Aug. 31) Ch. 1 Aug 28 Homework 1 Prerequisites (due Sept. 3) Week 2 Aug 31 Lab 1 Prerequisite Coding (due Aug. 31) Sep 1 Lecture 2 Data Sampling and Probability (QC due Sept. 8) Ch. 2 Sep 2 Discussion 1 Linear Algebra and Probability (video) (solutions) Sep 3 Lecture 3 Random Variables (QC due Sept. 8) Ch. 12.1-12.2 Sep 4 Homework 2 Trump Sampling (due Sept. 10) Week 3 Sep 8 Lab 2 SQL (due Sept. 8th) Sep 8 Lecture 4 SQL (QC due Sept. 14) Ch. 9 Sep 9 Discussion 2 Random Variables and SQL (video) (solutions) Sep 10 Lecture 5 Pandas I (QC due Sept. 14) Ch. 3 Sep 11 Project 1 Food Safety (due Sept. 24) Week 4 Sep 14 Lab 3 Pandas I (due Sept. 14) Sep 15 Lecture 6 Pandas II (QC due Sept. 21) Ch. 3 Sep 16 Discussion 3 Pandas (video) (solutions) Sep 17 Lecture 7 Data Cleaning and EDA (QC due Sept. 21) Ch. 4.1, Ch. 5 Sep 18 N/A Week 5 Sep 21 Lab 4 Data Cleaning and EDA (due Sept. 21) Sep 22 Lecture 8 Regular Expressions (QC due Sept. 28) Ch. 8 Sep 23 Discussion 4 Regex (notebook) (video) (solutions) Sep 24 Lecture 9 Visualization I (QC due Sept. 28) Ch. 6.1-6.3 Sep 25 Homework 3 Bike Sharing (due Oct. 1) Week 6 Sep 28 Lab 5 Transformations and KDEs (due Sept. 28) Sep 29 Lecture 10 Visualization II (QC due Oct. 5) Ch. 6.4-6.6 Sep 30 Discussion 5 Visualizations (notebook) (video) (solutions) Oct 1 Lecture 11 Modeling (QC due Oct. 5) Ch. 10 Oct 2 Homework 4 Trump Tweets (due Oct. 8) Week 7 Oct 5 Lab 6 Modeling, Summary Statistics, and Loss Functions (due Oct. 5) Oct 6 Lecture 12 Simple Linear Regression (QC due Oct. 12) Ch. 13.1-13.3 Oct 7 Discussion 6 Modeling and Linear Regression (video) (solutions) Oct 8 Lecture 13 Ordinary Least Squares (QC due Oct. 12) Ch. 13.4 Oct 9 Homework 5 Regression (due Oct. 22) Week 8 Oct 12 Lab 7 Simple Linear Regression (due Oct. 12) Oct 13 Review Sessions Midterm Review Oct 14 Discussion 7 Least Squares (video) (solutions) Oct 15 Exam Midterm (7-9PM PDT) Oct 16 N/A Week 9 Oct 19 N/A Oct 20 Lecture 14 Feature Engineering (QC due Oct. 26) Ch. 14 Survey Mid-Semester Survey (due Oct. 26) Oct 21 Discussion 8 Feature Engineering and Midterm Review (video) Oct 22 Lecture 15 Bias and Variance (QC due Oct. 26) Ch. 12, 15.1-15.2 Oct 23 Homework 6 Housing Week 10 Oct 26 Lab 8 Multiple Linear Regression and Feature Engineering (due Oct. 26) Oct 27 Lecture 16 Regularization &amp; Cross-Validation Ch. 16, Ch. 15.3 Oct 28 Discussion 9 Discussion 9 Oct 29 Lecture 17 Gradient Descent Ch. 11 Oct 30 TBD Week 11 Nov 2 Lab 9 Lab 9 Nov 3 N/A (Election Day) Nov 4 Discussion 10 Discussion 10 Nov 5 Lecture 18 Logistic Regression I Ch. 17.1-17.3 Nov 6 Homework 7 Gradient Descent and Logistic Regression Week 12 Nov 9 Lab 10 Lab 10 Nov 10 Lecture 19 Logistic Regression II, Classification Ch. 17.4-17.7 Nov 11 Discussion 11 Discussion 11 Nov 12 Lecture 20 Decision Trees Nov 13 Project 2 Spam/Ham Week 13 Nov 16 Lab 11 Lab 11 Nov 17 Lecture 21 Inference for Modeling Ch. 18.1, 18.3 Nov 18 Discussion 12 Discussion 12 Nov 19 Lecture 22 Dimensionality Reduction Nov 20 N/A Week 14 Nov 23 Lab 12 Lab 12 Nov 24 Lecture 23 PCA Nov 25 N/A (Thanksgiving) Nov 26 N/A (Thanksgiving) Nov 27 Homework 8 PCA Week 15 Nov 30 Lab 13 Lab 13 Dec 1 Lecture 24 Clustering Dec 2 Discussion 13 Discussion 13 Dec 3 Lecture 25 Big Data, Conclusion Dec 4 N/A Week 16 (RRR Week) Dec 8 Review Dec 10 Review Week 17 (Finals Week) Dec 15 Exam Final Exam (7-10PM PDT)",
    "url": "http://localhost:4000/fa20/",
    "relUrl": "/"
  },
  "4": {
    "id": "4",
    "title": "Lecture 1 – Introduction, Course Overview",
    "content": "Lecture 1 – Introduction, Course Overview Presented by Anthony D. Joseph and Fernando Perez Content by Suraj Rampure, Allen Shen, Joseph Gonzalez, Josh Hug, and Sam Lau slides video playlist code code HTML Welcome to Data 100, and to a new lecture format! The right column of the table below contains Quick Checks. These are required – they are worth 5% of your grade if you are an undergraduate – but are graded on completion, not correctness. A random one of the following six Google Forms will give you an alphanumeric code once you submit; you should take this code and enter it into the “Lecture 1” question in the “Quick Check Codes” assignment on Gradescope to get credit for submitting this Quick Check. You must submit this by Monday, August 31st at 11:59PM to get credit for it. Video Quick Check 1.1 Introductions from Professor Joseph and Professor Perez. 1.1 1.2 What is data science? An overview of the field, and issues to be aware of. 1.2 1.3 An overview of the topics covered in this course. 1.3 1.4 A detailed description of how each course component will run this semester. 1.4 1.5 An overview of the data science lifecycle and its four steps. 1.5 1.6 A demonstration of various data science tools using data from students in this class. 1.6",
    "url": "http://localhost:4000/fa20/lecture/lec01/",
    "relUrl": "/lecture/lec01/"
  },
  "5": {
    "id": "5",
    "title": "Lecture 2 – Data Sampling and Probability",
    "content": "Lecture 2 – Data Sampling and Probability Presented by Fernando Perez and Suraj Rampure Content by Fernando Perez, Suraj Rampure, Ani Adhikari, and Joseph Gonzalez slides video playlist A reminder – the right column of the table below contains Quick Checks. These are required – they are worth 5% of your grade if you are an undergraduate – but are graded on completion, not correctness. A random one of the following six Google Forms will give you an alphanumeric code once you submit; you should take this code and enter it into the “Lecture 2” question in the “Quick Check Codes” assignment on Gradescope to get credit for submitting this Quick Check. You must submit this by Tuesday, September 8th at 11:59PM to get credit for it. Video Quick Check 2.1 Censuses and surveys. Issues with the US Census. 2.1 2.2 Samples. Drawbacks to convenience and quota samples. 2.2 2.3 A case study in sampling bias (1936 election). 2.3 2.4 Sources of bias, and a formal definition of sampling frames. 2.4 2.5 Probability samples, and why we need them. 2.5 2.6 Introducing binomial and multinomial probability calculations. 2.6 2.7 Generalizing binomial and trinomial probability calculations. 2.7 2.8 (Extra) Using permutations and combinations to derive the binomial coefficient. 2.8 2.9 (Extra) Example usages of the binomial coefficient. 2.9",
    "url": "http://localhost:4000/fa20/lecture/lec02/",
    "relUrl": "/lecture/lec02/"
  },
  "6": {
    "id": "6",
    "title": "Lecture 3 – Random Variables",
    "content": "Lecture 3 – Random Variables Presented by Anthony D. Joseph and Suraj Rampure Content by Anthony D. Joseph, Suraj Rampure, Ani Adhikari slides video playlist (supplemental) Stat 88 chapter on distributions A random one of the following six Google Forms will give you an alphanumeric code once you submit; you should take this code and enter it into the “Lecture 3” question in the “Quick Check Codes” assignment on Gradescope to get credit for submitting this Quick Check. You must submit this by Tuesday, September 8th at 11:59PM to get credit for it. Video Quick Check 3.1 Formal definition of random variables. 3.1 3.2 Distributions of random variables. 3.2 3.3 Defining the Bernoulli and binomial distributions. 3.3 3.4 Discussing equality of random variables – equal vs. equal in distribution. 3.4 3.5 Expectation. Linearity of expectation. Sample calculations, and the expectation of the Bernoulli and binomial distributions. 3.5 3.6 Summary, and what&#39;s next. N/A",
    "url": "http://localhost:4000/fa20/lecture/lec03/",
    "relUrl": "/lecture/lec03/"
  },
  "7": {
    "id": "7",
    "title": "Lecture 4 – SQL",
    "content": "Lecture 4 - SQL Presented by Anthony D. Joseph Content by Anthony D. Joseph, Allen Shen, Josh Hug, John DeNero, Joseph Gonzalez slides video playlist code code HTML code walkthrough by Josh Hug code walkthrough by Allen Shen A random one of the following nine Google Forms will give you an alphanumeric code once you submit; you should take this code and enter it into the “Lecture 4” question in the “Quick Check Codes” assignment on Gradescope to get credit for submitting this Quick Check. You must submit this by Monday, September 14th at 11:59PM to get credit for it. Video Quick Check 4.1 Databases and database management systems. 4.1 4.2 Relational database schemas. 4.2 4.3 SQL overview and the DISTINCT keyword. 4.3 4.4 Types of joins in SQL. 4.4 4.5 NULL values in SQL. 4.5 4.6 SQL predicates and casting. 4.6 4.7 SQL sampling, subqueries, and common table expressions. 4.7 4.8 SQL CASE expressions and the SUBSTR function. 4.8 4.9 SQL summary and conclusion. 4.9",
    "url": "http://localhost:4000/fa20/lecture/lec04/",
    "relUrl": "/lecture/lec04/"
  },
  "8": {
    "id": "8",
    "title": "Lecture 5 – Pandas, Part 1",
    "content": "Lecture 5 - Pandas, Part 1 Presented by Fernando Perez Content by Fernando Perez, Josh Hug slides video playlist code code HTML Intro to Pandas if you’ve taken Data 8 (zip) A random one of the following Google Forms will give you an alphanumeric code once you submit; you should take this code and enter it into the “Lecture 5” question in the “Quick Check Codes” assignment on Gradescope to get credit for submitting this Quick Check. You must submit this by Monday, September 14th at 11:59PM to get credit for it. Video Quick Check 5.1.1 Pandas data frames, series, and indices. 5.1.1 5.1.2 Pandas indices demo. 5.1.2 5.2 Pandas indexing with the bracket operator. 5.2 5.3 Pandas boolean array selection, the isin function, and the query command. 5.3 5.4.1 Pandas indexing with .loc. 5.4.1 5.4.2 Pandas indexing with .iloc and Pandas sampling. 5.4.2 5.5.1 Pandas utility functions, properties, and the sort_values method. 5.5.1 5.5.2 The value_counts and unique methods in Pandas. An exploration of the baby names data set. 5.5.2",
    "url": "http://localhost:4000/fa20/lecture/lec05/",
    "relUrl": "/lecture/lec05/"
  },
  "9": {
    "id": "9",
    "title": "Lecture 6 – Pandas, Part 2",
    "content": "Lecture 6 - Pandas, Part 2 Presented by Fernando Perez Content by Fernando Perez, Josh Hug slides video playlist code code HTML, joins code HTML A random one of the following Google Forms will give you an alphanumeric code once you submit; you should take this code and enter it into the “Lecture 6” question in the “Quick Check Codes” assignment on Gradescope to get credit for submitting this Quick Check. You must submit this by Monday, September 21st at 11:59PM to get credit for it. Video Quick Check 6.1 Pandas string methods. 6.1 6.2 Adding, modifying, and removing columns in Pandas. 6.2 6.3 Using the Pandas groupby function for aggregation. 6.3 6.4 Puzzles using the Pandas groupby function. 6.4 6.5 Other features of the Pandas groupby function including size and filter. 6.5 6.6 Grouping by multiple columns and pivot tables in Pandas. 6.6 6.7 Joining two tables in Pandas. 6.7",
    "url": "http://localhost:4000/fa20/lecture/lec06/",
    "relUrl": "/lecture/lec06/"
  },
  "10": {
    "id": "10",
    "title": "Lecture 7 – Data Cleaning and EDA",
    "content": "Lecture 7 – Data Cleaning and EDA Presented by Anthony D. Joseph Content by Anthony D. Joseph, Joseph Gonzalez, Deborah Nolan, Joseph Hellerstein slides video playlist code code HTML (bonus) Joe Hellerstein’s primer on data models A random one of the following Google Forms will give you an alphanumeric code once you submit; you should take this code and enter it into the “Lecture 7” question in the “Quick Check Codes” assignment on Gradescope to get credit for submitting this Quick Check. You must submit this by Monday, September 21st at 11:59PM to get credit for it. Video Quick Check 7.1 Exploratory data analysis and its position in the data science lifecycle. The relationship between data cleaning and EDA. 7.1 7.2 Exploring various different data storage formats and their tradeoffs. 7.2 7.3 Primary keys and foreign keys. Eliminating redundancy in tables. 7.3 7.4 Defining and discussing the terms quantitative discrete, quantitative continuous, qualitative ordinal, qualitative nominal. 7.4 7.5 Discussing the granularity and scope of our data to ensure that it&#39;s appropriate for analysis. Discussing various methods of encoding time, and flaws to be aware of. 7.5 7.6 Ways in which our data can be incorrect or corrupt. Different methods for addressing missing values, and their tradeoffs. 7.6 7.7 Summarizing the process of EDA. 7.7 (Optional) 7.8 A demo of EDA on real data. N/A",
    "url": "http://localhost:4000/fa20/lecture/lec07/",
    "relUrl": "/lecture/lec07/"
  },
  "11": {
    "id": "11",
    "title": "Lecture 8 – Regular Expressions",
    "content": "Lecture 8 - Regular Expressions Presented by Anthony D. Joseph Content by Anthony D. Joseph, Josh Hug slides video playlist code code HTML A random one of the following Google Forms will give you an alphanumeric code once you submit; you should take this code and enter it into the “Lecture 8” question in the “Quick Check Codes” assignment on Gradescope to get credit for submitting this Quick Check. You must submit this by Monday, September 28th at 11:59PM to get credit for it. Video Quick Check 8.1 Motivation and canonicalizing strings. 8.1 8.2 Using the split method to extract from textual data. 8.2 8.3 Basic regular expression syntax (i.e. closures). Order of operations in regular expressions. 8.3 8.4 Expanded regular expression syntax (i.e. character classes). A couple of regular expression exercises. 8.4 8.5 Limitations of regular expressions. Other regular expression syntax (i.e. lazy closures). 8.5 8.6 Using regular expressions in Python. Regular expression groups. 8.6 8.7 Regular expression case studies on police data and restaurant data. 8.7",
    "url": "http://localhost:4000/fa20/lecture/lec08/",
    "relUrl": "/lecture/lec08/"
  },
  "12": {
    "id": "12",
    "title": "Lecture 9 – Visualization, Part 1",
    "content": "Lecture 9 – Visualization, Part 1 Presented by Fernando Perez Content by Fernando Perez, Suraj Rampure, Ani Adhikari, Sam Lau, Yifan Wu slides video playlist code code HTML A random one of the following Google Forms will give you an alphanumeric code once you submit; you should take this code and enter it into the “Lecture 9” question in the “Quick Check Codes” assignment on Gradescope to get credit for submitting this Quick Check. You must submit this by Monday, September 28th at 11:59PM to get credit for it. Video Quick Check 9.1 Formal definition of visualization. The purpose of visualization in the data science lifecycle. 9.1 9.2 Different ways we can map from data to properties of a visualization. 9.2 9.3 Defining distributions, and determining whether or not given visualizations contain a distribution. 9.3 9.4 Bar plots as a means of displaying the distribution of a qualitative variable, as well as for plotting a quantitative variable across several different categories. 9.4 9.5 Rug plots. Histograms, where areas are proportions. Reviewing histogram calculations from Data 8. Density curves as smoothed versions of histograms. 9.5 9.6 Describing distributions of quantitative variables using terms such as modes, skew, tails, and outliers. 9.6 9.7 Using box plots and violin plots to visualize quantitative distributions. Using overlaid histograms and density curves, and side by side box plots and violin plots, to compare multiple quantitative distributions. 9.7 9.8 Using scatter plots, hex plots, and contour plots to visualize the relationship between pairs of quantitative variables. Summary of visualization thus far. 9.8",
    "url": "http://localhost:4000/fa20/lecture/lec09/",
    "relUrl": "/lecture/lec09/"
  },
  "13": {
    "id": "13",
    "title": "Lecture 10 – Visualization, Part 2",
    "content": "Lecture 10 – Visualization, Part 2 Presented by Fernando Perez Content by Fernando Perez, Suraj Rampure, Ani Adhikari, Sam Lau, Yifan Wu, Deborah Nolan slides video playlist code code HTML Extra reading on colormaps: matplotlib colormaps (BIDS) How the Rainbow Color Map Misleads When to use Sequential and Diverging Palettes Color Use Guidelines A random one of the following Google Forms will give you an alphanumeric code once you submit; you should take this code and enter it into the “Lecture 10” question in the “Quick Check Codes” assignment on Gradescope to get credit for submitting this Quick Check. You must submit this by Monday, October 5th at 11:59PM to get credit for it. Video Quick Check 10.1 Ensuring that the axes in our visualizations aren&#39;t misleading. 10.1 10.2 Designing visualizations that are well-suited for making comparisons. 10.2 10.3 How to use color to create effective visualizations. How to choose color schemes that are clear and accessible. 10.3 10.4 How to choose markings that the human eye can easily interpret. Issues to avoid, such as jiggling baselines and overplotting. 10.4 10.5 Discussing the supplemental text that publication-ready plots need. 10.5 10.6 When to use smoothing. How kernel density estimates are created. Looking at various kernels. Understanding the impact of the bandwidth hyperparameter. 10.6 10.7 Discussing why we prefer linear relationships. Understanding how to &quot;reverse-engineer&quot; a linearized relationship to determine the true relationship. Identifying which transformations to use in order to linearize a relationship. 10.7",
    "url": "http://localhost:4000/fa20/lecture/lec10/",
    "relUrl": "/lecture/lec10/"
  },
  "14": {
    "id": "14",
    "title": "Lecture 10 – Visualization, Part 2",
    "content": "Lecture 10 – Visualization, Part 2 by Suraj Rampure (Summer 2020) slides video playlist code code HTML Extra reading on colormaps: How the Rainbow Color Map Misleads When to use Sequential and Diverging Palettes Color Use Guidelines Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 10.1 Ensuring that the axes in our visualizations aren&#39;t misleading. 10.1 10.2 Designing visualizations that are well-suited for making comparisons. 10.2 10.3 How to use color to create effective visualizations. How to choose color schemes that are clear and accessible. 10.3 10.4 How to choose markings that the human eye can easily interpret. Issues to avoid, such as jiggling baselines and overplotting. 10.4 10.5 Discussing the supplemental text that publication-ready plots need. 10.5 10.6 When to use smoothing. How kernel density estimates are created. Looking at various kernels. Understanding the impact of the bandwidth hyperparameter. 10.6 10.7 Discussing why we prefer linear relationships. Understanding how to &quot;reverse-engineer&quot; a linearized relationship to determine the true relationship. Identifying which transformations to use in order to linearize a relationship. 10.7",
    "url": "http://localhost:4000/fa20/lecture/old/lec10/",
    "relUrl": "/lecture/old/lec10/"
  },
  "15": {
    "id": "15",
    "title": "Lecture 11 – Introduction to Modeling",
    "content": "Lecture 11 – Introduction to Modeling by Suraj Rampure (Summer 2020) slides video playlist code code HTML Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 11.1 Motivating examples of models. 11.1 11.2 Defining the constant model. Formalizing the notion of a parameter. 11.2 11.3 Loss functions and their purpose. Squared loss and absolute loss. Minimizing average loss (i.e. empirical risk). 11.3 11.4 Minimizing mean squared error for the constant model using calculus, to show that the sample mean is the optimal model parameter in this case. 11.4 11.5 Performing the same optimization as in the last video, but by using a non-calculus algebraic manipulation. 11.5 11.6 Minimizing mean absolute error for the constant model using calculus, to show that the sample median is the optimal parameter in this case. Identifying that this solution isn&#39;t necessarily unique. 11.6 11.7 Comparing the loss surfaces of MSE and MAE for the constant model. Discussing the benefits and drawbacks of squared and absolute loss. Recapping the &quot;modeling process&quot;. 11.7",
    "url": "http://localhost:4000/fa20/lecture/old/lec11/",
    "relUrl": "/lecture/old/lec11/"
  },
  "16": {
    "id": "16",
    "title": "Lecture 11 – Introduction to Modeling",
    "content": "Lecture 11 – Introduction to Modeling Presented by Fernando Perez and Suraj Rampure Content by Fernando Perez, Suraj Rampure, Ani Adhikari, Deborah Nolan, Joseph Gonzalez slides video playlist code code HTML A random one of the following Google Forms will give you an alphanumeric code once you submit; you should take this code and enter it into the “Lecture 11” question in the “Quick Check Codes” assignment on Gradescope to get credit for submitting this Quick Check. You must submit this by Monday, October 5th at 11:59PM to get credit for it. Video Quick Check 11.1 Motivating examples of models. 11.1 11.2 Defining the constant model. Formalizing the notion of a parameter. 11.2 11.3 Loss functions and their purpose. Squared loss and absolute loss. Minimizing average loss (i.e. empirical risk). 11.3 11.4 Minimizing mean squared error for the constant model using calculus, to show that the sample mean is the optimal model parameter in this case. 11.4 11.5 Performing the same optimization as in the last video, but by using a non-calculus algebraic manipulation. 11.5 11.6 Minimizing mean absolute error for the constant model using calculus, to show that the sample median is the optimal parameter in this case. Identifying that this solution isn&#39;t necessarily unique. 11.6 11.7 Comparing the loss surfaces of MSE and MAE for the constant model. Discussing the benefits and drawbacks of squared and absolute loss. Recapping the &quot;modeling process&quot;. 11.7",
    "url": "http://localhost:4000/fa20/lecture/lec11/",
    "relUrl": "/lecture/lec11/"
  },
  "17": {
    "id": "17",
    "title": "Lecture 12 – Simple Linear Regression",
    "content": "Lecture 12 – Simple Linear Regression Presented by Anthony D. Joseph and Suraj Rampure Content by Suraj Rampure and Ani Adhikari slides video playlist code code HTML A random one of the following Google Forms will give you an alphanumeric code once you submit; you should take this code and enter it into the “Lecture 12” question in the “Quick Check Codes” assignment on Gradescope to get credit for submitting this Quick Check. You must submit this by Monday, October 12th at 11:59PM to get credit for it. Video Quick Check 12.0 Introduction and recap of the modeling process. 12.0 12.1 The correlation coefficient and its properties. 12.1 12.2 Defining the simple linear regression model, our first model with two parameters and an input variable. Motivating linear regression with the graph of averages. 12.2 12.3 Using calculus to derive the optimal model parameters for the simple linear regression model, when we choose squared loss as our loss function. 12.3 12.4 Visualizing and interpreting loss surface of the SLR model. 12.4 12.5 Interpreting the slope of the simple linear model. 12.5 12.6 Defining key terminology in the regression context. Expanding the simple linear model to include any number of features. 12.6 12.7 RMSE as a metric of accuracy. Multiple R-squared as a metric of explained variation. Summary. 12.7",
    "url": "http://localhost:4000/fa20/lecture/lec12/",
    "relUrl": "/lecture/lec12/"
  },
  "18": {
    "id": "18",
    "title": "Lecture 12 – Simple Linear Regression",
    "content": "Lecture 12 – Simple Linear Regression by Suraj Rampure (Summer 2020) slides video playlist code code HTML Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 12.1 The correlation coefficient and its properties. 12.1 12.2 Defining the simple linear regression model, our first model with two parameters and an input variable. Motivating linear regression with the graph of averages. 12.2 12.3 Using calculus to derive the optimal model parameters for the simple linear regression model, when we choose squared loss as our loss function. 12.3 12.4 Visualizing and interpreting loss surface of the SLR model. 12.4 12.5 Interpreting the slope of the simple linear model. 12.5 12.6 Defining key terminology in the regression context. Expanding the simple linear model to include any number of features. 12.6 12.7 RMSE as a metric of accuracy. Multiple R-squared as a metric of explained variation. Summary. 12.7",
    "url": "http://localhost:4000/fa20/lecture/old/lec12/",
    "relUrl": "/lecture/old/lec12/"
  },
  "19": {
    "id": "19",
    "title": "Lecture 13 – Ordinary Least Squares",
    "content": "Lecture 13 – Ordinary Least Squares by Suraj Rampure (Summer 2020) slides video playlist code code HTML Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 13.1 A quick recap of the modeling process, and a roadmap for lecture. 13.1 13.2 Defining the multiple linear regression model using linear algebra (dot products and matrix multiplication). Introducing the idea of a design matrix. 13.2 13.3 Defining the mean squared error of the multiple linear regression model as the (scaled) norm of the residual vector. 13.3 13.4 Using a geometric argument to determine the optimal model parameter. 13.4 13.5 Residual plots. Properties of residuals, with and without an intercept term in our model. 13.5 13.6 Discussing the conditions in which there isn&#39;t a unique solution for the optimal model parameter. A summary, and outline of what is to come. 13.6",
    "url": "http://localhost:4000/fa20/lecture/old/lec13/",
    "relUrl": "/lecture/old/lec13/"
  },
  "20": {
    "id": "20",
    "title": "Lecture 13 – Ordinary Least Squares",
    "content": "Lecture 13 – Ordinary Least Squares Presented by Anthony D. Joseph and Suraj Rampure Content by Suraj Rampure, Ani Adhikari, Deb Nolan, Joseph Gonzalez slides video playlist code code HTML The Quick Check for this lecture is due Monday, October 12th at 11:59PM. In order to get the Gradescope submission code, you will have to follow the instructions at the end of one of these Google Forms; the instructions for this lecture are more involved as we will have you access the exam platform that we are using for the Midterm exam next week. Video Quick Check 13.1 A quick recap of the modeling process, and a roadmap for lecture. 13.1 13.2 Defining the multiple linear regression model using linear algebra (dot products and matrix multiplication). Introducing the idea of a design matrix. 13.2 13.3 Defining the mean squared error of the multiple linear regression model as the (scaled) norm of the residual vector. 13.3 13.4 Using a geometric argument to determine the optimal model parameter. 13.4 13.5 Residual plots. Properties of residuals, with and without an intercept term in our model. 13.5 13.6 Discussing the conditions in which there isn&#39;t a unique solution for the optimal model parameter. A summary, and outline of what is to come. 13.6",
    "url": "http://localhost:4000/fa20/lecture/lec13/",
    "relUrl": "/lecture/lec13/"
  },
  "21": {
    "id": "21",
    "title": "Lecture 14 – Feature Engineering",
    "content": "Lecture 14 – Feature Engineering Presented by Joseph Gonzalez Content by Joseph Gonzalez, John DeNero, Josh Hug slides video playlist code code HTML: Part 1, Part 2, Part 3 (supplementary) video and code HTML from a live lecture in Summer 2020 that reinforced some of the mathematical ideas in this lecture Important: This lecture is a combination of two lectures from previous semesters (this is why the video titles don’t match our numbering). Read this before proceeding with the lectures, as it details which concepts you should focus on. Sections 14.1 through 14.4 discuss the core techniques of feature engineering. Slides are linked above, and code is in “Part 1” and “Part 2”. 14.1: Throughout this lecture, Radial Basis Functions are used as an example. For our purposes, they are purely an example, and are not in-scope. 14.2, 14.3: Entirely in scope. 14.4: Of the three techniques discussed, one-hot encoding is most important, though the others are still in scope. Sections 14.5 through 14.7 discuss pitfalls to be aware of in feature engineering. There are no accompanying slides; these ideas are primarily explained in the lecture notebook “Part 3”. 14.5: Focus on the numerical ideas here, not the syntax of model creation (though the code is linked above). 14.6: The focus of this video is about the content at the end where our design matrix has too many columns, not about the details of Radial Basis Functions. 14.7: See the above comment. The Quick Check for this lecture is due Monday, October 26th at 11:59PM. To get credit for this lecture’s Quick Checks, you will have to fill out all of the following Google Forms as well as the mid-semester survey, linked on the course website and on Piazza (as well as in one of the following forms). Video Quick Check 14.1 A demonstration of how to use scikit-learn to fit linear models. 14.1 14.2 Feature functions, as a method of transforming existing numerical data, and encoding non-numerical data for use in modeling. 14.2 14.3 Defining what it means for a model to be linear. The constant feature. More sophisticated numerical features. 14.3 14.4 Numerically encoding categorical data using various encodings (one-hot, bag of words, n-gram). 14.4 14.5 Issues we may run into when our design matrix has redundant features. 14.5 14.6 Issues we may run into when our design matrix has more features than observations. Radial basis functions. 14.6 14.7 Overfitting our model to the data we used to train it leads to poor generalizability to unseen data, which is the goal of modeling. 14.7",
    "url": "http://localhost:4000/fa20/lecture/lec14/",
    "relUrl": "/lecture/lec14/"
  },
  "22": {
    "id": "22",
    "title": "Lecture 14 – Feature Engineering",
    "content": "Lecture 14 – Feature Engineering by Joseph Gonzalez (Spring 2020) slides video playlist code code HTML: Part 1, Part 2, Part 3 Important: This lecture is a combination of two lectures from Spring 2020 (this is why the video titles don’t match our numbering). Read this before proceeding with the lectures, as it details which concepts you should focus on. Sections 14.1 through 14.4 discuss the core techniques of feature engineering. Slides are linked above, and code is in “Part 1” and “Part 2”. 14.1: Throughout this lecture, Radial Basis Functions are used as an example. For our purposes, they are purely an example, and are not in-scope. 14.2, 14.3: Entirely in scope. 14.4: Of the three techniques discussed, one-hot encoding is most important, though the others are still in scope. Sections 14.5 through 14.7 discuss pitfalls to be aware of in feature engineering. There are no accompanying slides; these ideas are primarily explained in the lecture notebook “Part 3”. 14.5: Focus on the numerical ideas here, not the syntax of model creation (though the code is linked above). 14.6: The focus of this video is about the content at the end where our design matrix has too many columns, not about the details of Radial Basis Functions. 14.7: See the above comment. Video Quick Check 14.1 A demonstration of how to use scikit-learn to fit linear models. 14.1 14.2 Feature functions, as a method of transforming existing numerical data, and encoding non-numerical data for use in modeling. 14.2 14.3 Defining what it means for a model to be linear. The constant feature. More sophisticated numerical features. 14.3 14.4 Numerically encoding categorical data using various encodings (one-hot, bag of words, n-gram). 14.4 14.5 Issues we may run into when our design matrix has redundant features. 14.5 14.6 Issues we may run into when our design matrix has more features than observations. Radial basis functions. 14.6 14.7 Overfitting our model to the data we used to train it leads to poor generalizability to unseen data, which is the goal of modeling. 14.7",
    "url": "http://localhost:4000/fa20/lecture/old/lec14/",
    "relUrl": "/lecture/old/lec14/"
  },
  "23": {
    "id": "23",
    "title": "Lecture 15 – Bias and Variance",
    "content": "Lecture 15 – Bias and Variance by Ani Adhikari (Spring 2020) slides video playlist Bias-Variance decomposition derivation Important: This lecture is taken from Spring 2020 (which again is why the video titles don’t match up with our numbering). In order to follow it, you must be familiar with the ideas from Lecture 3 (Random Variables). In the last lecture, we touched on this idea of “model complexity”. It is mentioned towards the end of this lecture, but will be covered more in-depth in Lecture 16 (Cross-Validation and Regularization). The algebra behind the decomposition of model risk into observational variance, model variance, and bias, is not in the slides or video but is in the link above. You should read it after watching this lecture. Video Quick Check 15.1 Introducing the data generating process and prediction error. Model risk. 15.1 15.2 Looking at different sources of error in our model – observation variance, model variance, and bias – and discussing how to mitigate them. 15.2 15.3 Decomposing model risk into the sum of observation variance, model variance, and the square of bias. 15.3",
    "url": "http://localhost:4000/fa20/lecture/old/lec15/",
    "relUrl": "/lecture/old/lec15/"
  },
  "24": {
    "id": "24",
    "title": "Lecture 15 – Bias and Variance",
    "content": "Lecture 15 – Bias and Variance Presented by Fernando Perez Content by Fernando Perez, Ani Adhikari, Suraj Rampure slides video playlist Bias-Variance decomposition derivation Important: The algebra behind the decomposition of model risk into observational variance, model variance, and bias, is not in the slides or video but is in the link above. You should read it after watching this lecture. Also, you may want to review Lecture 3 for a refresher on random variables. The Quick Check for this lecture is due Monday, October 26th at 11:59PM. A random one of the following Google Forms will give you an alphanumeric code once you submit; you should take this code and enter it into the “Lecture 15” question in the “Quick Check Codes” assignment on Gradescope to get credit for submitting this Quick Check. Video Quick Check 15.1 Variance of random variables. Walking through an alternate calculation of variance. Variance of a linear transformation. 15.1 15.2 Deriving the variance of a sum. Understanding covariance, correlation, and independence. 15.2 15.3 Variance of an i.i.d. sum. Variance of the Bernoulli and binomial distributions. 15.3 15.4 Variability of the sample mean. Reviewing inferential concepts from Data 8, but with the framework of random variables. 15.4 15.5 Introducing the data generating process and prediction error. Model risk. 15.5 15.6 Looking at different sources of error in our model – observation variance, model variance, and bias – and discussing how to mitigate them. 15.6 15.7 Decomposing model risk into the sum of observation variance, model variance, and the square of bias. 15.7",
    "url": "http://localhost:4000/fa20/lecture/lec15/",
    "relUrl": "/lecture/lec15/"
  },
  "25": {
    "id": "25",
    "title": "Lecture 16 – Cross-Validation and Regularization",
    "content": "Lecture 16 – Cross-Validation and Regularization by Joseph Gonzalez, Paul Shao (Spring 2020), and Suraj Rampure (Summer 2020) slides video playlist code code HTML: Part 1, Part 2 Important: This lecture is a combination of several lectures from Spring 2020 (this is why the video titles don’t match our numbering), plus a piece of “glue” added in this summer. Read this before proceeding with the lectures, as it details what materials you should focus on. Sections 16.1 through 16.4 discuss train-test splits and cross-validation. 16.1 walks through why we need to split our data into train and test in the first place, and how cross-validation works. It primarily consists of slides. 16.2 and 16.3 walk through the process of creating a basic train-test split, and evaluating models that we’ve fit on our training data using our testing data. Code is in “Part 1”. 16.4 walks through the process of implementing cross-validation. In this video there references to a Pipeline object in scikit-learn. This is not in scope for Summer 2020, so do not worry about its details. Code is in “Part 1”. Sections 16.5 and 16.6 discuss regularization. 16.5 discusses why we need to regularize, and how penalties on the norm of our parameter vector accomplish this goal. 16.6 explicitly lists the optimal model parameter when using the L2 penalty on our linear model (called “ridge regression”). There are also three supplementary videos accompanying this lecture. They don’t introduce any new material, but may still be helpful for your understanding. They are listed as supplementary and not required since the runtime of this lecture is already quite long. They do not have accompanying Quick Checks for this reason. 16.7 and 16.8 walk through implementing ridge and LASSO regression in a notebook. These videos are helpful in explaining how regularization and cross-validation are used in practice. These videos again use Pipeline, which is not in scope. Code is in “Part 2”. 16.9 is another supplementary video, created by Paul Shao (a TA for Data 100 in Spring 2020). It gives a great high-level overview of both the bias-variance tradeoff and regularization. Video Quick Check 16.1 Training error vs. testing error. Why we need to split our data into train and test. How cross-validation works, and why it is useful. 16.1 16.2 Using scikit-learn to construct a train-test split. 16.2 16.3 Building a linear model and determining its training and test error. 16.3 16.4 Implementing cross-validation, and using it to help select a model. 16.4 16.5 An overview of regularization. 14.5 16.6 Ridge regression and LASSO regression. 16.6 16.7 *Supplemental.* Using ridge regression and cross-validation in scikit-learn. N/A 16.8 *Supplemental.* Using LASSO regression and cross-validation in scikit-learn. N/A 16.9 *Supplemental.* An overview of the bias-variance tradeoff, and how it interfaces with regularization. N/A",
    "url": "http://localhost:4000/fa20/lecture/old/lec16/",
    "relUrl": "/lecture/old/lec16/"
  },
  "26": {
    "id": "26",
    "title": "Lecture 17 – Gradient Descent",
    "content": "Lecture 17 – Gradient Descent by Josh Hug (Fall 2019) and Joseph Gonzalez (Spring 2020) slides video playlist code code HTML Important: This lecture is taken from both Fall 2019 and Spring 2020. In order to follow the lecture, you should be familiar with the ideas from Discussion 1 Problem 2 (Calculus). The reference to Homework 6 Problem 7 in 17.2 should be a reference to Homework 5 Problem 3. In Homework 7, you will get more practice with learning rates and gradient descent. There is an updated version of the Loss Game mentioned in 17.3. Video Quick Check 17.1 Gradient descent in one dimension. Convexity. 17.1 17.2 Various methods of optimizing loss functions in one dimension. 17.2 17.3 Gradient descent in multiple dimensions. Interpretation of gradients. 17.3 17.4 Stochastic gradient descent (SGD). Comparison between gradient descent and SGD. 17.4",
    "url": "http://localhost:4000/fa20/lecture/old/lec17/",
    "relUrl": "/lecture/old/lec17/"
  },
  "27": {
    "id": "27",
    "title": "Lecture 18 – Logistic Regression, Part 1",
    "content": "Lecture 18 – Logistic Regression, Part 1 by Suraj Rampure (Summer 2020) slides video playlist code code HTML Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 18.1 Classification, and a brief overview of the machine learning taxonomy. 18.1 18.2 Pitfalls of using least squares to model probabilities. Creating a graph of averages to motivate the logistic regression model. 18.2 18.3 Deriving the logistic regression model from the assumption that the log-odds of the probability of belonging to class 1 is linear. 18.3 18.4 Formalizing the logistic regression model. Exploring properties of the logistic function. Interpreting the model coefficients. 18.4 18.5 Discussing the pitfalls of using squared loss with logistic regression. 18.5 18.6 Introducing cross-entropy loss, as a better alternative to squared loss for logistic regression. 18.6 18.7 Using maximum likelihood estimation to arrive at cross-entropy loss. 18.7 18.8 Demo of using scikit-learn to fit a logistic regression model. An overview of what&#39;s coming next. 18.8",
    "url": "http://localhost:4000/fa20/lecture/old/lec18/",
    "relUrl": "/lecture/old/lec18/"
  },
  "28": {
    "id": "28",
    "title": "Lecture 19 – Logistic Regression Part 2, Classification",
    "content": "Lecture 19 – Logistic Regression Part 2, Classification by Suraj Rampure (Summer 2020) slides video playlist code code HTML Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 19.1 Using thresholds to convert from predicted probabilities to classifications. 19.1 19.2 Defining several metrics of classifier performance – accuracy, precision, and recall. Confusion matrices. 19.2 19.3 Using scikit-learn to compute accuracy, precision, recall, and confusion matrices. 19.3 19.4 Exploring how threshold impacts accuracy, precision, and recall. Precision-recall curves. ROC curves. AUC. 19.4 19.5 Exploring the decision boundaries that result from a logistic regression classifier, and their relationship to the model&#39;s parameters. 19.5 19.6 Linear separability. Why we sometimes need regularization for logistic regression. 19.6 19.7 Summary. Brief introduction to multiclass classification. N/A",
    "url": "http://localhost:4000/fa20/lecture/old/lec19/",
    "relUrl": "/lecture/old/lec19/"
  },
  "29": {
    "id": "29",
    "title": "Lecture 2 – Data Sampling and Probability",
    "content": "Lecture 2 – Data Sampling and Probability by Suraj Rampure (Summer 2020) slides video playlist This is the first lecture of a brand-new format! Make sure to complete the Quick Check problems as you progress through the videos to confirm your understanding. Video Quick Check 2.1 Censuses and surveys. Issues with the US Census. 2.1 2.2 Samples. Drawbacks to convenience and quota samples. 2.2 2.3 A case study in sampling bias (1936 election). 2.3 2.4 Sources of bias, and a formal definition of sampling frames. 2.4 2.5 Probability samples, and why we need them. 2.5 2.6 Introducing binomial and multinomial probability calculations. 2.6 2.7 Generalizing binomial and trinomial probability calculations. 2.7 2.8 (Extra) Using permutations and combinations to derive the binomial coefficient. 2.8 2.9 (Extra) Example usages of the binomial coefficient. 2.9",
    "url": "http://localhost:4000/fa20/lecture/old/lec2/",
    "relUrl": "/lecture/old/lec2/"
  },
  "30": {
    "id": "30",
    "title": "Lecture 20 – Decision Trees",
    "content": "Lecture 20 – Decision Trees by Josh Hug (Fall 2019) slides video playlist code code HTML Important: This lecture is taken from the Fall 2019 semester. The reference to Lecture 22 in 20.1 should be a reference to Lecture 19. The references to lec25-decision-trees.ipynb in 20.1 should be references to lec20-decision-trees.ipynb. The slides in 20.4 should say: “Bagging often isn’t enough to reduce model variance!” Without selecting a random subset of features at each split, the decision trees fitted on bagged data often look very similar to one another; this means that they make similar predictions. As a result, the ensemble of decision trees would still have low bias and high model variance. Selecting a random subset of features at each split helps reduce model variance by making the decision trees in the ensemble different from one another. Video Quick Check 20.1 Decision tree basics. Decision trees in scikit-learn. 20.1 20.2 Overfitting and decision trees. 20.2 20.3 Decision tree generation. Finding the best split. Entropy and weighted entropy. 20.3 20.4 Restricting decision tree complexity. Preventing growth and pruning. Random forests and bagging. 20.4 20.5 Regression trees. Summary of decision trees, classification, and regression. 20.5",
    "url": "http://localhost:4000/fa20/lecture/old/lec20/",
    "relUrl": "/lecture/old/lec20/"
  },
  "31": {
    "id": "31",
    "title": "Lecture 21 – Inference for Modeling",
    "content": "Lecture 21 – Inference for Modeling by Suraj Rampure (Summer 2020) slides video playlist code code HTML The Data 8 textbook chapter on estimation may be very helpful. Video Quick Check 21.1 A big picture overview of inference. Parameters and estimators. Bias and variance of estimators. The sample mean estimator. 21.1 21.2 Using bootstrap resampling in order to estimate the sampling distribution of an estimator. 21.2 21.3 Defining confidence intervals more generally. Describing and demoing how we can use the bootstrap to create confidence intervals for population parameters. 21.3 21.4 The assumptions we make when modeling with linear regression.. 21.4 21.5 Using the bootstrap to estimate the sampling distributions of parameters in a linear regression model. Inference for the true slope of a feature. 21.5 21.6 Multicollinearity, and its impacts on the interpretability of the parameters of our model. A summary of the lecture, and a brief overview of the ML taxonomy. 21.6",
    "url": "http://localhost:4000/fa20/lecture/old/lec21/",
    "relUrl": "/lecture/old/lec21/"
  },
  "32": {
    "id": "32",
    "title": "Lecture 22 – Dimensionality Reduction",
    "content": "Lecture 22 – Dimensionality Reduction by Josh Hug (Fall 2019) slides video playlist code code HTML Important: This lecture is a combination of two lectures from the Fall 2019 semester. There are a couple of small typos in 20.4. To check whether a set of vectors is an orthonormal set, we should check whether V^T @ V is the identity matrix (not V @ V^T). For matrices whose columns form an orthonormal set, the property that the matrix’s transpose is equivalent to its inverse only holds true if the matrix is square. There is a set of extra slides at the end of the lecture slides. These slides contain a review of concepts in linear algebra such as matrix multiplication and rank. Video Quick Check 22.1 Dimensionality. Visualizing high-dimensional data. 22.1 22.2 More visualizations of high-dimensional data. 22.2 22.3 Matrix decomposition, redundancy, and rank. Introduction to the singular value decomposition (SVD). 22.3 22.4 The theory behind the singular value decomposition. Orthogonality and orthonormality. 22.4 22.5 Low rank approximations with the singular value decomposition. 22.5",
    "url": "http://localhost:4000/fa20/lecture/old/lec22/",
    "relUrl": "/lecture/old/lec22/"
  },
  "33": {
    "id": "33",
    "title": "Lecture 23 – Principal Component Analysis",
    "content": "Lecture 23 – Principal Component Analysis by Josh Hug (Fall 2019) slides video playlist code code HTML Live Lecture PCA notes (Optional) PCA Tutorial Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 23.1 Definition and computation of principal components. Geometric interpretation of principal components and low rank approximations. Data centering. 23.1 23.2 Review of regression. Comparing the SVD and regression. Minimizing the perpendicular error. 23.2 23.3 Interpretation of singular values. The relationship between singular values and variance. Analyzing scree plots. 23.3 23.4 Introduction to principal Component analysis (PCA). PCA for exploratory data analysis. 23.4",
    "url": "http://localhost:4000/fa20/lecture/old/lec23/",
    "relUrl": "/lecture/old/lec23/"
  },
  "34": {
    "id": "34",
    "title": "Lecture 24 – Clustering",
    "content": "Lecture 24 – Clustering by Josh Hug (Fall 2019) slides video playlist code code HTML Important: This lecture is taken from the Fall 2019 semester. The variant of K-Means mentioned throughout this lecture seeks to minimize distortion, but most packages that implement K-Means (including scikit-learn) seek to minimize inertia instead of distortion. You will work with scikit-learn’s implementation of K-Means in Lab 14. In the lecture, there are a couple of plots that you might not be familiar with. The initial clustering example from 24.1 is taken from the first problem of Fall 2019 Midterm 2, and you will see the state plot from the beginning of 24.5 on Homework 8. Video Quick Check 24.1 Introduction to clustering. Taxonomy of machine learning. Examples of clustering in practice. 24.1 24.2 The K-Means clustering algorithm. Example of K-Means clustering. 24.2 24.3 Loss functions for K-Means. Inertia and distortion. Optimizing distortion. 24.3 24.4 Agglomerative clustering as an alternative to K-Means. Example of agglomerative clustering. Dendrograms and other clustering algorithms. 24.4 24.5 Picking the number of clusters. The elbow method and silhouette scores. Summary of clustering and machine learning. 24.5",
    "url": "http://localhost:4000/fa20/lecture/old/lec24/",
    "relUrl": "/lecture/old/lec24/"
  },
  "35": {
    "id": "35",
    "title": "Lecture 25 – Big Data",
    "content": "Lecture 25 – Big Data by Josh Hug (Fall 2018) slides slides PDF video playlist code code HTML Important: This lecture is a combination of two lectures from the Fall 2018 semester. The Google slides version of the lecture slides has a lot of formatting issues. The PDF version might be better for viewing purposes. 25.2 has a little bit of redundancy, so feel free to skip through some of the redundant parts. Unlike the Fall 2018 semester, SQL was covered toward the beginning of the class this summer. There are also some references to “the project” or “project 2,” which would refer to Homework 9 this summer. Video Quick Check 25.1 Data in the organization. Operational data stores and data warehouses. Extract, transform, load (ETL). 25.1 25.2 The multidimensional data model. Fact tables and dimension tables. Star schemas and snowflake schemas. Online analytics processing (OLAP). 25.2 25.3 Data warehouses and data lakes. 25.3 25.4 Distributed file systems and fault tolerance. 25.4 25.5 Distributed aggregation with MapReduce. The MapReduce abstraction. 25.5 25.6 MapReduce technologies. Hadoop and Spark. Resilient Distributed Datasets (RDDs). 25.6 25.7 Spark notebook demo. 25.7",
    "url": "http://localhost:4000/fa20/lecture/old/lec25/",
    "relUrl": "/lecture/old/lec25/"
  },
  "36": {
    "id": "36",
    "title": "Lecture 3 – Random Variables",
    "content": "Lecture 3 – Random Variables by Suraj Rampure (Summer 2020) slides video playlist Lecture Recap 1, Part 1 (video) (notes) Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 3.1 Formal definition of random variables. 3.1 3.2 Distributions of random variables. 3.2 3.3 Defining the Bernoulli and binomial distributions. (Stat 88 reading) 3.3 3.4 Discussing equality of random variables – equal vs. equal in distribution. 3.4 3.5 Expectation. Linearity of expectation. Sample calculations, and the expectation of the Bernoulli and binomial distributions. 3.5 3.6 Variance of random variables. Walking through an alternate calculation of variance. Variance of a linear transformation. 3.6 3.7 Deriving the variance of a sum. Understanding covariance, correlation, and independence. 3.7 3.8 Variance of an i.i.d. sum. Variance of the Bernoulli and binomial distributions. 3.8 3.9 Variability of the sample mean. Reviewing inferential concepts from Data 8, but with the framework of random variables. 3.9",
    "url": "http://localhost:4000/fa20/lecture/old/lec3/",
    "relUrl": "/lecture/old/lec3/"
  },
  "37": {
    "id": "37",
    "title": "Lecture 4 – SQL",
    "content": "Lecture 4 - SQL by Allen Shen (Summer 2020) slides video playlist code code HTML code walkthrough (Josh Hug) code walkthrough (Allen Shen) Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 4.1 Databases and database management systems. 4.1 4.2 Relational database schemas. 4.2 4.3 SQL overview and the DISTINCT keyword. 4.3 4.4 Types of joins in SQL. 4.4 4.5 NULL values in SQL. 4.5 4.6 SQL predicates and casting. 4.6 4.7 SQL sampling, subqueries, and common table expressions. 4.7 4.8 SQL CASE expressions and the SUBSTR function. 4.8 4.9 SQL summary and conclusion. 4.9 Notebook Notebook walkthrough (Josh Hug). N/A Notebook Notebook walkthrough (Allen Shen). N/A",
    "url": "http://localhost:4000/fa20/lecture/old/lec4/",
    "relUrl": "/lecture/old/lec4/"
  },
  "38": {
    "id": "38",
    "title": "Lecture 5 – Pandas, Part 1",
    "content": "Lecture 5 - Pandas, Part 1 by Josh Hug (Fall 2019) slides video playlist code code HTML Intro to Pandas if you’ve taken Data 8 (zip) Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 5.1.1 Pandas data frames, series, and indices. 5.1.1 5.1.2 Pandas indices demo. 5.1.2 5.2 Pandas indexing with the bracket operator. 5.2 5.3 Pandas boolean array selection, the isin function, and the query command. 5.3 5.4.1 Pandas indexing with .loc. 5.4.1 5.4.2 Pandas indexing with .iloc and Pandas sampling. 5.4.2 5.5.1 Pandas utility functions, properties, and the sort_values method. 5.5.1 5.5.2 The value_counts and unique methods in Pandas. An exploration of the baby names data set. 5.5.2",
    "url": "http://localhost:4000/fa20/lecture/old/lec5/",
    "relUrl": "/lecture/old/lec5/"
  },
  "39": {
    "id": "39",
    "title": "Lecture 6 – Pandas, Part 2",
    "content": "Lecture 6 - Pandas, Part 2 by Josh Hug (Fall 2019) slides video playlist code code HTML joins code HTML Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 6.1 Pandas string methods. 6.1 6.2 Adding, modifying, and removing columns in Pandas. 6.2 6.3 Using the Pandas groupby function for aggregation. 6.3 6.4 Puzzles using the Pandas groupby function. 6.4 6.5 Other features of the Pandas groupby function including size and filter. 6.5 6.6 Grouping by multiple columns and pivot tables in Pandas. 6.6 6.7 Joining two tables in Pandas. 6.7",
    "url": "http://localhost:4000/fa20/lecture/old/lec6/",
    "relUrl": "/lecture/old/lec6/"
  },
  "40": {
    "id": "40",
    "title": "Lecture 7 – Data Cleaning and EDA",
    "content": "Lecture 7 – Data Cleaning and EDA by Joseph Gonzalez (Spring 2020) slides video playlist code (bonus) Joe Hellerstein’s primer on data models Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 7.1 Exploratory data analysis and its position in the data science lifecycle. The relationship between data cleaning and EDA. 7.1 7.2 Exploring various different data storage formats and their tradeoffs. 7.2 7.3 Primary keys and foreign keys. Eliminating redundancy in tables. 7.3 7.4 Defining and discussing the terms quantitative discrete, quantitative continuous, qualitative ordinal, qualitative nominal. 7.4 7.5 Discussing the granularity and scope of our data to ensure that it&#39;s appropriate for analysis. Discussing various methods of encoding time, and flaws to be aware of. 7.5 7.6 Ways in which our data can be incorrect or corrupt. Different methods for addressing missing values, and their tradeoffs. 7.6 7.7 Summarizing the process of EDA, and a demo of EDA on real data. 7.7",
    "url": "http://localhost:4000/fa20/lecture/old/lec7/",
    "relUrl": "/lecture/old/lec7/"
  },
  "41": {
    "id": "41",
    "title": "Lecture 8 – Regular Expressions",
    "content": "Lecture 8 - Regular Expressions by Josh Hug (Fall 2019) slides video playlist code code HTML Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 8.1 Canonicalizing strings and using the split method to extract from textual data. 8.1 8.2 Basic regular expression syntax (i.e. closures). Order of operations in regular expressions. 8.2 8.3 Expanded regular expression syntax (i.e. character classes). A couple of regular expression exercises. 8.3 8.4 Limitations of regular expressions. Other regular expression syntax (i.e. lazy closures). 8.4 8.5 Using regular expressions in Python. Regular expression groups. 8.5 8.6 Regular expression case studies on police data and restaurant data. 8.6",
    "url": "http://localhost:4000/fa20/lecture/old/lec8/",
    "relUrl": "/lecture/old/lec8/"
  },
  "42": {
    "id": "42",
    "title": "Lecture 9 – Visualization, Part 1",
    "content": "Lecture 9 – Visualization, Part 1 by Suraj Rampure (Summer 2020) slides video playlist code code HTML Make sure to complete the Quick Check questions in between each video. These are ungraded, but it’s in your best interest to do them. Video Quick Check 9.1 Formal definition of visualization. The purpose of visualization in the data science lifecycle. 9.1 9.2 Different ways we can map from data to properties of a visualization. 9.2 9.3 Defining distributions, and determining whether or not given visualizations contain a distribution. 9.3 9.4 Bar plots as a means of displaying the distribution of a qualitative variable, as well as for plotting a quantitative variable across several different categories. 9.4 9.5 Rug plots. Histograms, where areas are proportions. Reviewing histogram calculations from Data 8. Density curves as smoothed versions of histograms. 9.5 9.6 Describing distributions of quantitative variables using terms such as modes, skew, tails, and outliers. 9.6 9.7 Using box plots and violin plots to visualize quantitative distributions. Using overlaid histograms and density curves, and side by side box plots and violin plots, to compare multiple quantitative distributions. 9.7 9.8 Using scatter plots, hex plots, and contour plots to visualize the relationship between pairs of quantitative variables. Summary of visualization thus far. 9.8",
    "url": "http://localhost:4000/fa20/lecture/old/lec9/",
    "relUrl": "/lecture/old/lec9/"
  },
  "43": {
    "id": "43",
    "title": "Resources",
    "content": "Resources Exam Resources Semester Midterm (1) Midterm 2 Final Fall 2020 Exam (Solutions)     Summer 2020 Exam (Solutions) Exam (Solutions) Exam (Solutions) Spring 2020 Checkpoint (Solutions)   N/A Fall 2019 Exam (Solutions) Exam (Solutions) Exam (Solutions) Summer 2019 Exam (Solutions) [Video]   Exam (Solutions) Spring 2019 Exam (Solutions) [Video] Exam (Solutions) [Video] Exam (Solutions) Fall 2018 Exam (Solutions)   Exam (Solutions) Spring 2018 Exam (Solutions)   Exam (Solutions) [Video] Fall 2017 Exam (Solutions) [Video]   Exam (Solutions) Spring 2017 Exam (Solutions)   Exam (Solutions) Spring 2020 Checkpoint Reference Sheet Fall 2019 Midterm 1 Reference Sheet Spring 2019 Midterm 1 Reference Sheet Other Resources We will be posting all lecture materials on the course syllabus. In addition, they will also be listed in the following publicly visible Github Repo. Here is a collection of resources that will help you learn more about various concepts and skills covered in the class. Learning by reading is a key part of being a well rounded data scientist. We will not assign mandatory reading but instead encourage you to look at these and other materials. If you find something helpful, post it on Piazza, and consider contributing it to the course website. You can send us changes to the course website by forking and sending a pull request to the course website github repository. You will then become part of the history of Data 100 at Berkeley. Local Setup Click here to read our guide on how to set up our development environment locally (as an alternative to using DataHub). SQL Resources We’ve assembled some SQL Review Slides to help you brush up on SQL. We’ve also compiled a list of SQL practice problems, which can be found here, along with their solutions. This SQL Cheat Sheet is an awesome resource that was created by Luke Harrison, a former Data 100 student. Probability Practice We’ve compiled a few practice probability problems that we believe may help in understanding the ideas covered in the course. They can be found here, along with their solutions. We’d also like to point you to the textbook for Stat 88, an introductory probability course geared towards data science students at Berkeley. Regex Practice We’ve organized some regex problems to help you get extra practice on regex in a notebook format. They can be found here, along with their solutions. Web References As a data scientist you will often need to search for information on various libraries and tools. In this class we will be using several key python libraries. Here are their documentation pages: The Bash Command Line: Linux and Bash: Intro to Linux, Cloud Computing (which you can skip for the purposes of this class), and the Bash command line. You can skip all portions that don’t pertain to using the command line. Bash Part 2: Part 2 of the intro to command line. Python: Python Tutorial: Teach yourself python. This is a pretty comprehensive tutorial. Python + Numpy Tutorial this tutorial provides a great overview of a lot of the functionality we will be using in DS100. Python 101: A notebook demonstrating a lot of python functionality with some (minimal explanation). Data Visualization: matplotlib.pyplot tutorial: This short tutorial provides an overview of the basic plotting utilities we will be using. Altair Documentation: Altair(Vega-Lite) is a new and powerful visualization library. We might not get to teach it this semester, but you should check it out if you are interested in pursuing visualization deeper. In particular, you should find the example gallery helpful. Prof. Jeff Heer’s Visualization Curriculum: This repository contains a series of Python-based Jupyter notebooks that teaches data visualization using Vega-Lite and Altair. If you are interested in learning more about data visualization, you can find more materials in: Edward Tufte’s book sequences – a classic! Prof. Heer’s class. Pandas: The Pandas Cookbook: This provides a nice overview of some of the basic Pandas functions. However, it is slightly out of date. Learn Pandas A set of lessons providing an overview of the Pandas library. Python for Data Science Another set of notebook demonstrating Pandas functionality. Books Because data science is a relatively new and rapidly evolving discipline there is no single ideal textbook for this subject. Instead we plan to use reading from a collection of books all of which are free. However, we have listed a few optional books that will provide additional context for those who are interested. Principles and Techniques of Data Science This is the accompanying textbook written for DS100 course. Introduction to Statistical Learning (Free online PDF) This book is a great reference for the machine learning and some of the statistics material in the class Data Science from Scratch (Available as eBook for Berkeley students) This more applied book covers many of the topics in this class using Python but doesn’t go into sufficient depth for some of the more mathematical material. Doing Data Science (Available as eBook for Berkeley students) This books provides a unique case-study view of data science but uses R and not Python. Python for Data Analysis (Available as eBook for Berkeley students). This book provides a good reference for the Pandas library. Data Science Education Interested in bringing the Data Science major or curriculum to your academic institution? Please fill out this form if you would like support from Berkeley in offering some variant of our Data Science courses at your institution (or just to let us know that you’re interested). Information about the courses appear at data8.org and ds100.org. Please note that this form is for instructors. If you are only interested in learning Python or data science, please look at our Data 8 or Data 100 websites mentioned above.",
    "url": "http://localhost:4000/fa20/resources/",
    "relUrl": "/resources/"
  },
  "44": {
    "id": "44",
    "title": "Local Setup",
    "content": "Local Setup We will still be using datahub as our primary computing environment. This page serves as a guide for alternative environment setup. In other words: you don’t have to follow these instructions unless you’d like an alternative to datahub. Contents Installing conda by OS OSX Windows Linux Creating your environment Working on assignments locally Opening notebooks locally Verifying your environment Removing the environment to start over Submitting your work FAQ OSX You will need access to the command line. On a Mac, you can open the Terminal by opening Spotlight (Cmd + Space) and typing &quot;Terminal&quot;. Alternatively, you can go to your Applications screen and select Terminal (it might be in the folder named &quot;Other&quot;) Homebrew is a package manager for OSX. If you haven’t already, install it by running the following in the command line (copy, paste, and enter): # This downloads the Ruby code of the installation script and runs it /usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; Verify your installation by making sure brew --version doesn’t error at your terminal. Download and install Anaconda: # Uses curl to download the installation script curl https://repo.continuum.io/miniconda/Miniconda2-4.5.11-MacOSX-x86_64.sh &gt; miniconda.sh # Run the miniconda installer (you will need to enter your password) bash miniconda.sh Close and restart your terminal. Ensure the installation worked by running conda --version. You may remove the miniconda.sh script now if you’d like. Click here to continue to the next part of the setup. Windows Windows is especially prone to error if you aren’t careful about your configuration. If you’ve already had Anaconda or git installed and can’t get the other to work, try uninstalling everything and starting from scratch. Installing Anaconda: Visit the Anaconda website and download the installer for Python 3.7. Download the 64-bit installer if your computer is 64-bit (most likely), the 32-bit installer if not. See this FAQ if you are unsure. Run the exe file to install Anaconda. Leave all the options as default (install for all users, in the default location). Make sure both of these checkboxes are checked: 1) Verify that the installation is working by starting the Anaconda Prompt (you should be able to start it from the Start Menu) and typing python: Notice how the python prompt shows that it is running from Anaconda. Now you have conda installed! From now on, when we talk about the “Terminal” or “Command Prompt”, we are referring to the Anaconda Prompt that you just installed. Click here to continue to the next part of the setup. Linux These instructions assume you have apt-get (Ubuntu and Debian). For other distributions of Linux, substitute the appropriate package manager. Your terminal program allows you to type commands to control your computer. On Linux, you can open the Terminal by going to the Applications menu and clicking “Terminal”. Install wget. This is a command-line tool that lets you download files / webpages at the command line. sudo apt-get install wget Download the Anaconda installation script: wget -O install_anaconda.sh https://repo.continuum.io/miniconda/Miniconda2-4.5.11-Linux-x86_64.sh 4) Install Anaconda: bash install_anaconda.sh 5) Close and restart your terminal. Ensure the installation worked by running `conda --version`. You may remove the install_anaconda.sh script now if you’d like. Click here to continue to the next part of the setup. Creating your environment These instructions are the same for OSX, Windows, and Linux. Download the data100 data100_environment.yml] from the course repository here or: # download via curl curl https://raw.githubusercontent.com/DS-100/su20/gh-pages/resources/assets/local_setup/data100_environment.yml &gt; data100_environment.yml # OR download via wget wget -O data100_environment.yml https://raw.githubusercontent.com/DS-100/su20/gh-pages/resources/assets/local_setup/data100_environment.yml This YAML file is what we use to specify the dependencies and packages (and their versions) we wish to install into the conda environment we will make for this class. The purpose of the environment is to ensure that everyone in the course is using the same package versions for every assignment whether or not they are working on datahub. This is to prevent inconsistent behavior due to differences in package versions. Using the Terminal, navigate to the directory where you downloaded data100_environment.yml. Run these commands to create a new conda environment. Each conda environment maintains its own package versions, allowing us to switch between package versions easily. For example, this class uses Python 3, but you might have another that uses Python 2. With a conda environment, you can switch between those at will. # sanity check on conda installation. Should be 4.5 or higher conda --version # update conda just in case it&#39;s out of date # enter y if prompted to proceed conda update conda # download git conda install -c anaconda git # Create a python 3.6 conda environment with the full set # of packages specified in environment.yml (jupyter, numpy, pandas, ...) conda env create -f data100_environment.yml # Switch to the data100 environment conda activate data100 # Check if packages are in the environment # This should not be empty! conda list From now on, you can switch to the data100 env with conda activate data100, and switch back to the default env with conda deactivate. Working on assignments locally These instructions are the same for OSX, Windows, and Linux. To work on assignments, you should fetch the assignment on datahub, navigate to the assignment folder and click on the download icon on the top right: Then you can unzip the files into a folder of your choosing. Remember the location of your assignment files because you’ll need to navigate to that folder to open the notebook. Opening notebooks locally To open Jupyter notebooks, you’ll navigate to parent directory of the assignment in your terminal, activate the environment, and start up a jupyter server. This will look something like: cd path/to/assignment/directory conda activate data100 jupyter notebook This will automatically open the notebook interface in your browser. You can then browse to a notebook and open it. Make sure to always work in the data100 conda environment when you are using jupyter notebooks for this class. This ensures you have all the necessary packages required for the notebook to run. Verifying Your Environment You can tell if you are correct environment if your terminal looks something like: Additionally, conda env list outputs a list of all your conda environments, and data100 should appear with a * next to it (the active one). Removing the environment to start over If you feel as if you’ve messed up and need to start over, you can remove the environment with conda remove --name data100 --all To verify that the environment was removed, in your Terminal window or an Anaconda Prompt, run: conda info --envs Which should then no longer display the data100 environment. Submitting your work Submissions will still be handled via datahub. To upload your work, navigate to the appropriate assignment folder on datahub and click on the upload button on the top right. Remember to validate, submit, and upload to Gradescope (for homeworks and projects). FAQ Shell not properly configured to use conda activate If you had an older version of Anaconda installed (perhaps for another class), you may see the following message. Follow the instructions in the prompt to: Enable conda for all users sudo ln -s ... Put the base environment on PATH echo &quot;conda activate&quot; &gt;&gt; ~/.bash_profile&quot;. Note that ~/.bash_profile may be something different like ~/.bashrc. Manually remove the line that looks like export PATH=&quot;/usr/local/miniconda3/bin:$PATH&quot; from your .bash_profile. Use your favorite plaintext editor to do this (do not use a rich text editor like Microsoft Word!).",
    "url": "http://localhost:4000/fa20/setup/",
    "relUrl": "/setup/"
  },
  "45": {
    "id": "45",
    "title": "Staff",
    "content": "Staff Jump to Instructors, Teaching Assistants, or Tutors Note: Consult the calendar for the most up-to-date office hours for each GSI. Instructors {% assign instructors = site.staffers | where: &#39;role&#39;, &#39;Instructor&#39; %} {% for staffer in instructors %} {{ staffer }} {% endfor %} Teaching Assistants {% assign teaching_assistants = site.staffers | where: &#39;role&#39;, &#39;Teaching Assistant&#39; %} {% for staffer in teaching_assistants %} {{ staffer }} {% endfor %} Tutors {% assign readers = site.staffers | where: &#39;role&#39;, &#39;Tutor&#39; %} {% for staffer in readers %} {{ staffer }} {% endfor %}",
    "url": "http://localhost:4000/fa20/staff/",
    "relUrl": "/staff/"
  },
  "46": {
    "id": "46",
    "title": "Syllabus",
    "content": "Syllabus Jump to: About Data 100 Online Format Policies About Data 100 Combining data, computation, and inferential thinking, data science is redefining how people and organizations solve challenging problems and understand their world. This intermediate level class bridges between Data8 and upper division computer science and statistics courses as well as methods courses in other fields. In this class, we explore key areas of data science including question formulation, data collection and cleaning, visualization, statistical inference, predictive modeling, and decision making.​ Through a strong emphasis on data centric computing, quantitative critical thinking, and exploratory data analysis, this class covers key principles and techniques of data science. These include languages for transforming, querying and analyzing data; algorithms for machine learning methods including regression, classification and clustering; principles behind creating informative data visualizations; statistical concepts of measurement error and prediction; and techniques for scalable data processing. Goals Prepare students for advanced Berkeley courses in data-management, machine learning, and statistics, by providing the necessary foundation and context Enable students to start careers as data scientists by providing experience working with real-world data, tools, and techniques Empower students to apply computational and inferential thinking to address real-world problems Prerequisites While we are working to make this class widely accessible, we currently require the following (or equivalent) prerequisites. We are not enforcing prerequisites during enrollment. However, all of the prerequisties will be used starting very early on in the class. It is your responsibility to know the material in the prerequisites.: Foundations of Data Science: Data8 covers much of the material in Data 100 but at an introductory level. Data8 provides basic exposure to python programming and working with tabular data as well as visualization, statistics, and machine learning. Computing: The Structure and Interpretation of Computer Programs (CS 61A) or Computational Structures in Data Science (CS 88). These courses provide additional background in python programming (e.g., for loops, lambdas, debugging, and complexity) that will enable Data 100 to focus more on the concepts in Data Science and less on the details of programming in python. Math: Linear Algebra (Math 54, EE 16a, or Stat89a): We will need some basic concepts like linear operators, eigenvectors, derivatives, and integrals to enable statistical inference and derive new prediction algorithms. This may be satisfied concurrently to Data 100. Online Format This fall, Data 100 will be run entirely online. This section details exactly how each component of the course will operate. But here’s a nice high-level “typical week in the course”: Monday Tuesday Wednesday Thursday Friday Office Hours Office Hours Office Hours Office Hours Office Hours Live lab Lecture released Discussion Lecture released Homework released Lab due, Quick Check due     Homework due Lab released Note that these deadlines are subject to change. To see when any live events are scheduled, check the Calendar. To see when lectures, discussions, and assignments are released (and due), check the Home Page. Lecture There are 2 lectures per week. Lectures will be entirely pre-recorded, in a format that is optimized for online learning (short 5-10 minute videos with conceptual problems in between). Lecture videos will be released on the mornings of Tuesday and Thursday. Some of these will be from previous semesters, and some will be recorded this fall by the instructors. Lecture videos will be posted on YouTube. Each “lecture” will be an html page linked on the course website, containing videos and links to slides and code. There are “Quick Check” conceptual questions in between each lecture video, linked on the lecture webpage. See below for more details. Each lecture will also have a Piazza thread for students to ask questions. Note: Alongside each lecture are textbook readings. Textbook readings are purely supplementary, and may contain material that is not in scope (and may also not be comprehensive). Quick Checks Quick Checks, as mentioned above, are short conceptual questions embedded into each lecture, in the form of Google Forms. These are meant for you to check your understanding of the concepts that were just introduced. Since there are roughly 26 lectures, there are roughly 26 Quick Checks, each of which consists of 4-7 Google Forms. Quick Checks are graded on completion. That is, your score on them does not matter, you just need to do them. For each lecture, you will be required to submit a code to Gradescope that you will receive after completing one of the Google Forms for that lecture. These are due the Monday after the lecture is released. (Though we will assign grades using Gradescope, we will also collect emails on the Google Forms themselves.) Homeworks and Projects Homeworks are week-long assignments that are designed to help students develop an in-depth understanding of both the theoretical and practical aspects of ideas presented in lecture. Projects are two-week-long assignments that integrate these ideas with real-world datasets. In a typical week, homework is released on Friday and is due the following Thursday at 11:59PM. Near the midterm, or during weeks in which a project is assigned, you will have more than one week to work on the current assignment. One or two homeworks will be on-paper written assignments; the rest will be Jupyter notebooks. Homeworks have both visible and hidden autograder tests. The visible tests are mainly sanity checks, e.g. a probability is &lt;= 1, and are visible to students while they do the assignment. The hidden tests generally check for correctness, and are invisible to students while they are doing the assignment. The primary form of support students will have for homeworks and projects are the office hours we’ll host, and Piazza. Homeworks must be completed individually. Labs Labs are shorter programming assignments designed to give students familiarity with new ideas. In a typical week, lab is released on Friday and is due the following Monday. All lab autograder tests are visible. To help with lab, we will host live lab sections on Monday at various times, in which GSIs will walk through the assignment via Zoom. See the Calendar for when these are scheduled. Students can also get help with labs at office hours and on Piazza. Discussions Discussion sections are meant to allow students a chance to discuss conceptual ideas and solve problems with other students, with the help of a GSI (this becomes slightly harder given the fact that this course is being offered completely remotely). Each discussion consists of a worksheet. In a typical week, we will release the discussion worksheet on Wednesday morning. There are two “pathways” we envision students taking when it comes to consuming discussion content. Watching a pre-recorded discussion video. Each discussion worksheet will be accompanied with a GSI-created video walkthrough, released at the same time. Students should watch this video soon after it is released. With any lingering questions, students should come to office hours. Coming to a live Zoom discussion section. We will be holding live discussion sections at several times on Wednesdays. In the first few weeks, students will be able to attend whichever section they desire, but we will eventually require you to sign up for a particular section if you want to keep attending (this is to keep sections small and personal). Office Hours We plan on hosting roughly 10 hours of office hours each weekday. These hours are listed on the Calendar. OH will serve as a one-stop shop for students to get help with assignments. Office Hours can be accessed via oh.ds100.org, where students add themselves to the “queue” and specify the assignment they need help on. Once it’s their turn, they will be provided with a Zoom link to join, in order to get help from staff. The instructors will also be hosting conceptual office hours. These will be reflected on the Calendar. We are also holding “lost office hours” once a week. These are designed to accommodate students who are behind on material and would like help catching up. These are meant for conceptual questions only, not for assignment help. These will also be reflected on the Calendar. Exams There will be one midterm exam, on October 15th (7-9PM PDT), and a final exam on December 15th (7-10PM PDT). Alternate exams will only be given to students with a documented conflict, or to those who are in timezones very far from PDT, or to those who have extenuating circumstances. Policies Undergraduate Grading Scheme (for students enrolled in Data C100): Category Weight Details Homeworks 30% 9, with 2 drops Labs 10% Roughly 13, with 3 drops Projects 15% 7.5% each (2, with 0 drops) Quick Checks 5%   Midterm Exam 15%   Final 25%   Graduate Grading Scheme (for students enrolled in Data C200): Category Weight Details Homeworks 30% 9, with 2 drops Projects 15% 7.5% each (2, with 0 drops) Final Project 15%   Midterm Exam 15%   Final 25%   Note that a ninth homework and second homework drop were announced partway through the semester. Late Policy All assignments are due at 11:59 pm on the due date specified on the syllabus. Gradescope is where all assignments are submitted. Extensions are only provided to students with DSP accommodations, or in the case of exceptional circumstances. Homeworks and labs will not be accepted late. Gradescope may allow you to make late submissions, but you will later be given a 0. Projects are marked down by 10% per day, up to two days. After two days, project submissions will not be accepted. Submission times are rounded up to the next day. That is, 2 minutes late = 1 day late. Collaboration Policy and Academic Dishonesty Assignments Data science is a collaborative activity. While you may talk with others about the homework, we ask that you write your solutions individually in your own words. If you do discuss the assignments with others please include their names at the top of your notebook. Keep in mind that content from assignments will likely be covered on both the midterm and final. If we suspect that you have submitted plagiarized work, we will call you in for a meeting. If we then determine that plagiarism has occurred, we reserve the right to give you a negative full score (-100%) or lower on the assignments in question, along with reporting your offense to the Center of Student Conduct. Rather than copying someone else’s work, ask for help. You are not alone in this course! The entire staff is here to help you succeed. If you invest the time to learn the material and complete the assignments, you won’t need to copy any answers. (taken from 61A) We also ask that you do not post your assignment solutions publicly. Exams Cheating on exams is a serious offense. We have methods of detecting cheating on exams – so don’t do it! Students caught cheating on any exam will fail this course. We will be following the EECS departmental policy on Academic Honesty, so be sure you are familiar with it. We want you to succeed! If you are feeling overwhelmed, visit our office hours and talk with us. We know college can be stressful – and especially so during the COVID-19 pandemic – and we want to help you succeed.",
    "url": "http://localhost:4000/fa20/syllabus/",
    "relUrl": "/syllabus/"
  }
  
}
